{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN61dINz7fXc0Qfrd8buF88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Deep-seek-R1-MedicalSFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "## Objective:\n",
        "Adapt `DeepSeek-R1-Distill-Llama-8B` for medical chain-of-thought reasoning.\n",
        "\n",
        "## Key Components:\n",
        "- **Model:** `unsloth/DeepSeek-R1-Distill-Llama-8B`\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- **Dataset:** 500 samples from `medical-o1-reasoning-SFT`\n",
        "- **Tools:**\n",
        "  - `Unsloth` (2x faster training)\n",
        "  - 4-bit quantization\n",
        "  - LoRA adapters\n",
        "- **Result:** 44-minute training resulting in concise medical reasoning with structured `<think>` outputs.\n",
        "\n",
        "## Performance Improvement:\n",
        "\n",
        "| **Metric**         | **Before Fine-Tuning** | **After Fine-Tuning** |\n",
        "|--------------------|------------------------|-----------------------|\n",
        "| **Response Length** | 450 words              | 150 words             |\n",
        "| **Reasoning Style** | Verbose                | Focused               |\n",
        "| **Answer Format**   | Bulleted               | Paragraph             |\n"
      ],
      "metadata": {
        "id": "bs2w04WH_Evr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step-by-step  fine-tune DeepSeek-R1-Distill-Llama-8B on medical data"
      ],
      "metadata": {
        "id": "6cURM5LRAw-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "qXEdvYFJBWFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zwPSzhO-o5A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The '%%capture' magic command in Jupyter notebooks suppresses output from subsequent cells.\n",
        "\n",
        "!pip install kaggle\n",
        "# Installs the 'kaggle' package using pip. Assumes pip is installed and configured.\n",
        "\n",
        "!pip install unsloth\n",
        "# Installs the 'unsloth' package using pip. Similar assumption as above.\n",
        "\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "# Installs a specific version of the 'unsloth' package directly from its GitHub repository.\n",
        "# '--force-reinstall': Forces reinstallation even if the package is already installed.\n",
        "# '--no-cache-dir': Avoids caching the installation files.\n",
        "# '--no-deps': Skips installing dependencies, useful if dependencies are already satisfied.\n",
        "# 'git+https://github.com/unslothai/unsloth.git': GitHub repository URL from which to install the package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2: Authentication in Google Colab**"
      ],
      "metadata": {
        "id": "waLclZRYIT1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "# Imports the 'login' function from the 'huggingface_hub' package to authenticate with Hugging Face.\n",
        "\n",
        "from google.colab import userdata\n",
        "# Imports 'userdata' from Google Colab, which allows access to stored secrets or credentials.\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "# Gets the Hugging Face authentication token stored in Google Colab's 'userdata' for secure access.\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(hf_token)\n",
        "# Uses the retrieved token to authenticate the user with Hugging Face's hub."
      ],
      "metadata": {
        "id": "UmIYt48ru6Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# Imports the 'wandb' library, which is used for experiment tracking and logging in machine learning.\n",
        "\n",
        "from google.colab import userdata\n",
        "# Imports 'userdata' from Google Colab to access stored secrets or credentials.\n",
        "\n",
        "# Retrieve the Weights & Biases (W&B) API token from Colab secrets\n",
        "wb_token = userdata.get('wandb')\n",
        "# Gets the stored W&B API key to authenticate with the W&B platform.\n",
        "\n",
        "wandb.login(key=wb_token)\n",
        "# Logs into W&B using the retrieved API key for tracking experiments.\n",
        "\n",
        "# Initialize a new W&B run for experiment tracking\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset',  # Specifies the W&B project name\n",
        "    job_type=\"training\",  # Labels this run as a training job\n",
        "    anonymous=\"allow\"  # Allows anonymous logging if authentication isn't provided\n",
        ")\n"
      ],
      "metadata": {
        "id": "VplvaogkwrXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3: Model Initialization**"
      ],
      "metadata": {
        "id": "rUN1Y4u1BuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# Imports the 'FastLanguageModel' class from the 'unsloth' library, which is optimized for efficient language model training and inference.\n",
        "\n",
        "# Define model configuration parameters\n",
        "max_seq_length = 2048  # Sets the maximum sequence length for the model.\n",
        "dtype = None  # Specifies the data type for model computation (None means the default type will be used).\n",
        "load_in_4bit = True  # Enables 4-bit quantization for reduced memory usage and faster inference.\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Specifies the model to be loaded from Hugging Face Hub.\n",
        "    max_seq_length=max_seq_length,  # Uses the defined max sequence length.\n",
        "    dtype=dtype,  # Uses the specified data type (None defaults to the modelâ€™s recommended type).\n",
        "    load_in_4bit=load_in_4bit,  # Enables 4-bit quantization if True.\n",
        "    token=hf_token,  # Uses the Hugging Face authentication token to access private models if necessary.\n",
        ")"
      ],
      "metadata": {
        "id": "GNG2D8WKBvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.Inference to model Model**"
      ],
      "metadata": {
        "id": "ChbEiUQDJiTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the prompt format for inference\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\"\n",
        "\n",
        "# Define the medical question for inference\n",
        "question = (\n",
        "    \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \"\n",
        "    \"but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would \"\n",
        "    \"cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
        ")\n",
        "\n",
        "# Prepare the model for inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the input prompt and move it to the GPU for efficient processing\n",
        "inputs = tokenizer(\n",
        "    [prompt_style.format(question, \"\")],  # Format the prompt with the question\n",
        "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "    padding=True,  # Ensure proper padding for batch processing\n",
        "    truncation=True  # Prevent overly long inputs from causing issues\n",
        ").to(\"cuda\")  # Move tensors to GPU\n",
        "\n",
        "# Generate model output based on the input prompt\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,  # Input token IDs\n",
        "    attention_mask=inputs.attention_mask,  # Attention mask for proper token processing\n",
        "    max_new_tokens=1200,  # Limit the response length to avoid excessive output\n",
        "    use_cache=True,  # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output into a human-readable format\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Extract and print the response portion after \"### Response:\"\n",
        "print(response[0].split(\"### Response:\")[1].strip())"
      ],
      "metadata": {
        "id": "YKgtOmN3JvNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n"
      ],
      "metadata": {
        "id": "DcFTcPM3ymTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Question\"]\n",
        "    cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }"
      ],
      "metadata": {
        "id": "-QKyszWqysgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "dataset[\"text\"][0]"
      ],
      "metadata": {
        "id": "RT82XMtby2J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Parameter-Efficient Fine-Tuning (PEFT) to the model using LoRA (Low-Rank Adaptation)\n",
        "# This allows fine-tuning large models with fewer resources by only updating a small subset of parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The pre-trained model to which LoRA will be applied\n",
        "    r=16,  # Rank of the low-rank matrices used in LoRA. Higher values increase capacity but also computational cost.\n",
        "           # Suggested values: 8, 16, 32, 64, 128. Choose based on your task and resources.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # List of model layers to apply LoRA to.\n",
        "                                                           # These are typically attention and feed-forward layers.\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Controls the magnitude of updates.\n",
        "                    # A higher value increases the impact of LoRA updates.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA layers. Set to 0 for optimal performance.\n",
        "                     # Dropout can help prevent overfitting but is not necessary here.\n",
        "    bias=\"none\",  # Whether to include bias terms in LoRA. \"none\" is optimized for efficiency.\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enables gradient checkpointing to save memory during training.\n",
        "                                           # \"unsloth\" is optimized for very long sequences and reduces VRAM usage by 30%.\n",
        "    random_state=3407,  # Random seed for reproducibility. Ensures consistent results across runs.\n",
        "    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (RS-LoRA). Set to False by default.\n",
        "    loftq_config=None,  # Configuration for LoftQ (if applicable). Set to None as it is not used here.\n",
        ")"
      ],
      "metadata": {
        "id": "QSiQzVRrjej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Setup"
      ],
      "metadata": {
        "id": "sPmYEzQYCbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "50rz0OaDCd89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Start Training"
      ],
      "metadata": {
        "id": "5RfyuKQLC2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "No8CPQAmCz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save & Deploy"
      ],
      "metadata": {
        "id": "AhMB-1lmC9es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally\n",
        "model.save_pretrained_merged(\"DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Push to Hub\n",
        "model.push_to_hub_merged(\"username/DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")"
      ],
      "metadata": {
        "id": "ZyCx4pHvC708"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}