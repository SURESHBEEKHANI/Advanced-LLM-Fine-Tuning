{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQ6/lJzuWIXyGa525dyUOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Deep-seek-R1-MedicalSFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "## Objective:\n",
        "Adapt `DeepSeek-R1-Distill-Llama-8B` for medical chain-of-thought reasoning.\n",
        "\n",
        "## Key Components:\n",
        "- **Model:** `unsloth/DeepSeek-R1-Distill-Llama-8B`\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- **Dataset:** 500 samples from `medical-o1-reasoning-SFT`\n",
        "- **Tools:**\n",
        "  - `Unsloth` (2x faster training)\n",
        "  - 4-bit quantization\n",
        "  - LoRA adapters\n",
        "- **Result:** 44-minute training resulting in concise medical reasoning with structured `<think>` outputs.\n",
        "\n",
        "## Performance Improvement:\n",
        "\n",
        "| **Metric**         | **Before Fine-Tuning** | **After Fine-Tuning** |\n",
        "|--------------------|------------------------|-----------------------|\n",
        "| **Response Length** | 450 words              | 150 words             |\n",
        "| **Reasoning Style** | Verbose                | Focused               |\n",
        "| **Answer Format**   | Bulleted               | Paragraph             |\n"
      ],
      "metadata": {
        "id": "bs2w04WH_Evr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step-by-step  fine-tune DeepSeek-R1-Distill-Llama-8B on medical data"
      ],
      "metadata": {
        "id": "6cURM5LRAw-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "qXEdvYFJBWFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zwPSzhO-o5A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The '%%capture' magic command in Jupyter notebooks suppresses output from subsequent cells.\n",
        "\n",
        "!pip install kaggle\n",
        "# Installs the 'kaggle' package using pip. Assumes pip is installed and configured.\n",
        "\n",
        "!pip install unsloth\n",
        "# Installs the 'unsloth' package using pip. Similar assumption as above.\n",
        "\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "# Installs a specific version of the 'unsloth' package directly from its GitHub repository.\n",
        "# '--force-reinstall': Forces reinstallation even if the package is already installed.\n",
        "# '--no-cache-dir': Avoids caching the installation files.\n",
        "# '--no-deps': Skips installing dependencies, useful if dependencies are already satisfied.\n",
        "# 'git+https://github.com/unslothai/unsloth.git': GitHub repository URL from which to install the package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2: Authentication in Google Colab**"
      ],
      "metadata": {
        "id": "waLclZRYIT1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "# Imports the 'login' function from the 'huggingface_hub' package to authenticate with Hugging Face.\n",
        "\n",
        "from google.colab import userdata\n",
        "# Imports 'userdata' from Google Colab, which allows access to stored secrets or credentials.\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "# Gets the Hugging Face authentication token stored in Google Colab's 'userdata' for secure access.\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(hf_token)\n",
        "# Uses the retrieved token to authenticate the user with Hugging Face's hub."
      ],
      "metadata": {
        "id": "UmIYt48ru6Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# Imports the 'wandb' library, which is used for experiment tracking and logging in machine learning.\n",
        "\n",
        "from google.colab import userdata\n",
        "# Imports 'userdata' from Google Colab to access stored secrets or credentials.\n",
        "\n",
        "# Retrieve the Weights & Biases (W&B) API token from Colab secrets\n",
        "wb_token = userdata.get('wandb')\n",
        "# Gets the stored W&B API key to authenticate with the W&B platform.\n",
        "\n",
        "wandb.login(key=wb_token)\n",
        "# Logs into W&B using the retrieved API key for tracking experiments.\n",
        "\n",
        "# Initialize a new W&B run for experiment tracking\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset',  # Specifies the W&B project name\n",
        "    job_type=\"training\",  # Labels this run as a training job\n",
        "    anonymous=\"allow\"  # Allows anonymous logging if authentication isn't provided\n",
        ")\n"
      ],
      "metadata": {
        "id": "VplvaogkwrXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3: Model Initialization**"
      ],
      "metadata": {
        "id": "rUN1Y4u1BuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# Imports the 'FastLanguageModel' class from the 'unsloth' library, which is optimized for efficient language model training and inference.\n",
        "\n",
        "# Define model configuration parameters\n",
        "max_seq_length = 2048  # Sets the maximum sequence length for the model.\n",
        "dtype = None  # Specifies the data type for model computation (None means the default type will be used).\n",
        "load_in_4bit = True  # Enables 4-bit quantization for reduced memory usage and faster inference.\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Specifies the model to be loaded from Hugging Face Hub.\n",
        "    max_seq_length=max_seq_length,  # Uses the defined max sequence length.\n",
        "    dtype=dtype,  # Uses the specified data type (None defaults to the modelâ€™s recommended type).\n",
        "    load_in_4bit=load_in_4bit,  # Enables 4-bit quantization if True.\n",
        "    token=hf_token,  # Uses the Hugging Face authentication token to access private models if necessary.\n",
        ")"
      ],
      "metadata": {
        "id": "GNG2D8WKBvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.Inference to model Model**"
      ],
      "metadata": {
        "id": "ChbEiUQDJiTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the prompt format for inference\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\"\n",
        "\n",
        "# Define the medical question for inference\n",
        "question = (\n",
        "    \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \"\n",
        "    \"but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would \"\n",
        "    \"cystometry most likely reveal about her residual volume and detrusor contractions?\"\n",
        ")\n",
        "\n",
        "# Prepare the model for inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the input prompt and move it to the GPU for efficient processing\n",
        "inputs = tokenizer(\n",
        "    [prompt_style.format(question, \"\")],  # Format the prompt with the question\n",
        "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "    padding=True,  # Ensure proper padding for batch processing\n",
        "    truncation=True  # Prevent overly long inputs from causing issues\n",
        ").to(\"cuda\")  # Move tensors to GPU\n",
        "\n",
        "# Generate model output based on the input prompt\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,  # Input token IDs\n",
        "    attention_mask=inputs.attention_mask,  # Attention mask for proper token processing\n",
        "    max_new_tokens=1200,  # Limit the response length to avoid excessive output\n",
        "    use_cache=True,  # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output into a human-readable format\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Extract and print the response portion after \"### Response:\"\n",
        "print(response[0].split(\"### Response:\")[1].strip())"
      ],
      "metadata": {
        "id": "YKgtOmN3JvNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.Dataset Preparation for Fine-Tuning**"
      ],
      "metadata": {
        "id": "OsXsS01hK_UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt style for training data\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "\"\"\"\n",
        "\n",
        "# Retrieve the End-of-Sequence (EOS) token from the tokenizer\n",
        "EOS_TOKEN = tokenizer.eos_token  # Ensures each generated text ends properly\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Formats input data into the specified prompt style for training.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary containing \"Question\", \"Complex_CoT\", and \"Response\".\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with formatted text prompts.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"text\": [\n",
        "            train_prompt_style.format(question, cot, response) + EOS_TOKEN\n",
        "            for question, cot, response in zip(examples[\"Question\"], examples[\"Complex_CoT\"], examples[\"Response\"])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Load the dataset from Hugging Face's Hub\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "    \"en\",\n",
        "    split=\"train[0:500]\",  # Selects the first 500 training examples\n",
        "    trust_remote_code=True  # Allows loading datasets with external code dependencies\n",
        ")\n",
        "\n",
        "# Apply formatting function to dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Display the first formatted training example\n",
        "dataset[\"text\"][0]"
      ],
      "metadata": {
        "id": "DcFTcPM3ymTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.Applying LoRA for Parameter-Efficient Fine-Tuning (PEFT) on Large Models**"
      ],
      "metadata": {
        "id": "HyaO0_X0LRUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Parameter-Efficient Fine-Tuning (PEFT) to the model using LoRA (Low-Rank Adaptation)\n",
        "# This allows fine-tuning large models with fewer resources by only updating a small subset of parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The pre-trained model to which LoRA will be applied\n",
        "    r=16,  # Rank of the low-rank matrices used in LoRA. Higher values increase capacity but also computational cost.\n",
        "           # Suggested values: 8, 16, 32, 64, 128. Choose based on your task and resources.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # List of model layers to apply LoRA to.\n",
        "                                                           # These are typically attention and feed-forward layers.\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Controls the magnitude of updates.\n",
        "                    # A higher value increases the impact of LoRA updates.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA layers. Set to 0 for optimal performance.\n",
        "                     # Dropout can help prevent overfitting but is not necessary here.\n",
        "    bias=\"none\",  # Whether to include bias terms in LoRA. \"none\" is optimized for efficiency.\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enables gradient checkpointing to save memory during training.\n",
        "                                           # \"unsloth\" is optimized for very long sequences and reduces VRAM usage by 30%.\n",
        "    random_state=3407,  # Random seed for reproducibility. Ensures consistent results across runs.\n",
        "    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (RS-LoRA). Set to False by default.\n",
        "    loftq_config=None,  # Configuration for LoftQ (if applicable). Set to None as it is not used here.\n",
        ")"
      ],
      "metadata": {
        "id": "QSiQzVRrjej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.Supervised Fine-Tuning with Optimized Training for Large Models**"
      ],
      "metadata": {
        "id": "sPmYEzQYCbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "# Imports the 'SFTTrainer' class from the 'trl' (Transformer Reinforcement Learning) library,\n",
        "# which is designed to facilitate supervised fine-tuning (SFT) of models.\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "# Imports the 'TrainingArguments' class from the 'transformers' library, which contains configurations for model training.\n",
        "\n",
        "from unsloth import is_bfloat16_supported\n",
        "# Imports the 'is_bfloat16_supported' function from the 'unsloth' library, which checks if bfloat16 precision is supported.\n",
        "\n",
        "# Initialize the SFTTrainer for fine-tuning the model\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Specifies the pre-trained model to be fine-tuned.\n",
        "    tokenizer=tokenizer,  # The tokenizer used for tokenizing inputs and outputs during training.\n",
        "    train_dataset=dataset,  # The training dataset containing the formatted examples.\n",
        "    dataset_text_field=\"text\",  # Defines the column from the dataset that contains the text data (prompts).\n",
        "    max_seq_length=max_seq_length,  # Maximum length of sequences to feed into the model during training.\n",
        "    dataset_num_proc=2,  # Number of processes to use for data processing. Speed up data preparation.\n",
        "\n",
        "    # Define training hyperparameters using the TrainingArguments class\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # Batch size per device for training. Controls the number of samples processed in parallel.\n",
        "        gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights.\n",
        "                                        # Helps simulate larger batch sizes without needing additional GPU memory.\n",
        "        # For full training runs, set num_train_epochs and warmup_ratio:\n",
        "        warmup_steps=5,  # Number of steps for the learning rate warm-up.\n",
        "        max_steps=60,  # Total number of training steps. Limits the training process to 60 steps.\n",
        "        learning_rate=2e-4,  # Initial learning rate for the optimizer.\n",
        "        fp16=not is_bfloat16_supported(),  # Use FP16 (half-precision) training if bfloat16 is not supported.\n",
        "        bf16=is_bfloat16_supported(),  # Use BF16 (bfloat16) precision if supported by the hardware.\n",
        "        logging_steps=10,  # Log the training progress every 10 steps.\n",
        "        optim=\"adamw_8bit\",  # Use the AdamW optimizer with 8-bit precision for efficiency.\n",
        "        weight_decay=0.01,  # Weight decay applied to prevent overfitting.\n",
        "        lr_scheduler_type=\"linear\",  # Linear learning rate scheduler (gradual decay of learning rate).\n",
        "        seed=3407,  # Random seed for reproducibility.\n",
        "        output_dir=\"outputs\",  # Directory to save the model outputs (checkpoints, logs).\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "50rz0OaDCd89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        ""
      ],
      "metadata": {
        "id": "9m3VzTLPMfrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Start Training**"
      ],
      "metadata": {
        "id": "5RfyuKQLC2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "# Initiates the training process using the 'train' method of the SFTTrainer class.\n",
        "# This method trains the model using the specified dataset, hyperparameters, and training configurations\n",
        "# defined earlier in the 'trainer' object. It returns the training statistics, such as loss and accuracy,\n",
        "# which are stored in the 'trainer_stats' variable for further analysis or logging."
      ],
      "metadata": {
        "id": "No8CPQAmCz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        ""
      ],
      "metadata": {
        "id": "52sI-PGdMobZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. **Inference After Fine-Tuning**"
      ],
      "metadata": {
        "id": "FqbIYM0rMwCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Define the format for the inference prompt\n",
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "# Define the medical question for inference\n",
        "question = (\n",
        "    \"A 45-year-old man with a history of alcohol use, who has been abstinent \"\n",
        "    \"for the past 10 years, presents with sudden onset dysarthria, shuffling gait, \"\n",
        "    \"and intention tremors. Given this clinical presentation and history, what is the most likely diagnosis?\"\n",
        ")\n",
        "\n",
        "# Prepare the model for inference mode (prepares model for generating responses)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the formatted prompt and move the tensors to GPU for efficient processing\n",
        "inputs = tokenizer(\n",
        "    [prompt_format.format(question=question, response=\"\")],  # Format the prompt with the medical question\n",
        "    return_tensors=\"pt\",  # Return the tokenized inputs as PyTorch tensors\n",
        "    padding=True,  # Ensure proper padding for batch processing\n",
        "    truncation=True  # Prevent excessively long inputs from causing issues\n",
        ").to(\"cuda\")  # Move tensors to GPU for faster computation\n",
        "\n",
        "# Initialize the TextStreamer to stream the output token-by-token\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the model's output, streaming tokens in real-time as they are generated\n",
        "_ = model.generate(\n",
        "    **inputs,  # Provide the tokenized input text to the model for processing\n",
        "    streamer=text_streamer,  # Enable token-by-token streaming to show real-time output\n",
        "    max_new_tokens=1200  # Limit the response length to avoid excessively long outputs\n",
        ")"
      ],
      "metadata": {
        "id": "_guzLAGFNJkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.Saving, loading finetuned models**\n",
        "To save the final model as LoRA adapters"
      ],
      "metadata": {
        "id": "AhMB-1lmC9es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model locally to a directory called \"lora_model\"\n",
        "# This allows for later use of the fine-tuned model without retraining\n",
        "model.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Save the tokenizer associated with the model locally to the same \"lora_model\" directory\n",
        "# The tokenizer is essential for processing inputs in the same way as during training\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "ZyCx4pHvC708"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}