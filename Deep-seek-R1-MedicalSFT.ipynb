{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMXyDZEJbpJGfFLIhlJs9RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Deep-seek-R1-MedicalSFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "## Objective:\n",
        "Adapt `DeepSeek-R1-Distill-Llama-8B` for medical chain-of-thought reasoning.\n",
        "\n",
        "## Key Components:\n",
        "- **Model:** `unsloth/DeepSeek-R1-Distill-Llama-8B`\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- **Dataset:** 500 samples from `medical-o1-reasoning-SFT`\n",
        "- **Tools:**\n",
        "  - `Unsloth` (2x faster training)\n",
        "  - 4-bit quantization\n",
        "  - LoRA adapters\n",
        "- **Result:** 44-minute training resulting in concise medical reasoning with structured `<think>` outputs.\n",
        "\n",
        "## Performance Improvement:\n",
        "\n",
        "| **Metric**         | **Before Fine-Tuning** | **After Fine-Tuning** |\n",
        "|--------------------|------------------------|-----------------------|\n",
        "| **Response Length** | 450 words              | 150 words             |\n",
        "| **Reasoning Style** | Verbose                | Focused               |\n",
        "| **Answer Format**   | Bulleted               | Paragraph             |\n"
      ],
      "metadata": {
        "id": "bs2w04WH_Evr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step-by-step  fine-tune DeepSeek-R1-Distill-Llama-8B on medical data"
      ],
      "metadata": {
        "id": "6cURM5LRAw-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup"
      ],
      "metadata": {
        "id": "qXEdvYFJBWFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zwPSzhO-o5A"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Authentication"
      ],
      "metadata": {
        "id": "rnn7hFVTBj-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "LR-AWYUsBmMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Initialization"
      ],
      "metadata": {
        "id": "rUN1Y4u1BuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "GNG2D8WKBvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dataset Preparation"
      ],
      "metadata": {
        "id": "buQLgKLUB-Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "    \"en\",\n",
        "    split=\"train[0:500]\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "bUAL5vWdCA-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prompt Formatting"
      ],
      "metadata": {
        "id": "x_Rb8qA_CMxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for q, cot, ans in zip(examples[\"Question\"], examples[\"Complex_CoT\"], examples[\"Response\"]):\n",
        "        text = f\"\"\"Below is an instruction... [truncated prompt template]\"\"\" + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "4Gg1eZkTCJBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. LoRA Configuration\n",
        "\n"
      ],
      "metadata": {
        "id": "5AoQntp6CSt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16\n",
        ")"
      ],
      "metadata": {
        "id": "DeBCLX1yCUps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Setup"
      ],
      "metadata": {
        "id": "sPmYEzQYCbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        max_steps=60,\n",
        "        fp16=True,\n",
        "        output_dir=\"outputs\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "50rz0OaDCd89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Start Training"
      ],
      "metadata": {
        "id": "5RfyuKQLC2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "No8CPQAmCz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save & Deploy"
      ],
      "metadata": {
        "id": "AhMB-1lmC9es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally\n",
        "model.save_pretrained_merged(\"DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Push to Hub\n",
        "model.push_to_hub_merged(\"username/DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")"
      ],
      "metadata": {
        "id": "ZyCx4pHvC708"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}