{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCmMEZXuw2K910QWU5AF0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Deep-seek-R1-MedicalSFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "## Objective:\n",
        "Adapt `DeepSeek-R1-Distill-Llama-8B` for medical chain-of-thought reasoning.\n",
        "\n",
        "## Key Components:\n",
        "- **Model:** `unsloth/DeepSeek-R1-Distill-Llama-8B`\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "- **Dataset:** 500 samples from `medical-o1-reasoning-SFT`\n",
        "- **Tools:**\n",
        "  - `Unsloth` (2x faster training)\n",
        "  - 4-bit quantization\n",
        "  - LoRA adapters\n",
        "- **Result:** 44-minute training resulting in concise medical reasoning with structured `<think>` outputs.\n",
        "\n",
        "## Performance Improvement:\n",
        "\n",
        "| **Metric**         | **Before Fine-Tuning** | **After Fine-Tuning** |\n",
        "|--------------------|------------------------|-----------------------|\n",
        "| **Response Length** | 450 words              | 150 words             |\n",
        "| **Reasoning Style** | Verbose                | Focused               |\n",
        "| **Answer Format**   | Bulleted               | Paragraph             |\n"
      ],
      "metadata": {
        "id": "bs2w04WH_Evr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step-by-step  fine-tune DeepSeek-R1-Distill-Llama-8B on medical data"
      ],
      "metadata": {
        "id": "6cURM5LRAw-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1: Install All the Required Packages"
      ],
      "metadata": {
        "id": "qXEdvYFJBWFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zwPSzhO-o5A"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Initialization"
      ],
      "metadata": {
        "id": "rUN1Y4u1BuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the FastLanguageModel class from the unsloth library\n",
        "# This library is likely optimized for efficient language model operations\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load a pre-trained language model and its corresponding tokenizer\n",
        "# The model being loaded is \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "# This is a distilled version of the Llama 8B model, optimized for faster inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Name of the pre-trained model\n",
        "    max_seq_length=2048,  # Maximum sequence length the model can handle\n",
        "    load_in_4bit=True,  # Load the model in 4-bit precision for memory efficiency\n",
        "    token=hf_token  # Hugging Face token for authentication (e.g., for private models)\n",
        ")"
      ],
      "metadata": {
        "id": "GNG2D8WKBvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wuZxT-lJkC_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Parameter-Efficient Fine-Tuning (PEFT) to the model using LoRA (Low-Rank Adaptation)\n",
        "# This allows fine-tuning large models with fewer resources by only updating a small subset of parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The pre-trained model to which LoRA will be applied\n",
        "    r=16,  # Rank of the low-rank matrices used in LoRA. Higher values increase capacity but also computational cost.\n",
        "           # Suggested values: 8, 16, 32, 64, 128. Choose based on your task and resources.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # List of model layers to apply LoRA to.\n",
        "                                                           # These are typically attention and feed-forward layers.\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Controls the magnitude of updates.\n",
        "                    # A higher value increases the impact of LoRA updates.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA layers. Set to 0 for optimal performance.\n",
        "                     # Dropout can help prevent overfitting but is not necessary here.\n",
        "    bias=\"none\",  # Whether to include bias terms in LoRA. \"none\" is optimized for efficiency.\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enables gradient checkpointing to save memory during training.\n",
        "                                           # \"unsloth\" is optimized for very long sequences and reduces VRAM usage by 30%.\n",
        "    random_state=3407,  # Random seed for reproducibility. Ensures consistent results across runs.\n",
        "    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (RS-LoRA). Set to False by default.\n",
        "    loftq_config=None,  # Configuration for LoftQ (if applicable). Set to None as it is not used here.\n",
        ")"
      ],
      "metadata": {
        "id": "QSiQzVRrjej7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dataset Preparation"
      ],
      "metadata": {
        "id": "buQLgKLUB-Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "    \"en\",\n",
        "    split=\"train[0:500]\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "bUAL5vWdCA-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prompt Formatting"
      ],
      "metadata": {
        "id": "x_Rb8qA_CMxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for q, cot, ans in zip(examples[\"Question\"], examples[\"Complex_CoT\"], examples[\"Response\"]):\n",
        "        text = f\"\"\"Below is an instruction... [truncated prompt template]\"\"\" + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "4Gg1eZkTCJBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. LoRA Configuration\n",
        "\n"
      ],
      "metadata": {
        "id": "5AoQntp6CSt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16\n",
        ")"
      ],
      "metadata": {
        "id": "DeBCLX1yCUps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Setup"
      ],
      "metadata": {
        "id": "sPmYEzQYCbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        max_steps=60,\n",
        "        fp16=True,\n",
        "        output_dir=\"outputs\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "50rz0OaDCd89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Start Training"
      ],
      "metadata": {
        "id": "5RfyuKQLC2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "No8CPQAmCz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save & Deploy"
      ],
      "metadata": {
        "id": "AhMB-1lmC9es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally\n",
        "model.save_pretrained_merged(\"DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Push to Hub\n",
        "model.push_to_hub_merged(\"username/DeepSeek-R1-Medical-COT\", tokenizer, save_method=\"merged_16bit\")"
      ],
      "metadata": {
        "id": "ZyCx4pHvC708"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}