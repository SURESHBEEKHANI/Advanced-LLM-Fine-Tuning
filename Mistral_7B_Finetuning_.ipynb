{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNm9Z+peYiW2l9GCDR02DTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Mistral_7B_Finetuning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sGCE5w5WA1h",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# - accelerate: A library to optimize and accelerate deep learning training and inference, especially on distributed systems.\n",
        "#   It simplifies the use of mixed-precision training, model parallelism, and multi-GPU systems.\n",
        "\n",
        "# - peft: A library for Parameter-Efficient Fine-Tuning (PEFT) of large language models.\n",
        "#   It provides tools to fine-tune pre-trained models with fewer parameters, saving memory and computational resources.\n",
        "\n",
        "# - bitsandbytes: A lightweight library for 8-bit optimizers and memory-efficient GPU computation.\n",
        "#   This library helps reduce memory usage during training by using low-precision (8-bit) computations, making large models more feasible on limited hardware.\n",
        "\n",
        "# - trl: The Transformers Reinforcement Learning library for fine-tuning models with reinforcement learning techniques.\n",
        "#   It extends the Hugging Face Transformers library by providing methods for training models using reinforcement learning (RL).\n",
        "\n",
        "# - py7zr: A library to handle .7z archive files, used for extracting compressed data.\n",
        "#   It allows extracting and managing `.7z` archives, often used to store large datasets or model weights in a compressed format.\n",
        "\n",
        "# - auto-gptq: A library for efficient quantization of models for lower hardware requirements.\n",
        "#   This library facilitates the quantization of large models, reducing their size and computational requirements, making them suitable for deployment on edge devices.\n",
        "\n",
        "# - optimum: A library providing performance optimizations for models, especially for Hugging Face models on specific hardware.\n",
        "#   It offers various optimizations to enhance model inference speed, especially on hardware like GPUs or TPUs.\n",
        "\n",
        "# - transformers: A popular library by Hugging Face for working with pre-trained transformer models.\n",
        "#   It provides easy access to a wide range of state-of-the-art transformer models for natural language processing tasks.\n",
        "\n",
        "# Install all these libraries in one command:\n",
        "!pip install accelerate peft bitsandbytes trl py7zr auto-gptq optimum transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "q6dNiNUJd9-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the PyTorch library, which provides tools for tensor operations and deep learning models.\n",
        "import torch\n",
        "\n",
        "# Importing the `notebook_login` function from huggingface_hub to facilitate authentication with the Hugging Face Hub.\n",
        "# This allows you to upload or access models and datasets stored on the Hugging Face platform.\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Importing the `load_dataset` function from the `datasets` library to load datasets from the Hugging Face Hub or local files.\n",
        "# The `Dataset` class is also imported to work with datasets once they are loaded.\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Importing the `prepare_model_for_kbit_training` function from the PEFT (Parameter-Efficient Fine-Tuning) library.\n",
        "# This function prepares models for efficient fine-tuning with lower precision (k-bit) to reduce memory usage and computational cost.\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Importing the `LoraConfig` class from the PEFT library.\n",
        "# This class is used for configuring the LoRA (Low-Rank Adaptation) technique, which is an efficient method for fine-tuning large models with fewer parameters.\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Importing the `get_peft_model` function from the PEFT library.\n",
        "# This function helps create a parameter-efficient fine-tuning model by applying techniques like LoRA.\n",
        "from peft import get_peft_model\n",
        "\n",
        "# Importing the `SFTTrainer` class from the trl (Transformers Reinforcement Learning) library.\n",
        "# `SFTTrainer` is used to fine-tune transformer models with supervised fine-tuning techniques, integrating reinforcement learning for better performance.\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Importing the `TrainingArguments` class from the Hugging Face `transformers` library.\n",
        "# This class is used to configure the settings for training transformer models, such as batch size, learning rate, and logging options.\n",
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "JJgJAt-cexzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "5UX5fekWgPHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Loading the SAMSum Dataset**\n",
        "\n",
        "In this section, we load the **SAMSum dataset** using the Hugging Face `datasets` library. The SAMSum dataset is designed for dialogue summarization tasks, where the goal is to summarize conversations into concise summaries."
      ],
      "metadata": {
        "id": "FGahEyL5pYWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SAMSum dataset using the Hugging Face `datasets` library.\n",
        "# \"samsum\" is the name of the dataset, which is designed for dialogue summarization tasks.\n",
        "# The \"split\" parameter specifies the portion of the dataset to load.\n",
        "# In this case, \"train\" indicates we are loading the training data.\n",
        "df = load_dataset(\"samsum\", split=\"train\")"
      ],
      "metadata": {
        "id": "-UXXLjgRgs6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the loaded dataset (Hugging Face Dataset object) to a Pandas DataFrame.\n",
        "# The `to_pandas()` method transforms the dataset into a DataFrame,\n",
        "# which is a tabular data structure commonly used for data analysis in Python.\n",
        "# This allows us to leverage Pandas' powerful data manipulation and analysis capabilities.\n",
        "data_df = df.to_pandas()"
      ],
      "metadata": {
        "id": "_O9NivxchGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the DataFrame\n",
        "# The `shape` attribute returns a tuple representing the dimensions of the DataFrame:\n",
        "# - The first value is the number of rows (examples in the dataset).\n",
        "# - The second value is the number of columns (features in the dataset).\n",
        "data_df.shape"
      ],
      "metadata": {
        "id": "5YNCzbOyqKh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 7361 rows from the 'data_df' DataFrame and assign it to the 'data' variable.\n",
        "# This operation selects a random subset of the rows without replacement.\n",
        "data = data_df.sample(7361)"
      ],
      "metadata": {
        "id": "FwpUTPrPjoZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'text' in the 'data' DataFrame.\n",
        "data[\"text\"] = data[[\"dialogue\", \"summary\"]].apply(\n",
        "    lambda x:\n",
        "    # Define a custom string format where:\n",
        "    # - \"###Human:\" introduces the dialogue for summarization.\n",
        "    # - \"###Assistant:\" provides the summary.\n",
        "    \"###Human: Summarize this following dialogue: \" + x[\"dialogue\"] + \"\\n###Assistant: \" + x[\"summary\"],\n",
        "    axis=1  # Ensure the lambda function operates row-wise across the DataFrame.\n",
        ")"
      ],
      "metadata": {
        "id": "lYVYRz4Ij4dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape (number of rows and columns) of the 'data' DataFrame.\n",
        "# 'data.shape' returns a tuple: (number_of_rows, number_of_columns).\n",
        "data.shape"
      ],
      "metadata": {
        "id": "y2TVFm6TotVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'data' DataFrame (pandas) into a Hugging Face 'Dataset' object.\n",
        "# This is useful for working with datasets in machine learning workflows,\n",
        "# as the 'Dataset' object provides additional functionality and compatibility with Hugging Face tools.\n",
        "data = Dataset.from_pandas(data)"
      ],
      "metadata": {
        "id": "50tiLCGerEl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "fWhe6bpMrGxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the Quantized Model (GPTQ) and Tokenizer for Preprocessing**\n",
        "\n",
        "we will load a quantized version of the `\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"` model from Hugging Face, along with its corresponding tokenizer for preprocessing."
      ],
      "metadata": {
        "id": "jwdTIL48yYac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules from the Hugging Face transformers library:\n",
        "from transformers import (\n",
        "    # AutoModelForCausalLM: Automatically loads a pre-trained model for causal language modeling (e.g., GPT models).\n",
        "    AutoModelForCausalLM,\n",
        "\n",
        "    # AutoTokenizer: Automatically loads the appropriate tokenizer corresponding to the model architecture.\n",
        "    AutoTokenizer,\n",
        "\n",
        "    # GPTQConfig: Loads configuration settings for the GPTQ (quantized) model.\n",
        "    GPTQConfig,\n",
        "\n",
        "    # TrainingArguments: Provides arguments for configuring model training, such as learning rate, batch size, and more.\n",
        "    TrainingArguments\n",
        ")\n"
      ],
      "metadata": {
        "id": "Rbn8aa_QvGOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for the specified pre-trained model ('TheBloke/Mistral-7B-Instruct-v0.1-GPTQ')\n",
        "# using the AutoTokenizer class from Hugging Face.\n",
        "# The tokenizer is used to process and encode input text before feeding it into the model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")"
      ],
      "metadata": {
        "id": "3VGQd0qIvoJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "wJH-RXyUzDO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token=tokenizer.eos_token"
      ],
      "metadata": {
        "id": "recBdzvJzLRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a configuration object for GPTQ (a method for quantization of models)\n",
        "quantization_config_loading = GPTQConfig(\n",
        "    # Set the number of bits for quantization to 4\n",
        "    bits=4,\n",
        "\n",
        "    # Disable exllama, which is an efficient transformer implementation, during quantization\n",
        "    disable_exllama=True,\n",
        "\n",
        "    # Pass the tokenizer that will be used for tokenizing input data\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "gPYU0Unoz0TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained causal language model using the AutoModelForCausalLM class from Hugging Face's transformers library\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    # Specify the model identifier for the pretrained model (Mistral-7B-Instruct-v0.1-GPTQ)\n",
        "    \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
        "\n",
        "    # Pass the previously defined quantization configuration (which includes 4-bit quantization and tokenizer)\n",
        "    quantization_config=quantization_config_loading,\n",
        "\n",
        "    # Automatically determine the optimal device (CPU or GPU) for model loading and computation\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "RquRypyOz_hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "GMPAAcMA0Qoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable the cache mechanism during model inference to prevent storing activations\n",
        "# This can help reduce memory usage at the cost of potentially slower performance\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Set the pretraining tensor parallelism (pretraining_tp) value to 1\n",
        "# This controls the degree of parallelism used during model training for distributed training.\n",
        "# Setting it to 1 means no tensor parallelism will be used (it could affect training speed and efficiency in multi-GPU setups)\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Enable gradient checkpointing for the model\n",
        "# Gradient checkpointing reduces memory usage during backpropagation by saving intermediate activations\n",
        "# and recomputing them during the backward pass instead of storing them in memory\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "FDBa7w1E0uh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-Tuning Techniques for Pretrained Models**"
      ],
      "metadata": {
        "id": "gJ6GS1Zn2EUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary functions and classes from the PEFT library (Parameter-Efficient Fine-Tuning)\n",
        "from peft import prepare_model_for_kbit_training  # Function to prepare a model for training with low-bit precision (e.g., 4-bit or 8-bit training)\n",
        "from peft import LoraConfig  # Configuration class for LoRA (Low-Rank Adaptation) to define hyperparameters for PEFT methods\n",
        "from peft import get_peft_model  # Function to apply PEFT methods (e.g., LoRA) to a model for efficient fine-tuning\n",
        "\n",
        "# Importing the SFTTrainer class from the TRL (Transformer Reinforcement Learning) library\n",
        "from trl import SFTTrainer  # Trainer class for supervised fine-tuning, which helps fine-tune language models efficiently\n",
        "\n",
        "# Importing TrainingArguments from Hugging Face's transformers library for setting training configurations\n",
        "from transformers import TrainingArguments  # Class to define training parameters such as learning rate, batch size, epochs, etc.\n"
      ],
      "metadata": {
        "id": "vUWm-ifv2SzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "11dsakBt3IKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the LoraConfig class for Low-Rank Adaptation (LoRA) with specific hyperparameters\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                    # The rank of the low-rank matrices. A higher rank allows the model to capture more complex patterns. Typically, values between 8 and 32 work well.\n",
        "    lora_alpha=16,           # Scaling factor for the LoRA matrices. It controls the effect of the low-rank adaptation on the model’s weights.\n",
        "    lora_dropout=0.05,       # Dropout rate applied during training to the LoRA layers. Helps prevent overfitting by randomly dropping units from the layer.\n",
        "    bias=\"none\",             # Specifies whether biases are included in the LoRA layers. \"none\" means no bias terms will be added to the LoRA modules.\n",
        "    task_type=\"CAUSAL_LM\",   # Defines the task type for which the model is fine-tuned. \"CAUSAL_LM\" refers to causal language modeling (used in autoregressive tasks like text generation).\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Specifies which parts of the model will have LoRA applied. Here, it targets the query (`q_proj`) and value (`v_proj`) projection layers in the attention mechanism of a transformer model.\n",
        ")"
      ],
      "metadata": {
        "id": "Be_aWBVG3iVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the PEFT (Parameter-Efficient Fine-Tuning) method to the model using the defined LoRA configuration.\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "851qLvJ03wkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments for the fine-tuning process using Hugging Face's TrainingArguments class.\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"mistral-finetuned-samsum\",  # Directory where the model checkpoints and outputs will be saved.\n",
        "    per_device_train_batch_size=8,          # Batch size for training on each device (e.g., GPU). In this case, 8 samples per batch.\n",
        "    gradient_accumulation_steps=1,          # Number of steps to accumulate gradients before performing a backward pass. In this case, no gradient accumulation.\n",
        "    optim=\"paged_adamw_32bit\",              # Optimizer to be used for training. Here, \"paged_adamw_32bit\" is an AdamW variant optimized for memory usage.\n",
        "    learning_rate=2e-4,                     # The learning rate for the optimizer. In this case, it is set to 0.0002.\n",
        "    lr_scheduler_type=\"cosine\",             # The type of learning rate scheduler. \"cosine\" means the learning rate will follow a cosine decay during training.\n",
        "    save_strategy=\"epoch\",                  # The strategy to save model checkpoints. \"epoch\" means the model will be saved after each epoch.\n",
        "    logging_steps=100,                      # Number of steps between each logging event. Here, it logs every 100 steps during training.\n",
        "    num_train_epochs=1,                     # The total number of training epochs. In this case, the model will be trained for 1 epoch.\n",
        "    max_steps=250,                          # Maximum number of training steps. This can be used to limit the total number of updates if you don't want to train for the full number of epochs.\n",
        "    fp16=True,                              # Enable mixed-precision training (float16) to speed up training and reduce memory usage, particularly on compatible GPUs.\n",
        ")"
      ],
      "metadata": {
        "id": "PI1x7lRa4MxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFTTrainer (Supervised Fine-Tuning Trainer) for fine-tuning the model with the specified configuration.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,                       # The model to be fine-tuned, which has been adapted with PEFT (e.g., LoRA).\n",
        "    train_dataset=data,                 # The dataset used for training. This dataset contains the data that the model will learn from.\n",
        "    peft_config=peft_config,            # The PEFT configuration (e.g., LoRA) that specifies the fine-tuning method and parameters.\n",
        "    dataset_text_field=\"text\",          # The field in the dataset that contains the text data (typically \"text\" or \"input_ids\").\n",
        "    args=training_arguments,            # The training arguments that define the training process (batch size, learning rate, etc.).\n",
        "    tokenizer=tokenizer,                # The tokenizer used to convert text into input format for the model and vice versa.\n",
        "    packing=False,                      # Whether to pack the sequences into a fixed length. Setting it to False means no packing.\n",
        "    max_seq_length=512                  # The maximum length of the input sequences. Sequences longer than this will be truncated.\n",
        ")"
      ],
      "metadata": {
        "id": "slitfel74b5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}