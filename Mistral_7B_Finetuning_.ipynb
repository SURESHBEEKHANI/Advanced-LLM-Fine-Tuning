{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+SN1VdXsVy+BUs44Ilazn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Mistral_7B_Finetuning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sGCE5w5WA1h",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# - accelerate: A library to optimize and accelerate deep learning training and inference, especially on distributed systems.\n",
        "#   It simplifies the use of mixed-precision training, model parallelism, and multi-GPU systems.\n",
        "\n",
        "# - peft: A library for Parameter-Efficient Fine-Tuning (PEFT) of large language models.\n",
        "#   It provides tools to fine-tune pre-trained models with fewer parameters, saving memory and computational resources.\n",
        "\n",
        "# - bitsandbytes: A lightweight library for 8-bit optimizers and memory-efficient GPU computation.\n",
        "#   This library helps reduce memory usage during training by using low-precision (8-bit) computations, making large models more feasible on limited hardware.\n",
        "\n",
        "# - trl: The Transformers Reinforcement Learning library for fine-tuning models with reinforcement learning techniques.\n",
        "#   It extends the Hugging Face Transformers library by providing methods for training models using reinforcement learning (RL).\n",
        "\n",
        "# - py7zr: A library to handle .7z archive files, used for extracting compressed data.\n",
        "#   It allows extracting and managing `.7z` archives, often used to store large datasets or model weights in a compressed format.\n",
        "\n",
        "# - auto-gptq: A library for efficient quantization of models for lower hardware requirements.\n",
        "#   This library facilitates the quantization of large models, reducing their size and computational requirements, making them suitable for deployment on edge devices.\n",
        "\n",
        "# - optimum: A library providing performance optimizations for models, especially for Hugging Face models on specific hardware.\n",
        "#   It offers various optimizations to enhance model inference speed, especially on hardware like GPUs or TPUs.\n",
        "\n",
        "# - transformers: A popular library by Hugging Face for working with pre-trained transformer models.\n",
        "#   It provides easy access to a wide range of state-of-the-art transformer models for natural language processing tasks.\n",
        "\n",
        "# Install all these libraries in one command:\n",
        "!pip install accelerate peft bitsandbytes trl py7zr auto-gptq optimum transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "q6dNiNUJd9-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the PyTorch library, which provides tools for tensor operations and deep learning models.\n",
        "import torch\n",
        "\n",
        "# Importing the `notebook_login` function from huggingface_hub to facilitate authentication with the Hugging Face Hub.\n",
        "# This allows you to upload or access models and datasets stored on the Hugging Face platform.\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Importing the `load_dataset` function from the `datasets` library to load datasets from the Hugging Face Hub or local files.\n",
        "# The `Dataset` class is also imported to work with datasets once they are loaded.\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Importing the `prepare_model_for_kbit_training` function from the PEFT (Parameter-Efficient Fine-Tuning) library.\n",
        "# This function prepares models for efficient fine-tuning with lower precision (k-bit) to reduce memory usage and computational cost.\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Importing the `LoraConfig` class from the PEFT library.\n",
        "# This class is used for configuring the LoRA (Low-Rank Adaptation) technique, which is an efficient method for fine-tuning large models with fewer parameters.\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Importing the `get_peft_model` function from the PEFT library.\n",
        "# This function helps create a parameter-efficient fine-tuning model by applying techniques like LoRA.\n",
        "from peft import get_peft_model\n",
        "\n",
        "# Importing the `SFTTrainer` class from the trl (Transformers Reinforcement Learning) library.\n",
        "# `SFTTrainer` is used to fine-tune transformer models with supervised fine-tuning techniques, integrating reinforcement learning for better performance.\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Importing the `TrainingArguments` class from the Hugging Face `transformers` library.\n",
        "# This class is used to configure the settings for training transformer models, such as batch size, learning rate, and logging options.\n",
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "JJgJAt-cexzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "5UX5fekWgPHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}