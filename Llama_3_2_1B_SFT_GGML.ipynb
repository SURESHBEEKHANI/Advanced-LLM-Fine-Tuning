{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Llama_3_2_1B_SFT_GGML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede",
      "metadata": {
        "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede"
      },
      "source": [
        "### **Fine-Tuning Meta-Llama-3.2-3B Used unsloth for CPU and GPU Inference - GGML**\n",
        "\n",
        "On September 25, 2024, Meta introduced Llama 3.2, a collection of multilingual large language models (LLMs) in 1B and 3B sizes. These models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. Notably, the Llama 3.2 1B and 3B models support a context length of 128K tokens, making them suitable for extensive text processing tasks.\n",
        "HUGGING FACE\n",
        "\n",
        "To access the Llama 3.2-1B model, you can download it from [Hugging Face](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct) The approval process is typically swift, often taking about 20 minutes.\n",
        "HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a7bfcf-0552-433f-8afb-145e661ab34a",
      "metadata": {
        "id": "01a7bfcf-0552-433f-8afb-145e661ab34a"
      },
      "source": [
        "### Table of Contents\n",
        "1. Install dependancies\n",
        "2. Download model\n",
        "3. Fintuning flow\n",
        "4. convert GGML formate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a",
      "metadata": {
        "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a"
      },
      "source": [
        "## Step 1: Install All the Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bace79c2-2f05-4c2b-bbea-db212e93839d",
      "metadata": {
        "id": "bace79c2-2f05-4c2b-bbea-db212e93839d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a8fd97-b407-4642-fef4-7a81b0a8062e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import All the Required Libraries"
      ],
      "metadata": {
        "id": "lgU5P1iGA3F1"
      },
      "id": "lgU5P1iGA3F1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be21a74-407f-49a9-83e9-771cf90d29b1",
      "metadata": {
        "id": "2be21a74-407f-49a9-83e9-771cf90d29b1"
      },
      "outputs": [],
      "source": [
        "# Importing standard and third-party libraries\n",
        "import os  # Provides functions to interact with the operating system, e.g., managing paths and environment variables.\n",
        "import torch  # PyTorch library for tensor operations, model training, and deep learning tasks.\n",
        "\n",
        "# Importing utilities from the `datasets` library\n",
        "from datasets import load_dataset  # For loading datasets from Hugging Face's Datasets library.\n",
        "\n",
        "# Importing components from Hugging Face's Transformers library\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,  # For loading pre-trained causal language models.\n",
        "    AutoTokenizer,  # For tokenizing input text based on the model's requirements.\n",
        "    BitsAndBytesConfig,  # For configuring quantization (e.g., 8-bit weights) to optimize memory usage.\n",
        "    HfArgumentParser,  # A parser for handling command-line arguments.\n",
        "    TrainingArguments,  # Defines training parameters such as batch size, learning rate, etc.\n",
        "    pipeline,  # For creating pipelines (e.g., text generation, summarization).\n",
        "    logging,  # Provides tools for managing logging during training and evaluation.\n",
        ")\n",
        "\n",
        "# Importing utilities for parameter-efficient fine-tuning (PEFT)\n",
        "from peft import LoraConfig, PeftModel  # For configuring and applying LoRA (Low-Rank Adaptation) fine-tuning.\n",
        "\n",
        "# Importing utilities from the `trl` library\n",
        "from trl import SFTTrainer  # A trainer specifically designed for supervised fine-tuning of language models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67588314-d055-4403-b8d3-0f9662a3148e",
      "metadata": {
        "id": "67588314-d055-4403-b8d3-0f9662a3148e"
      },
      "outputs": [],
      "source": [
        "# Input access token\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fc909e-efb6-4b4b-8c10-e0ed72477600",
      "metadata": {
        "id": "a3fc909e-efb6-4b4b-8c10-e0ed72477600"
      },
      "source": [
        "## 2. Load in Llama 3 and our dataset\n",
        "\n",
        "Because we are using the base model, there is not an exact prompt template we have to follow. The dataset we are using follows LLama3's template format so it should be fine for downstream tasks that use the Llama3 chat format. If you're bringing your own data, you can format it however you want as long as you use the same formatting downstream.\n",
        "\n",
        "Here's the official [Llama3 chat template](https://huggingface.co/blog/llama3#how-to-prompt-llama-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b422e214-977a-4c45-9d66-b6a698c546eb",
      "metadata": {
        "id": "b422e214-977a-4c45-9d66-b6a698c546eb"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "dataset_name = \"scooterman/guanaco-llama3-1k\"\n",
        "new_model = \"suresh-llama/Llama-3.2-1B-SFT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a03b3f-5398-455e-880c-f503cba51821",
      "metadata": {
        "id": "c3a03b3f-5398-455e-880c-f503cba51821"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5081c400-f9c0-403b-b145-0e75e5a9982e",
      "metadata": {
        "id": "5081c400-f9c0-403b-b145-0e75e5a9982e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77919e07-7495-4a50-82c7-99459ba815c2",
      "metadata": {
        "id": "77919e07-7495-4a50-82c7-99459ba815c2"
      },
      "source": [
        "## 3. Set our Training Arguments\n",
        "\n",
        "A lot of tutorials simply paste a list of arguments leaving it up to the reader to figure out what each argument does. Below I've added comments which explain what each argument does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614e0a54-2740-438d-a72c-f3c0215c6558",
      "metadata": {
        "id": "614e0a54-2740-438d-a72c-f3c0215c6558"
      },
      "outputs": [],
      "source": [
        "# Output directory where the results and checkpoint are stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs - how many times does the model see the whole dataset\n",
        "num_train_epochs = 1 #Increase this for a larger finetune\n",
        "\n",
        "# Enable fp16/bf16 training. This is the type of each weight. Since we are on an A100\n",
        "# we can set bf16 to true because it can handle that type of computation\n",
        "bf16 = True\n",
        "\n",
        "# Batch size is the number of training examples used to train a single forward and backward pass.\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Gradients are accumulated over multiple mini-batches before updating the model weights.\n",
        "# This allows for effectively training with a larger batch size on hardware with limited memory\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# memory optimization technique that reduces RAM usage during training by intermittently storing\n",
        "# intermediate activations instead of retaining them throughout the entire forward pass, trading\n",
        "# computational time for lower memory consumption.\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 5\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 100\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba492aa-ad13-46d4-be7b-923240b6a133",
      "metadata": {
        "id": "0ba492aa-ad13-46d4-be7b-923240b6a133"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bec1455-c08d-4671-bb3c-48cf07ec86a8",
      "metadata": {
        "id": "7bec1455-c08d-4671-bb3c-48cf07ec86a8"
      },
      "source": [
        "## Run our training job using WandB for logging\n",
        "\n",
        "Weights and Biases is industry standard for monitoring and evaluating your training job. I highly suggest setting up an account to monitor this run and use it for future ML jobs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1250d948-e191-45a4-bd57-1fd9226666a0",
      "metadata": {
        "scrolled": true,
        "id": "1250d948-e191-45a4-bd57-1fd9226666a0"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57e00a2-a90a-4e43-a79d-2fe98dda58f6",
      "metadata": {
        "id": "f57e00a2-a90a-4e43-a79d-2fe98dda58f6"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93b0c17-77af-4328-889a-d418d0814101",
      "metadata": {
        "id": "c93b0c17-77af-4328-889a-d418d0814101"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6ad908-f10e-4605-bff3-efdfbe722a5c",
      "metadata": {
        "id": "3c6ad908-f10e-4605-bff3-efdfbe722a5c"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef576ca-fa8d-4045-8a75-3e74116d4abf",
      "metadata": {
        "id": "1ef576ca-fa8d-4045-8a75-3e74116d4abf"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}