{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "882f0508aaf34481b7bdad5dc37ae2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4d9f271b7a74e089a97d8bb18faaa89",
              "IPY_MODEL_fc229f0a46bf4f33874807f34a0e7e99",
              "IPY_MODEL_75eae8ee26ae4b50873b76a2ba06dbee"
            ],
            "layout": "IPY_MODEL_449b3f23110d4cd2bb51d33d3eec32a8"
          }
        },
        "b4d9f271b7a74e089a97d8bb18faaa89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d6617a251994fde9583949fb61b1665",
            "placeholder": "​",
            "style": "IPY_MODEL_ad088e7b637d410eadcf0016ef53e589",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fc229f0a46bf4f33874807f34a0e7e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d426ee6547504dfa96b36f66458d8281",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1d96b44292b41c2aa1fc59e3db04e70",
            "value": 2
          }
        },
        "75eae8ee26ae4b50873b76a2ba06dbee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b62c5b3615604256acd61c2250d4488e",
            "placeholder": "​",
            "style": "IPY_MODEL_8c84412b58c24c068ae609fe82929f92",
            "value": " 2/2 [01:00&lt;00:00, 27.87s/it]"
          }
        },
        "449b3f23110d4cd2bb51d33d3eec32a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6617a251994fde9583949fb61b1665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad088e7b637d410eadcf0016ef53e589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d426ee6547504dfa96b36f66458d8281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d96b44292b41c2aa1fc59e3db04e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b62c5b3615604256acd61c2250d4488e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c84412b58c24c068ae609fe82929f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/fine-tune-llama-2-7b-ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "q57Zf8p9ivZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific versions of necessary Python libraries using pip.\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "# -q: Suppresses pip output to keep the console clean.\n",
        "\n",
        "# accelerate==0.21.0: The Accelerate library facilitates efficient model training and inference,\n",
        "# particularly for distributed and hardware-accelerated setups.\n",
        "\n",
        "# peft==0.4.0: The Parameter-Efficient Fine-Tuning (PEFT) library is used for fine-tuning\n",
        "# large language models with fewer resources, commonly leveraging LoRA and related techniques.\n",
        "\n",
        "# bitsandbytes==0.40.2: Provides optimized 8-bit and 4-bit quantization routines\n",
        "# for reducing memory usage during model training and inference.\n",
        "\n",
        "# transformers==4.31.0: The Hugging Face Transformers library is a core library for\n",
        "# pre-trained transformer models such as BERT, GPT, and others, enabling easy integration and usage.\n",
        "\n",
        "# trl==0.4.7: The Transformers Reinforcement Learning (TRL) library extends Hugging Face Transformers\n",
        "# to include tools for reinforcement learning with language models, like PPO."
      ],
      "metadata": {
        "id": "GLXwJqbjtPho",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:23:37.712292Z",
          "iopub.execute_input": "2025-01-14T07:23:37.712605Z",
          "iopub.status.idle": "2025-01-14T07:23:55.711037Z",
          "shell.execute_reply.started": "2025-01-14T07:23:37.712579Z",
          "shell.execute_reply": "2025-01-14T07:23:55.710181Z"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "8vwn6xGIi03f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the os module to handle operating system-level tasks like file paths and environment variables.\n",
        "import os\n",
        "\n",
        "# Import PyTorch for tensor operations and GPU/TPU acceleration.\n",
        "import torch\n",
        "\n",
        "# Import the load_dataset function from the datasets library to easily access and load datasets.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import various components from the Hugging Face Transformers library for handling models and tokenization.\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,  # Used to load pre-trained causal language models (e.g., GPT-style models).\n",
        "    AutoTokenizer,        # Handles tokenization for the associated model.\n",
        "    BitsAndBytesConfig,   # Enables configuration of 8-bit quantization using the bitsandbytes library.\n",
        "    HfArgumentParser,     # A utility for defining and parsing command-line arguments for training scripts.\n",
        "    TrainingArguments,    # Provides a set of configurations for model training (e.g., batch size, learning rate).\n",
        "    pipeline,             # A high-level API for tasks like text generation, summarization, etc.\n",
        "    logging,              # Handles logging for better visibility into the Transformers processes.\n",
        ")\n",
        "\n",
        "# Import LoraConfig and PeftModel from the PEFT library for parameter-efficient fine-tuning using LoRA.\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "# Import SFTTrainer from the TRL library, which simplifies supervised fine-tuning of models.\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:23:55.712034Z",
          "iopub.execute_input": "2025-01-14T07:23:55.712247Z",
          "iopub.status.idle": "2025-01-14T07:24:12.875862Z",
          "shell.execute_reply.started": "2025-01-14T07:23:55.712230Z",
          "shell.execute_reply": "2025-01-14T07:24:12.874988Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**In case of Llama 2, the following prompt template is used for the chat models**"
      ],
      "metadata": {
        "id": "J5d58CfLoWZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "System Prompt (optional) to guide the model\n",
        "\n",
        "\n",
        "User prompt (required) to give the instruction\n",
        "\n",
        "\n",
        "Model Answer (required)"
      ],
      "metadata": {
        "id": "5lYYzhCRov5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "AS1Ee8JunXgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We will reformat our instruction dataset to follow Llama 2 template."
      ],
      "metadata": {
        "id": "ck708D51o-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Orignal Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco"
      ],
      "metadata": {
        "id": "YS0zP2DKpJRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k"
      ],
      "metadata": {
        "id": "DKzi2s6RpNdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Complete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2"
      ],
      "metadata": {
        "id": "jVEAxq2piiyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know how this dataset was created, you can check this notebook.  \n",
        "\n",
        "https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing"
      ],
      "metadata": {
        "id": "Cg_GnaVRrv8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: You don’t need to follow a specific prompt template if you’re using the base Llama 2 model instead of the chat version."
      ],
      "metadata": {
        "id": "jSFfMkSpphFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**How to fine tune Llama 2**"
      ],
      "metadata": {
        "id": "zdcR5JHdp6qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Free Google Colab offers a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)"
      ],
      "metadata": {
        "id": "FDEaxMxFqAV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We also need to consider the overhead due to optimizer states, gradients, and forward activations"
      ],
      "metadata": {
        "id": "rwJnbDU1qNx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA."
      ],
      "metadata": {
        "id": "DIeTamzXqcC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here."
      ],
      "metadata": {
        "id": "1lDdMCfyqvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3**"
      ],
      "metadata": {
        "id": "8gXkTpnRq9nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a llama-2-7b-chat-hf model (chat model)\n",
        "2. Train it on the abneraigc/wiki_medical_terms_llama (500 samples), which will produce our fine-tuned model Llama-2-7b-chat-finetune"
      ],
      "metadata": {
        "id": "A3XhjPfHq_mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch"
      ],
      "metadata": {
        "id": "l6CPejEgv7hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # Specifies the pre-trained model from Hugging Face hub\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"abneraigc/wiki_medical_terms_llama\"  # Dataset to fine-tune the model, containing medical-related terms\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"  # Name of the model after fine-tuning\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64  # Dimensionality of the LoRA attention matrix (low-rank approximation)\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16  # Scaling factor for LoRA layers\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.2  # Dropout rate to prevent overfitting in LoRA layers\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True  # Activates quantization to 4-bit precision for base model\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"  # Data type for computations in 4-bit quantization\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"  # Type of quantization (e.g., nf4: non-uniform)\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False  # Enable nested quantization (double quantization)\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"  # Directory to save model checkpoints and logs\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1  # Number of epochs to train the model\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False  # Use fp16 mixed precision training if True, useful for A100 GPUs\n",
        "bf16 = False  # Use bf16 if True, useful for certain GPUs like A100\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4  # Batch size per GPU for training\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4  # Batch size per GPU for evaluation\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1  # Accumulate gradients over multiple steps for larger batches\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True  # Use gradient checkpointing to reduce memory usage\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3  # Gradient clipping to prevent exploding gradients\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4  # Learning rate for the AdamW optimizer\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001  # Regularization to prevent overfitting\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"  # Optimizer used (AdamW)\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"  # Learning rate scheduler type (cosine decay)\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1  # If -1, training runs for all epochs; otherwise, specifies total steps\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03  # Warm-up ratio to scale learning rate from 0 to target value\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "group_by_length = True  # Group sequences of similar length for efficiency in batching\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0  # Save checkpoints every X steps (set to 0 if not saving frequently)\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25  # Log training progress every X steps\n",
        "\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "ib_We3NLtj2E",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:24:12.877560Z",
          "iopub.execute_input": "2025-01-14T07:24:12.877900Z",
          "iopub.status.idle": "2025-01-14T07:24:12.884194Z",
          "shell.execute_reply.started": "2025-01-14T07:24:12.877869Z",
          "shell.execute_reply": "2025-01-14T07:24:12.883355Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4:Load everything and start the fine-tuning process**"
      ],
      "metadata": {
        "id": "G9w5Txi5wI2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
        "\n",
        "\n",
        "2. Then, we’re configuring bitsandbytes for 4-bit quantization.\n",
        "\n",
        "\n",
        "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
        "\n",
        "\n",
        "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!"
      ],
      "metadata": {
        "id": "qNx4Es23wjzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load dataset**"
      ],
      "metadata": {
        "id": "HBon7F6P4yjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "data = load_dataset(dataset_name, split=\"train\")  #"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:24:12.885551Z",
          "iopub.execute_input": "2025-01-14T07:24:12.885777Z",
          "iopub.status.idle": "2025-01-14T07:24:16.839051Z",
          "shell.execute_reply.started": "2025-01-14T07:24:12.885758Z",
          "shell.execute_reply": "2025-01-14T07:24:16.838378Z"
        },
        "id": "QwoW91dm4yjV"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data.shuffle(seed=42).select(range(500))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:24:16.839891Z",
          "iopub.execute_input": "2025-01-14T07:24:16.840164Z",
          "iopub.status.idle": "2025-01-14T07:24:16.851963Z",
          "shell.execute_reply.started": "2025-01-14T07:24:16.840143Z",
          "shell.execute_reply": "2025-01-14T07:24:16.851119Z"
        },
        "id": "FzqcZiIE4yjV"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:24:16.852771Z",
          "iopub.execute_input": "2025-01-14T07:24:16.852965Z",
          "iopub.status.idle": "2025-01-14T07:24:17.270305Z",
          "shell.execute_reply.started": "2025-01-14T07:24:16.852948Z",
          "shell.execute_reply": "2025-01-14T07:24:17.269615Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjP87Ut94yjV",
        "outputId": "000c2b4e-7bc4-4d1c-9b03-407846f7c064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first record\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:16:35.275033Z",
          "iopub.execute_input": "2025-01-14T08:16:35.275515Z",
          "iopub.status.idle": "2025-01-14T08:16:35.304424Z",
          "shell.execute_reply.started": "2025-01-14T08:16:35.275479Z",
          "shell.execute_reply": "2025-01-14T08:16:35.303504Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAMem1zY4yjX",
        "outputId": "2cce0866-8cef-4e31-c157-a605ace59302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '<s> [INST] <<SYS>> You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information. <</SYS>> What is Superficial vein thrombosis and explain in detail? [/INST] Superficial vein thrombosis (SVT) is a blood clot formed in a superficial vein, a vein near the surface of the body. Usually there is thrombophlebitis, which is an inflammatory reaction around a thrombosed vein, presenting as a painful induration (thickening of the skin) with redness. SVT itself has limited significance (in terms of direct morbidity and mortality) when compared to a deep vein thrombosis (DVT), which occurs deeper in the body at the deep venous system level. However, SVT can lead to serious complications (as well as signal other serious problems, such as genetic mutations that increase ones risk for clotting), and is therefore no longer regarded as a benign condition. If the blood clot is too near the saphenofemoral junction there is a higher risk of pulmonary embolism, a potentially life-threatening complication.\\nSVT has risk factors similar to those for other thrombotic conditions and can arise from a variety of causes. Diagnosis is often based on symptoms. There are multiple possible treatments, with the goal of providing symptomatic relief and preventing complications.\\n\\nSigns and symptoms\\nSVT is recognized by the presence of pain, warmth, redness, and tenderness over a superficial vein. The SVT may present as a \"cord-like\" structure upon palpation. The affected vein may be hard along its entire length. SVTs tend to involve the legs, though they can affect any superficial vein (e.g. those in the arms).\\n\\nComplications\\nSVT in the lower extremities can lead to a dangerous complication in which the clot travels to the lungs, called pulmonary embolism (PE). This is because lower limb SVTs can migrate from superficial veins into deeper veins. In a French population, the percent of people with SVTs that also suffered from PEs was 4.7%. In the same population, deep vein thrombosis (DVT) was found in 24.6% of people with SVTs. However, because superficial veins lack muscular support, any clots that form are far less likely to be squeezed by muscle contraction, dislodged, and induce a PE.SVTs can recur after they resolve, which is termed \"migratory thrombophlebitis.\" Migratory thrombophlebitis is a complication that may be due to more serious disorders, such as cancer and other hypercoagulable states.\\n\\nCauses\\nSVTs of the legs are often due to varicose veins, though most people with varicose veins do not develop SVTs. SVTs of the arms are often due to the placement of intravenous catheters.Many of the risk factors that are associated with SVT are also associated with other thrombotic conditions (e.g. DVT). These risk factors include age, cancer, history of thromboembolism, pregnancy, use of oral contraceptive medications (containing estrogen), hormone replacement therapy, recent surgery, and certain autoimmune diseases (especially Behçets and Buergers diseases). Other risk factors include immobilization (stasis) and laparoscopy.Hypercoagulable states due to genetic conditions that increase the risk of clotting may contribute to the development of SVT, such as factor V Leiden, prothrombin 20210A mutation, and protein C, S, and antithrombin III and factor XII deficiency.\\n\\nMechanism\\nThe mechanism for the development of an SVT depends upon the specific etiology of the SVT. For example, varicose veins and prolonged bed rest both may induce SVTs due to slowing the flow of blood through superficial veins.\\n\\nDiagnosis\\nSVTs may be diagnosed based upon clinical criteria by a healthcare professional. A more specific evaluation can be made by ultrasound. An ultrasound can be useful in situations in which an SVT occurs above the knee and is not associated with a varicose vein, because ultrasounds can detect more serious clots like DVTs. The diagnostic utility of D-dimer testing in the setting of SVTs has yet to be fully established.\\n\\nClassification\\nSVTs can be classified as either varicose vein (VV) or non-varicose (NV) associated. NV-SVTs are more likely to be associated with genetic procoagulable states compared to VV-SVTs. SVTs can also be classified by pathophysiology. That is, primary SVTs are characterized by inflammation that is localized to the veins. Secondary SVTs are characterized by systemic inflammatory processes.A subclass of SVTs are septic thrombophlebitis, which are SVTs that occur in the setting of an infection.\\n\\nTreatment\\nThe goal of treatment in SVT is to reduce local inflammation and prevent the SVT from extending from its point of origin. Treatment may entail the use of compression, physical activity, medications, or surgical interventions. The optimal treatment for many SVT sites (i.e. upper limbs, neck, abdominal and thoracic walls, and the penis) has not been determined.\\n\\nCompression\\nMultiple compression bandages exist. Fixed compression bandages, adhesive short stretch bandages, and graduated elastic compression stockings have all be used in the treatment of SVTs. The benefit of compression stockings is unclear, though they are frequently used.\\n\\nPhysical activity\\nInactivity is contraindicated in the aftermath of an SVT. Uninterrupted periods of sitting or standing may cause the SVT to elongate from its point of origin, increasing the risk for complications and clinical worsening.\\n\\nMedications\\nMedications used for the treatment of SVT include anticoagulants, NSAIDs (except aspirin), antibiotics, and corticosteroids.\\n\\nAnticoagulants\\nSVTs that occur within the great saphenous vein within 3 cm of the saphenofemoral junction are considered to be equivalent in risk to DVTs.  These high risk SVTs are treated identically with therapeutic anticoagulation. Anticoagulation is also used for intermediate risk SVTs that are greater than 3 cm from the saphenofemoral junction or are greater than 4–5 cm in length.Anticoagulation for high risk SVTs includes the use of vitamin K antagonists or novel oral anticoagulants (NOACs) for 3 months. Anticoagulation for intermediate risk SVTs includes fondaparinux 2.5 mg daily for 45 days or the use of intermediate to therapeutic dose low molecular weight heparin for 4–6 weeks.\\n\\nNSAIDs\\nNSAIDs (non-steroidal anti-inflammatory drugs) can be used in both oral or topical formulations for the relief of SVT symptoms </s>'}\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)  # Determine the compute data type for quantization based on user settings.\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,  # Enable 4-bit quantization if specified.\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,  # Set quantization type (e.g., symmetric or asymmetric).\n",
        "    bnb_4bit_compute_dtype=compute_dtype,  # Use the determined compute data type.\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,  # Enable nested quantization if applicable.\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:  # If using float16 and 4-bit quantization...\n",
        "    major, _ = torch.cuda.get_device_capability()  # Check the CUDA device's compute capability.\n",
        "    if major >= 8:  # If the GPU supports bfloat16...\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")  # Inform the user about bfloat16 support.\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,  # Use the specified model name.\n",
        "    quantization_config=bnb_config,  # Apply the 4-bit quantization configuration.\n",
        "    device_map=device_map  # Use the defined device mapping for model loading.\n",
        ")\n",
        "model.config.use_cache = False  # Disable caching to prevent potential issues with certain training configurations.\n",
        "model.config.pretraining_tp = 1  # Set the pretraining tensor parallelism parameter to 1.\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # Load the tokenizer for the model, allowing remote code if necessary.\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token to the same as the end-of-sequence token.\n",
        "tokenizer.padding_side = \"right\"  # Set the padding side to avoid overflow issues with fp16 training.\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,  # Set the LoRA alpha scaling parameter.\n",
        "    lora_dropout=lora_dropout,  # Apply dropout for LoRA layers to improve generalization.\n",
        "    r=lora_r,  # Define the rank of LoRA matrices.\n",
        "    bias=\"none\",  # Specify bias handling for LoRA layers.\n",
        "    task_type=\"CAUSAL_LM\",  # Indicate that this configuration is for causal language modeling.\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,  # Specify where to save the model outputs and logs.\n",
        "    num_train_epochs=num_train_epochs,  # Define the number of training epochs.\n",
        "    per_device_train_batch_size=per_device_train_batch_size,  # Set the batch size per GPU.\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # Accumulate gradients over several steps for larger effective batch size.\n",
        "    optim=optim,  # Specify the optimizer to use during training.\n",
        "    save_steps=save_steps,  # Define how often to save model checkpoints.\n",
        "    logging_steps=logging_steps,  # Define how often to log training progress.\n",
        "    learning_rate=learning_rate,  # Set the initial learning rate.\n",
        "    weight_decay=weight_decay,  # Apply weight decay to prevent overfitting.\n",
        "    fp16=fp16,  # Enable mixed-precision training using float16 if applicable.\n",
        "    bf16=bf16,  # Enable mixed-precision training using bfloat16 if applicable.\n",
        "    max_grad_norm=max_grad_norm,  # Clip gradients to prevent exploding gradients.\n",
        "    max_steps=max_steps,  # Set the maximum number of training steps.\n",
        "    warmup_ratio=warmup_ratio,  # Specify the ratio of warmup steps for learning rate scheduling.\n",
        "    group_by_length=group_by_length,  # Group sequences by length to optimize memory usage during training.\n",
        "    lr_scheduler_type=lr_scheduler_type,  # Choose the learning rate scheduler.\n",
        "    report_to=\"tensorboard\"  # Log training metrics to TensorBoard.\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Use the pre-trained model.\n",
        "    train_dataset=dataset,  # Provide the training dataset.\n",
        "    peft_config=peft_config,  # Apply the LoRA configuration.\n",
        "    dataset_text_field=\"text\",  # Specify the text field in the dataset for training.\n",
        "    max_seq_length=max_seq_length,  # Limit the maximum sequence length for training examples.\n",
        "    tokenizer=tokenizer,  # Use the tokenizer for text preprocessing.\n",
        "    args=training_arguments,  # Pass the training arguments.\n",
        "    packing=packing,  # Enable sequence packing if applicable.\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()  # Start training the model with the specified trainer and configurations.\n",
        "trainer.model.save_pretrained(new_model)\n"
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc",
        "outputId": "165bebdf-44fc-4234-d623-897095e08ba0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T07:24:17.271121Z",
          "iopub.execute_input": "2025-01-14T07:24:17.271374Z",
          "iopub.status.idle": "2025-01-14T08:01:43.260123Z",
          "shell.execute_reply.started": "2025-01-14T07:24:17.271343Z",
          "shell.execute_reply": "2025-01-14T08:01:43.259417Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "882f0508aaf34481b7bdad5dc37ae2ca",
            "b4d9f271b7a74e089a97d8bb18faaa89",
            "fc229f0a46bf4f33874807f34a0e7e99",
            "75eae8ee26ae4b50873b76a2ba06dbee",
            "449b3f23110d4cd2bb51d33d3eec32a8",
            "3d6617a251994fde9583949fb61b1665",
            "ad088e7b637d410eadcf0016ef53e589",
            "d426ee6547504dfa96b36f66458d8281",
            "e1d96b44292b41c2aa1fc59e3db04e70",
            "b62c5b3615604256acd61c2250d4488e",
            "8c84412b58c24c068ae609fe82929f92"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "882f0508aaf34481b7bdad5dc37ae2ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 10/125 01:50 < 26:32, 0.07 it/s, Epoch 0.07/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Flush memory\n",
        "del trainer, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8YqeQzdC55PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "CouxD8Lp6H_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# This ensures that the tokenizer is aligned with the `base_model` and can properly process input text\n",
        "\n",
        "# Reload the model in fp16 (16-bit floating point precision) for more efficient memory usage and faster training\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,  # Load the base model architecture\n",
        "    low_cpu_mem_usage=True,  # Optimize memory usage to reduce CPU memory consumption during loading\n",
        "    return_dict=True,  # Return the model output as a dictionary (rather than a tuple), which is useful for further processing\n",
        "    torch_dtype=torch.float16,  # Set the model to use 16-bit precision for reduced memory usage and faster computations\n",
        "    device_map=\"auto\",  # Automatically map model components to available devices (e.g., GPU)\n",
        ")\n",
        "\n",
        "# Merge the adapter model with the base (fp16) model\n",
        "# The adapter is a small model that modifies the behavior of the base model based on additional training or fine-tuning\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)  # Load the adapter weights from `new_model` and apply them to `fp16_model`\n",
        "\n",
        "# Merge the adapter into the base model and unload the adapter weights from memory to save space\n",
        "model = model.merge_and_unload()  # This merges the adapter's functionality into the model and releases memory from the adapter weights"
      ],
      "metadata": {
        "id": "SxYSO2ku6Y8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"llama-2-7b-SFT\")\n",
        "tokenizer.save_pretrained(\"llama-2-7b-SFT\")"
      ],
      "metadata": {
        "id": "GDg5s274660D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 5: Check the plots on tensorboard, as follows**"
      ],
      "metadata": {
        "id": "NBtXo_Tgw43o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "crj9svNe4hU5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:01:43.476515Z",
          "iopub.execute_input": "2025-01-14T08:01:43.476844Z",
          "iopub.status.idle": "2025-01-14T08:01:50.541831Z",
          "shell.execute_reply.started": "2025-01-14T08:01:43.476811Z",
          "shell.execute_reply": "2025-01-14T08:01:50.541102Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 6:Use the text generation pipeline to ask questions like “What is Superficial vein thrombosis and explain in detail?” Note that I’m formatting the input to match Llama 2 prompt template.**"
      ],
      "metadata": {
        "id": "TDqxnjvExGG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \" What is Superficial vein thrombosis and explain in detail? ?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "frlSLPin4IJ4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:17:23.974072Z",
          "iopub.execute_input": "2025-01-14T08:17:23.974395Z",
          "iopub.status.idle": "2025-01-14T08:17:32.032455Z",
          "shell.execute_reply.started": "2025-01-14T08:17:23.974370Z",
          "shell.execute_reply": "2025-01-14T08:17:32.031643Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mkQCviG0Zta-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:03:04.490814Z",
          "iopub.execute_input": "2025-01-14T08:03:04.491102Z",
          "iopub.status.idle": "2025-01-14T08:03:05.194190Z",
          "shell.execute_reply.started": "2025-01-14T08:03:04.491079Z",
          "shell.execute_reply": "2025-01-14T08:03:05.193441Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert our model to GGUF\n",
        "\n",
        "Before we dive into Ollama, its important to take a second and understand the role that `gguf` and `llama.cpp` play in the process.\n",
        "\n",
        "Most people that use LLMs grab them from huggingface using the `AutoModel` class. This is how we did it above. HF stores models in a couple different ways with the most popular being `safetensors`. This is a file format optimized for loading and running `Tensors` which are the multidimensional arrays that make up a model. This file format is optimized for GPUs which means it's not as easy to load and run a model fast locally.\n",
        "\n",
        "One solution that addresses this is the `gguf` format. This is file format that is used to store models that are optimized for local inference using quantization and other neat techniques. This file format is then consumed by runners that support it (ie. `llama.cpp` and Ollama).\n",
        "\n",
        "There's a good bit of complexity here and heres a fantastic [blog post](https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/) that gets into the weeds. For now this is what we need to know\n",
        "\n",
        "1. We have a finetuned Llama3 model saved in the llama-brev directory in the safetensors format\n",
        "2. In order to use this model via Ollama, we need it to be in the `gguf` format\n",
        "3. We can use helper tools in the `llama.cpp` repository to convert"
      ],
      "metadata": {
        "id": "oTymko3V9Xn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to gguf\n",
        "\n",
        "The first thing we do is build llama.cpp in order to use the conversion tools"
      ],
      "metadata": {
        "id": "llK4J0ru_Is1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step might take a while\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUDA=1 make"
      ],
      "metadata": {
        "id": "39DPBXtx9Unn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Push Model to Hugging Face Hub**"
      ],
      "metadata": {
        "id": "2bn7tjdByJ_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model."
      ],
      "metadata": {
        "id": "CpyMDvc5yQrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Z8VUygTdqVnJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:03:48.181960Z",
          "iopub.execute_input": "2025-01-14T08:03:48.182292Z",
          "iopub.status.idle": "2025-01-14T08:03:48.186262Z",
          "shell.execute_reply.started": "2025-01-14T08:03:48.182259Z",
          "shell.execute_reply": "2025-01-14T08:03:48.185253Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Step 1: Log in to your Hugging Face account using your API token\n",
        "api_token = \"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\"  # Replace with your API token\n",
        "login(token=api_token)\n",
        "\n",
        "# Step 2: Push the model and tokenizer to the Hugging Face Hub\n",
        "model_name = \"SURESHBEEKHANI/Llama-2-7b-chat-finetune\"\n",
        "\n",
        "# Push the model to the Hub\n",
        "model.push_to_hub(model_name, check_pr=True)\n",
        "\n",
        "# Push the tokenizer to the Hub\n",
        "tokenizer.push_to_hub(model_name, check_pr=True)\n"
      ],
      "metadata": {
        "id": "x-xPb-_qB0dz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-14T08:08:31.436970Z",
          "iopub.execute_input": "2025-01-14T08:08:31.437336Z",
          "iopub.status.idle": "2025-01-14T08:14:46.530255Z",
          "shell.execute_reply.started": "2025-01-14T08:08:31.437297Z",
          "shell.execute_reply": "2025-01-14T08:14:46.529490Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now use this model for inference by loading it like any other Llama 2 model from the Hub."
      ],
      "metadata": {
        "id": "6nxb3tkLyXeI"
      }
    }
  ]
}
