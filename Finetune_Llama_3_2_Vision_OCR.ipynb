{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCKpXMQUybex6zxJ0bmd9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Finetune_Llama_3_2_Vision_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Install and upgrade UnSloth library for optimized model training\n",
        "\n",
        "- Use `%%capture` to suppress installation output in Jupyter/Colab environments.\n",
        "- Install the `unsloth` package from PyPI for initial setup.\n",
        "- Uninstall the existing `unsloth` package to ensure a clean installation.\n",
        "- Upgrade to the latest version of `unsloth` directly from the GitHub repository."
      ],
      "metadata": {
        "id": "1L-0cn_CjkEE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDTd1MSsVnY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the model\n",
        "\n",
        "\n",
        "Load the `unsloth/Llama-3.2-11B-Vision-Instruct` model using FastVisionModel.\n",
        "\n",
        "Enable 4-bit quantization to reduce memory usage.\n",
        "Utilize UnSloth's gradient checkpointing for efficient training and inference.\n"
      ],
      "metadata": {
        "id": "Vt22_B_djphz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "id": "F1WT3bF2eVZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Add LoRA fine-tuning configuration for Llama 3.2 Vision model\n",
        "\n",
        "- Apply LoRA (Low-Rank Adaptation) to fine-tune the vision and language components of the model.\n",
        "- Enable fine-tuning for vision layers, language layers, attention modules, and MLP modules.\n",
        "- Configure LoRA parameters such as rank (`r`), alpha (`lora_alpha`), and dropout (`lora_dropout`).\n"
      ],
      "metadata": {
        "id": "u6jAtJ2rjvmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 16,\n",
        "    lora_dropout               = 0,\n",
        "    bias                       = \"none\",\n",
        "    random_state               = 3407,\n",
        "    use_rslora                 = False,\n",
        "    loftq_config               = None,\n",
        ")"
      ],
      "metadata": {
        "id": "u4ZmTZPag4UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"DataStudio/OCR_handwritting_HAT2023\",\n",
        "                       split=\"train[0:500]\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "_qgK1uTrg8qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset[45][\"text\"]"
      ],
      "metadata": {
        "id": "vacmURNLiajN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "You are an expert in Optical Character Recognition (OCR). Your task is to accurately extract and transcribe text from images while preserving the original structure, formatting, and readability. Ensure precision in capturing all characters, words, and symbols, minimizing errors and distortions.\n",
        "\"\"\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    \"\"\"\n",
        "    Converts a dataset sample into a structured conversation format for OCR fine-tuning.\n",
        "\n",
        "    Parameters:\n",
        "    sample (dict): A dictionary containing an image and its corresponding extracted text.\n",
        "\n",
        "    Returns:\n",
        "    dict: A structured conversation containing user input (instruction + image)\n",
        "          and assistant output (extracted text).\n",
        "    \"\"\"\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "# Convert the entire dataset into conversation format\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ],
      "metadata": {
        "id": "T7uQ8CPSha0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Enable inference on Llama 3.2 Vision model for generating product descriptions\n",
        "\n",
        "- Prepare the model for inference using `FastVisionModel.for_inference`.\n",
        "- Generate a product description by processing an image and instruction using the Vision-Language Model.\n",
        "- Use a streaming approach to display generated text in real-time.\n"
      ],
      "metadata": {
        "id": "gkq5VVsDj8y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[45][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ZcfvfdHbiymF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Set up SFTTrainer for fine-tuning Llama 3.2 Vision model on product descriptions\n",
        "\n",
        "- Enable the model for training using `FastVisionModel.for_training`.\n",
        "- Configure the `SFTTrainer` from the `trl` library for supervised fine-tuning (SFT) with UnSloth optimizations.\n",
        "- Use `UnslothVisionDataCollator` to handle multi-modal data efficiently during training.\n"
      ],
      "metadata": {
        "id": "VEYPrrtxkMbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model)  # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # For Weights and Biases\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "rVMmJJhmi6TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "6ZuRDMGki9c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "DOYa6892jEo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model=\"lora_model\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[40][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "KPoS9MdsjLb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}