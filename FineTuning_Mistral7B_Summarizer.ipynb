{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/FineTuning_Mistral7B_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "RPhTxO0YXNB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel  # Importing the FastLanguageModel class from the unsloth library\n",
        "import torch  # Importing PyTorch for handling tensors and computations\n",
        "\n",
        "# Set the maximum sequence length for the model's input\n",
        "max_seq_length = 2048  # The maximum number of tokens the model can process in one sequence. Customize as needed.\n",
        "# Note: The library internally supports RoPE (Rotary Position Embedding) scaling to handle long sequences.\n",
        "\n",
        "# Set the data type for model computation\n",
        "dtype = None  # Automatically detect the best precision.\n",
        "# Set dtype to 'torch.float16' for Tesla T4/V100 GPUs, or 'torch.bfloat16' for Ampere and newer GPUs.\n",
        "\n",
        "# Choose whether to use 4-bit quantization for the model\n",
        "load_in_4bit = True  # Enabling 4-bit quantization reduces memory usage and speeds up computation.\n",
        "# Set to False if higher precision is needed or memory is not a concern.\n",
        "\n",
        "# Load the model and tokenizer using the FastLanguageModel class\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v0.3\",  # Specify the model name to load. Replace with any model of your choice.\n",
        "    max_seq_length=max_seq_length,  # Pass the chosen maximum sequence length.\n",
        "    dtype=dtype,  # Pass the chosen data type for computations.\n",
        "    load_in_4bit=load_in_4bit,  # Pass whether to use 4-bit quantization.\n",
        ")\n",
        "\n",
        "# Explanation:\n",
        "# - `FastLanguageModel.from_pretrained` is a convenient method to load both the model and tokenizer.\n",
        "# - `model_name`: The name of the pre-trained model. Example: \"unsloth/mistral-7b-v0.3\".\n",
        "# - `max_seq_length`: Configures the maximum token length the model can handle in one input.\n",
        "# - `dtype`: Allows precise control over computation precision for optimal performance on different hardware.\n",
        "# - `load_in_4bit`: If True, enables 4-bit quantization to reduce memory footprint while maintaining good accuracy.\n"
      ],
      "metadata": {
        "id": "SaH0Mdp5XFtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZOevBwVblpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for generating text responses in the Alpaca format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Define the End-Of-Sequence token for the tokenizer\n",
        "EOS_TOKEN = tokenizer.eos_token  # Ensures the model recognizes the end of the prompt and stops generating further text.\n",
        "\n",
        "# Function to format examples using the Alpaca prompt template\n",
        "def formatting_prompts_func(examples):\n",
        "    # Extract instructions, inputs, and outputs from the examples\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    # List to store the formatted prompts\n",
        "    texts = []\n",
        "\n",
        "    # Iterate over instructions, inputs, and outputs\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Format the instruction, input, and output into the Alpaca prompt\n",
        "        # Append EOS_TOKEN to ensure the model generation stops at the end\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Add the formatted text to the list\n",
        "\n",
        "    # Return the list of formatted texts as a dictionary for compatibility with the dataset API\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"samsum\", split=\"train\")  # Load the \"samsum\" dataset, using the training split.\n",
        "\n",
        "# Apply the formatting function to the dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "# The `batched=True` argument processes multiple examples at once, improving efficiency.\n"
      ],
      "metadata": {
        "id": "4DY2jFcem1Av"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}