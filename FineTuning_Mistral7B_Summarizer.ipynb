{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/FineTuning_Mistral7B_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "RPhTxO0YXNB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel  # Importing the FastLanguageModel class from the unsloth library\n",
        "import torch  # Importing PyTorch for handling tensors and computations\n",
        "\n",
        "# Set the maximum sequence length for the model's input\n",
        "max_seq_length = 2048  # The maximum number of tokens the model can process in one sequence. Customize as needed.\n",
        "# Note: The library internally supports RoPE (Rotary Position Embedding) scaling to handle long sequences.\n",
        "\n",
        "# Set the data type for model computation\n",
        "dtype = None  # Automatically detect the best precision.\n",
        "# Set dtype to 'torch.float16' for Tesla T4/V100 GPUs, or 'torch.bfloat16' for Ampere and newer GPUs.\n",
        "\n",
        "# Choose whether to use 4-bit quantization for the model\n",
        "load_in_4bit = True  # Enabling 4-bit quantization reduces memory usage and speeds up computation.\n",
        "# Set to False if higher precision is needed or memory is not a concern.\n",
        "\n",
        "# Load the model and tokenizer using the FastLanguageModel class\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v0.3\",  # Specify the model name to load. Replace with any model of your choice.\n",
        "    max_seq_length=max_seq_length,  # Pass the chosen maximum sequence length.\n",
        "    dtype=dtype,  # Pass the chosen data type for computations.\n",
        "    load_in_4bit=load_in_4bit,  # Pass whether to use 4-bit quantization.\n",
        ")\n",
        "\n",
        "# Explanation:\n",
        "# - `FastLanguageModel.from_pretrained` is a convenient method to load both the model and tokenizer.\n",
        "# - `model_name`: The name of the pre-trained model. Example: \"unsloth/mistral-7b-v0.3\".\n",
        "# - `max_seq_length`: Configures the maximum token length the model can handle in one input.\n",
        "# - `dtype`: Allows precise control over computation precision for optimal performance on different hardware.\n",
        "# - `load_in_4bit`: If True, enables 4-bit quantization to reduce memory footprint while maintaining good accuracy.\n"
      ],
      "metadata": {
        "id": "SaH0Mdp5XFtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "spUJjfhNz8_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZOevBwVblpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep and Load\n",
        "We now use the text-summarizer dataset from [SURESHBEEKHANI](https://huggingface.co/datasets/SURESHBEEKHANI/text-summarizer)\n"
      ],
      "metadata": {
        "id": "bt_WTHiY1mlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a string template for the prompt format used for generating responses\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# EOS_TOKEN is the special token that signifies the end of a sequence. It ensures the model stops generating when it reaches this token.\n",
        "EOS_TOKEN = tokenizer.eos_token  # Retrieve the EOS token from the tokenizer (ensures proper stopping in generation)\n",
        "\n",
        "# Define the formatting function for processing the dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]  # Extract instructions from the dataset\n",
        "    inputs = examples[\"dialogue\"]  # Extract dialogues (context) from the dataset\n",
        "    outputs = examples[\"summary\"]  # Extract expected summaries from the dataset\n",
        "\n",
        "    texts = []  # Initialize an empty list to store the formatted text prompts\n",
        "    # Loop over the instructions, inputs (dialogues), and outputs (summaries) to create the full prompt\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Use the alpaca_prompt template to format each instruction, input, and output\n",
        "        # Ensure EOS_TOKEN is appended to avoid endless generation during model training or inference\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Append the formatted text to the list\n",
        "\n",
        "    # Return the formatted text wrapped in a dictionary with the key \"text\" for use in model training\n",
        "    return { \"text\": texts }\n",
        "\n",
        "# Load the dataset from the \"SURESHBEEKHANI/text-summarizer\" dataset repository\n",
        "# 'split=\"train\"' indicates we are loading the training data.\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"SURESHBEEKHANI/text-summarizer\", split=\"train\")\n",
        "\n",
        "# Apply the formatting function to the dataset in batches, preparing the data for model training\n",
        "# 'batched=True' means the function will process multiple examples at once, improving efficiency.\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n"
      ],
      "metadata": {
        "id": "4DY2jFcem1Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "YE4pAaekptdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "sa0hTI6jyhG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "from trl import SFTTrainer  # SFTTrainer is a class used for training models using Supervised Fine-Tuning (SFT)\n",
        "from transformers import TrainingArguments  # TrainingArguments holds the configuration for model training\n",
        "from unsloth import is_bfloat16_supported  # Utility to check if bfloat16 is supported in the system\n",
        "\n",
        "# Initialize the SFTTrainer with several parameters for training\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # The model to be trained (not defined here, assumed to be defined elsewhere)\n",
        "    tokenizer=tokenizer,  # The tokenizer to be used for text preprocessing (assumed to be defined elsewhere)\n",
        "    train_dataset=dataset,  # The dataset used for training (assumed to be defined elsewhere)\n",
        "    dataset_text_field=\"text\",  # Field in the dataset containing the input text\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length for tokenized inputs (assumed to be defined elsewhere)\n",
        "    dataset_num_proc=2,  # Number of processes to use for data preprocessing; can speed up data loading\n",
        "    packing=False,  # Whether to use sequence packing, which can make training faster for short sequences\n",
        "    args=TrainingArguments(  # Set the training arguments and hyperparameters\n",
        "        per_device_train_batch_size=2,  # Batch size per device (e.g., GPU or CPU) during training\n",
        "        gradient_accumulation_steps=4,  # Number of steps before performing a gradient update (helps with large batch sizes)\n",
        "        warmup_steps=5,  # Number of steps for the learning rate warmup before it starts decaying\n",
        "        max_steps=60,  # Total number of training steps. Usually corresponds to num_train_epochs * num_steps_per_epoch\n",
        "        learning_rate=2e-4,  # Learning rate for the optimizer\n",
        "        fp16=not is_bfloat16_supported(),  # Use mixed-precision (float16) if bfloat16 is not supported by the hardware\n",
        "        bf16=is_bfloat16_supported(),  # Use bfloat16 if supported by the hardware\n",
        "        logging_steps=1,  # Frequency (in steps) to log training metrics (e.g., loss) during training\n",
        "        optim=\"adamw_8bit\",  # Optimizer type. Using AdamW with 8-bit precision for memory efficiency\n",
        "        weight_decay=0.01,  # Weight decay parameter for regularization to prevent overfitting\n",
        "        lr_scheduler_type=\"linear\",  # Learning rate scheduler type; here it's set to linear decay\n",
        "        seed=3407,  # Random seed for reproducibility of results\n",
        "        output_dir=\"outputs\",  # Directory where the model checkpoints and outputs will be saved\n",
        "        report_to=\"none\",  # Specify where to report metrics (e.g., use \"wandb\" for reporting to Weights & Biases)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The trainer is now set up and can be used for training the model."
      ],
      "metadata": {
        "id": "Tbtx0xgNynxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "T_SPkOg3yvA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "lAWT7FkLy0Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "xLZNYl4O3xMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ],
      "metadata": {
        "id": "qqXQvpqd341z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Below is a passage of text. Write a concise summary of the text below.\n",
        "\n",
        "### Text:\n",
        "{}\n",
        "\n",
        "### Summary:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(  # Format the alpaca_prompt with the specific instruction and input text.\n",
        "            \"The quick brown fox jumps over the lazy dog. The dog, despite being lazy, tries to catch the fox but fails. The fox quickly disappears into the forest, leaving the dog behind. This is a classic example of the speed of the fox being unmatched by the dogâ€™s sluggishness.\"  # Example of input text to summarize.\n",
        "            ,  # Leave the summary part empty for generation.\n",
        "            \"\"  # Output: empty as the model will generate the summary.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "# Generate the summary using the model.\n",
        "# 'max_new_tokens' controls how many tokens the model is allowed to generate.\n",
        "# 'use_cache' allows for faster generation by caching previous results.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated tokens back into readable text.\n",
        "# This will give us the model's summary of the provided input text.\n",
        "tokenizer.batch_decode(outputs)  # Convert the output tokens to text.\n"
      ],
      "metadata": {
        "id": "KLW2gbbS34kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ],
      "metadata": {
        "id": "10YzIGxp5KnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template for text summarization.\n",
        "# This will instruct the model to generate a concise summary of the input text.\n",
        "alpaca_prompt = \"\"\"Below is a passage of text. Write a concise summary of the text below.\n",
        "\n",
        "### Text:\n",
        "{}\n",
        "\n",
        "### Summary:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) optimizes the model for faster inference.\n",
        "# This typically involves optimizations that speed up processing, such as reducing latency and improving GPU utilization.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Tokenize the input text for the summarization task.\n",
        "# Provide an example of input text that needs to be summarized. The model will generate a summary based on this input.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(  # Format the alpaca_prompt with a specific text input and empty summary part.\n",
        "            \"The quick brown fox jumps over the lazy dog. The dog, despite being lazy, tries to catch the fox but fails. The fox quickly disappears into the forest, leaving the dog behind.\"  # Example input text to summarize.\n",
        "            ,  # Leave the summary part empty for the model to generate.\n",
        "            \"\"  # Output: Empty since the model will generate the summary.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input text into PyTorch tensors, as required for model input.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "# Import TextStreamer from transformers to stream the output generation.\n",
        "# The TextStreamer class will allow the model to generate the summary token by token.\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Initialize the TextStreamer with the tokenizer to decode the generated tokens during streaming.\n",
        "# This enables real-time generation of summaries.\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the summary using the model, and stream the output token by token.\n",
        "# This helps in quickly receiving the summary output, especially for longer texts.\n",
        "_ = model.generate(\n",
        "    **inputs,  # Provide the tokenized inputs (text) to the model.\n",
        "    streamer=text_streamer,  # Enable token-by-token streaming for faster output generation.\n",
        "    max_new_tokens=128  # Set the maximum number of tokens to generate (the length of the summary).\n",
        ")\n"
      ],
      "metadata": {
        "id": "BzJnADOp5Jnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters"
      ],
      "metadata": {
        "id": "Z7j1aCbF5-hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I5LDZmpW5-QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "bqPOJ7yU5ikI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ],
      "metadata": {
        "id": "inOFYfZF598K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import FastLanguageModel from the unsloth library.\n",
        "# This class allows you to load pre-trained models, configure them for fast inference, and perform tasks like text generation.\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the pre-trained model and tokenizer using the FastLanguageModel class.\n",
        "# - 'model_name' is the name of the model you trained (in this case, \"lora_model\").\n",
        "# - 'max_seq_length' is the maximum sequence length the model can handle for input.\n",
        "# - 'dtype' is the data type for model weights (such as float32 or float16).\n",
        "# - 'load_in_4bit' specifies whether to load the model with reduced 4-bit precision for efficiency (saves memory).\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\",  # Replace with the name of the model you used for training.\n",
        "    max_seq_length = max_seq_length,  # Maximum length of the input sequence.\n",
        "    dtype = dtype,  # Data type for the model (e.g., float32, float16).\n",
        "    load_in_4bit = load_in_4bit,  # Option to load the model in 4-bit precision to save memory.\n",
        ")\n",
        "\n",
        "# Enable optimizations for inference to make the model's token generation up to 2x faster.\n",
        "# This improves performance by reducing latency and making the model more efficient during text generation.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference for optimized generation.\n",
        "\n",
        "# 'alpaca_prompt' should be the pre-defined prompt template you are using to format your input for the model.\n",
        "# This template typically has placeholders for instructions, inputs, and outputs that are formatted during generation.\n",
        "\n",
        "# Tokenize the input text using the tokenizer.\n",
        "# Here we are preparing the input by filling the prompt template with specific instructions.\n",
        "# The instruction asks about a famous tall tower in Paris, and the input/output are left blank for the model to generate a response.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(  # Format the alpaca_prompt with the provided instruction and input.\n",
        "            \"What is a famous tall tower in Paris?\",  # Instruction: Ask about a famous tower in Paris.\n",
        "            \"\",  # Input: Leave it blank, as the model will generate the response.\n",
        "            \"\"  # Output: Left empty, as the model will generate the answer.\n",
        "        ),\n",
        "    ], return_tensors=\"pt\"  # Convert the formatted input text into PyTorch tensors (required for the model to process).\n",
        ").to(\"cuda\")  # Move the input tensors to the GPU to speed up computation.\n",
        "\n",
        "# Generate the output using the model based on the tokenized inputs.\n",
        "# The model will generate a response with a maximum of 64 new tokens.\n",
        "# 'use_cache=True' allows for more efficient generation by reusing intermediate states during the generation process.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated token IDs back into human-readable text using the tokenizer.\n",
        "# 'batch_decode' converts the tokenized outputs into strings of text.\n",
        "tokenizer.batch_decode(outputs)  # Retrieve the final generated output (e.g., the model's response to the question).\n"
      ],
      "metadata": {
        "id": "QF4T-jfE6Nn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push the trained model to the Hugging Face Model Hub using the GGUF format"
      ],
      "metadata": {
        "id": "EQbhrTsu8T18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/Mistral_7B_Summarizer_SFT_GGUF\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"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\",  # Provide the Hugging Face token f\n",
        ")"
      ],
      "metadata": {
        "id": "hgXVqHlG7Sqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAizpzqiFcrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}