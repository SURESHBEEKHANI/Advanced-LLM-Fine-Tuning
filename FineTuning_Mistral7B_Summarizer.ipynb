{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/FineTuning_Mistral7B_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "RPhTxO0YXNB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel  # Importing the FastLanguageModel class from the unsloth library\n",
        "import torch  # Importing PyTorch for handling tensors and computations\n",
        "\n",
        "# Set the maximum sequence length for the model's input\n",
        "max_seq_length = 2048  # The maximum number of tokens the model can process in one sequence. Customize as needed.\n",
        "# Note: The library internally supports RoPE (Rotary Position Embedding) scaling to handle long sequences.\n",
        "\n",
        "# Set the data type for model computation\n",
        "dtype = None  # Automatically detect the best precision.\n",
        "# Set dtype to 'torch.float16' for Tesla T4/V100 GPUs, or 'torch.bfloat16' for Ampere and newer GPUs.\n",
        "\n",
        "# Choose whether to use 4-bit quantization for the model\n",
        "load_in_4bit = True  # Enabling 4-bit quantization reduces memory usage and speeds up computation.\n",
        "# Set to False if higher precision is needed or memory is not a concern.\n",
        "\n",
        "# Load the model and tokenizer using the FastLanguageModel class\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v0.3\",  # Specify the model name to load. Replace with any model of your choice.\n",
        "    max_seq_length=max_seq_length,  # Pass the chosen maximum sequence length.\n",
        "    dtype=dtype,  # Pass the chosen data type for computations.\n",
        "    load_in_4bit=load_in_4bit,  # Pass whether to use 4-bit quantization.\n",
        ")\n",
        "\n",
        "# Explanation:\n",
        "# - `FastLanguageModel.from_pretrained` is a convenient method to load both the model and tokenizer.\n",
        "# - `model_name`: The name of the pre-trained model. Example: \"unsloth/mistral-7b-v0.3\".\n",
        "# - `max_seq_length`: Configures the maximum token length the model can handle in one input.\n",
        "# - `dtype`: Allows precise control over computation precision for optimal performance on different hardware.\n",
        "# - `load_in_4bit`: If True, enables 4-bit quantization to reduce memory footprint while maintaining good accuracy.\n"
      ],
      "metadata": {
        "id": "SaH0Mdp5XFtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "spUJjfhNz8_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZOevBwVblpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a string template for the prompt format used for generating responses\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# EOS_TOKEN is the special token that signifies the end of a sequence. It ensures the model stops generating when it reaches this token.\n",
        "EOS_TOKEN = tokenizer.eos_token  # Retrieve the EOS token from the tokenizer (ensures proper stopping in generation)\n",
        "\n",
        "# Define the formatting function for processing the dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]  # Extract instructions from the dataset\n",
        "    inputs = examples[\"dialogue\"]  # Extract dialogues (context) from the dataset\n",
        "    outputs = examples[\"summary\"]  # Extract expected summaries from the dataset\n",
        "\n",
        "    texts = []  # Initialize an empty list to store the formatted text prompts\n",
        "    # Loop over the instructions, inputs (dialogues), and outputs (summaries) to create the full prompt\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Use the alpaca_prompt template to format each instruction, input, and output\n",
        "        # Ensure EOS_TOKEN is appended to avoid endless generation during model training or inference\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Append the formatted text to the list\n",
        "\n",
        "    # Return the formatted text wrapped in a dictionary with the key \"text\" for use in model training\n",
        "    return { \"text\": texts }\n",
        "\n",
        "# Load the dataset from the \"SURESHBEEKHANI/text-summarizer\" dataset repository\n",
        "# 'split=\"train\"' indicates we are loading the training data.\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"SURESHBEEKHANI/text-summarizer\", split=\"train\")\n",
        "\n",
        "# Apply the formatting function to the dataset in batches, preparing the data for model training\n",
        "# 'batched=True' means the function will process multiple examples at once, improving efficiency.\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n"
      ],
      "metadata": {
        "id": "4DY2jFcem1Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "YE4pAaekptdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "sa0hTI6jyhG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "from trl import SFTTrainer  # SFTTrainer is a class used for training models using Supervised Fine-Tuning (SFT)\n",
        "from transformers import TrainingArguments  # TrainingArguments holds the configuration for model training\n",
        "from unsloth import is_bfloat16_supported  # Utility to check if bfloat16 is supported in the system\n",
        "\n",
        "# Initialize the SFTTrainer with several parameters for training\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # The model to be trained (not defined here, assumed to be defined elsewhere)\n",
        "    tokenizer=tokenizer,  # The tokenizer to be used for text preprocessing (assumed to be defined elsewhere)\n",
        "    train_dataset=dataset,  # The dataset used for training (assumed to be defined elsewhere)\n",
        "    dataset_text_field=\"text\",  # Field in the dataset containing the input text\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length for tokenized inputs (assumed to be defined elsewhere)\n",
        "    dataset_num_proc=2,  # Number of processes to use for data preprocessing; can speed up data loading\n",
        "    packing=False,  # Whether to use sequence packing, which can make training faster for short sequences\n",
        "    args=TrainingArguments(  # Set the training arguments and hyperparameters\n",
        "        per_device_train_batch_size=2,  # Batch size per device (e.g., GPU or CPU) during training\n",
        "        gradient_accumulation_steps=4,  # Number of steps before performing a gradient update (helps with large batch sizes)\n",
        "        warmup_steps=5,  # Number of steps for the learning rate warmup before it starts decaying\n",
        "        max_steps=60,  # Total number of training steps. Usually corresponds to num_train_epochs * num_steps_per_epoch\n",
        "        learning_rate=2e-4,  # Learning rate for the optimizer\n",
        "        fp16=not is_bfloat16_supported(),  # Use mixed-precision (float16) if bfloat16 is not supported by the hardware\n",
        "        bf16=is_bfloat16_supported(),  # Use bfloat16 if supported by the hardware\n",
        "        logging_steps=1,  # Frequency (in steps) to log training metrics (e.g., loss) during training\n",
        "        optim=\"adamw_8bit\",  # Optimizer type. Using AdamW with 8-bit precision for memory efficiency\n",
        "        weight_decay=0.01,  # Weight decay parameter for regularization to prevent overfitting\n",
        "        lr_scheduler_type=\"linear\",  # Learning rate scheduler type; here it's set to linear decay\n",
        "        seed=3407,  # Random seed for reproducibility of results\n",
        "        output_dir=\"outputs\",  # Directory where the model checkpoints and outputs will be saved\n",
        "        report_to=\"none\",  # Specify where to report metrics (e.g., use \"wandb\" for reporting to Weights & Biases)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The trainer is now set up and can be used for training the model."
      ],
      "metadata": {
        "id": "Tbtx0xgNynxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "T_SPkOg3yvA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "lAWT7FkLy0Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}