{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Llama_3_8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e77c63e-eb1a-4731-a37f-752997945e8a",
      "metadata": {
        "id": "9e77c63e-eb1a-4731-a37f-752997945e8a"
      },
      "source": [
        "# Fine-tune and Convert Llama3-8B to a GGUF Model File for CPU and GPU Inference 🚀\n",
        "\n",
        "Hi everyone!\n",
        "\n",
        "Over the past few months, open-source AI models have taken the community by storm. However, there’s been a lack of comprehensive guides showing how to fine-tune a model and convert it into a format ready for deployment on both CPU and GPU setups. So, we’ve created one for you! 🎉\n",
        "\n",
        "## In this guide, we’ll cover the following steps:\n",
        "\n",
        "1. **Fine-tune Llama3-8B** using an innovative method called **ORPO**. This technique combines Supervised Fine-tuning with Direct Preference Optimization, all in a single streamlined step.  \n",
        "2. **Install `llama.cpp`** and convert the model into the **GGUF** format, while discussing various options like LoRA integration.  \n",
        "3. **Create a deployment configuration file**, which acts like a blueprint for packaging and deploying models.  \n",
        "4. **Test the model locally on both CPU and GPU** and then **push it to a shared hub**.  \n",
        "5. **Pull and deploy the model on another machine** to run inference efficiently using either CPU or GPU resources.  \n",
        "\n",
        "Grab some snacks, fasten your seatbelt, and let’s dive in! 🚀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb0724a-d33c-46c5-aa8c-1fcf8340b3a8",
      "metadata": {
        "id": "0bb0724a-d33c-46c5-aa8c-1fcf8340b3a8"
      },
      "source": [
        "# Phase 1: Finetune Llama3 with ORPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf10d7e-22c1-4d26-9fdb-346877cf38c9",
      "metadata": {
        "id": "1bf10d7e-22c1-4d26-9fdb-346877cf38c9"
      },
      "source": [
        "So far, we've released guides on finetuning Llama3 with 2 different approaches\n",
        "1. [Supervised Finetuning](https://github.com/brevdev/notebooks/blob/main/llama3_finetune_inference.ipynb)\n",
        "2. [Direct Preference Optimization](https://github.com/brevdev/notebooks/blob/main/llama3dpo.ipynb)\n",
        "\n",
        "If you havn't taken a look at these, I suggest skimming through the code for the SFT notebook and the explanation at the top of the DPO notebook. TLDR: Most of the finetuning in guides and online use SFT. SFT is good at adapting a model to domain-specific ouput but might also increase the probability of generating outputs that are not as desirable. To solve this issue, we can then perform a process known as preference alignment. This can be done using RLHF or DPO. However, these are both computationally expensive.\n",
        "\n",
        "Enter Odds Ratio Policy Optimization. At a high level, this combines SFT and DPO into 1 neat step which weakly penalizes rejected responses while strongly rewarding preferred responses. Here we optimize for two objectives at once: learning domain-specific output AND aligning the output with out preferences. If you'd like to dive deeper into ORPO, check out MLabonne's [fantastic guide](https://huggingface.co/blog/mlabonne/orpo-llama-3) on HuggingFace\n",
        "\n",
        "![ORPO](https://i.imgur.com/ftrth4Q.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3db3f2f-3afc-4e6d-beca-a2d8a3459fd3",
      "metadata": {
        "id": "b3db3f2f-3afc-4e6d-beca-a2d8a3459fd3"
      },
      "source": [
        "## Install Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "611980ad-56f3-4375-8174-b01c6f921cb5",
      "metadata": {
        "scrolled": true,
        "id": "611980ad-56f3-4375-8174-b01c6f921cb5",
        "outputId": "fd382b7a-f6fe-4b79-ca46-bd9ddf4dbb75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Collecting trl\n",
            "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting datasets>=2.21.0 (from trl)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.21.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.11.11)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, bitsandbytes, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.0 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 trl-0.13.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes wandb transformers peft accelerate trl  # Install bitsandbytes for efficient model training, wandb for experiment tracking, transformers for pre-trained models, peft for parameter-efficient fine-tuning, accelerate for distributed training, and trl for reinforcement learning with transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c25c100-5268-4197-bd68-6f3fb3922328",
      "metadata": {
        "id": "0c25c100-5268-4197-bd68-6f3fb3922328"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "# Import the garbage collection module, which can be used to manage memory by manually releasing unused objects.\n",
        "\n",
        "import os\n",
        "# Import the OS module to interact with the operating system, such as handling file paths and environment variables.\n",
        "\n",
        "import torch\n",
        "# Import the PyTorch library for deep learning tasks, which provides support for tensors and GPU-based computation.\n",
        "\n",
        "import wandb\n",
        "# Import the Weights & Biases (wandb) library, which is used for tracking experiments, visualizing metrics, and logging hyperparameters during training.\n",
        "\n",
        "from datasets import load_dataset\n",
        "# Import the `load_dataset` function from the Hugging Face Datasets library to easily load datasets for training.\n",
        "\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "# Import components from the PEFT library: LoraConfig to configure LoRA (Low-Rank Adaptation) for efficient fine-tuning,\n",
        "# PeftModel to apply PEFT-based modifications, and prepare_model_for_kbit_training to prepare a model for efficient training with low-precision (e.g., 8-bit).\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,  # Import AutoModelForCausalLM to automatically load a pre-trained causal language model.\n",
        "    AutoTokenizer,  # Import AutoTokenizer to automatically load a tokenizer corresponding to the pre-trained model.\n",
        "    BitsAndBytesConfig,  # Import BitsAndBytesConfig to configure efficient 8-bit optimizers for model training.\n",
        "    TrainingArguments,  # Import TrainingArguments to define the configuration for model training, such as batch size, learning rate, etc.\n",
        "    pipeline,  # Import pipeline for easy inference and processing tasks with pre-trained models.\n",
        ")\n",
        "\n",
        "from trl import ORPOConfig, ORPOTrainer\n",
        "# Import ORPOConfig to configure the ORPO fine-tuning method and ORPOTrainer to handle the training process for models using the ORPO technique (Optimization with Regularization for Pre-trained Objectives)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dda9016-cb24-41fc-8756-a6eed21b5777",
      "metadata": {
        "id": "0dda9016-cb24-41fc-8756-a6eed21b5777"
      },
      "source": [
        "## Load Model, Tokenizer, and Dataset\n",
        "\n",
        "We'll be working with the Llama3-8B instruct model. But the steps are almost the same with any other model if you decide to finetune with a LoRA adapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cd703c54-9a29-481e-85bc-f7623431e746",
      "metadata": {
        "id": "cd703c54-9a29-481e-85bc-f7623431e746"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# Define the base pre-trained model to use. Here, \"meta-llama/Meta-Llama-3-8B-Instruct\" refers to the 3.8 billion parameter version of Llama trained for instruction-following tasks.\n",
        "\n",
        "new_model = \"Llama-3-8B-adapter\"\n",
        "# Define the name for the new model that will be created. This is typically a model that has been fine-tuned or adapted from the base model, using methods like LoRA, to create a specialized version for a specific task or domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "187f871a-fa77-4f5a-8e10-db9b6aae210b",
      "metadata": {
        "id": "187f871a-fa77-4f5a-8e10-db9b6aae210b"
      },
      "outputs": [],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Configure the model to be loaded in 4-bit precision to reduce memory usage and improve training efficiency.\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type for 4-bit precision. \"nf4\" is a specific format for 4-bit quantization used for memory-efficient model training.\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Set the compute data type to bfloat16, a 16-bit floating-point format optimized for training on modern hardware like TPUs and GPUs.\n",
        "    bnb_4bit_use_double_quant=True,  # Enable double quantization to improve the accuracy and stability of 4-bit quantization.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99e66bf1-bb51-4a19-be82-475d1ea5911e",
      "metadata": {
        "id": "99e66bf1-bb51-4a19-be82-475d1ea5911e"
      },
      "outputs": [],
      "source": [
        "## configuration of LoRA (Low-Rank Adaptation),\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Set the rank of the low-rank adaptation matrices. The rank controls the number of parameters introduced by LoRA. A higher rank allows for more capacity but increases the memory and computation cost.\n",
        "    lora_alpha=32,  # Set the scaling factor for LoRA layers. It controls the magnitude of the adapted weights, affecting how much influence the LoRA adaptation has on the original model.\n",
        "    lora_dropout=0.05,  # Set the dropout rate for the LoRA layers. Dropout helps prevent overfitting by randomly deactivating some neurons during training.\n",
        "    bias=\"none\",  # Specify how biases are treated in LoRA layers. \"none\" means no bias is added to the LoRA layers. Options may also include \"all\" to add biases or other configurations.\n",
        "    task_type=\"CAUSAL_LM\",  # Set the type of task for the model. \"CAUSAL_LM\" indicates a causal language model, such as GPT, which generates text based on preceding tokens.\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']  # Specify the target modules in the model to apply LoRA. These modules are typically projection layers in transformer architectures.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4f1da12a-aadb-4ba5-8e0b-938d476f624d",
      "metadata": {
        "id": "4f1da12a-aadb-4ba5-8e0b-938d476f624d",
        "outputId": "8412c31c-1708-4055-d963-456866324692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "913f0e1e9b474436979e35cc9afc3436",
            "cb5b69c49faf40e88d2abab4c02e7d58",
            "e8a197d9aae64628aa20d641831bb384",
            "e5f4160285c4497ab0d41b855edb5082",
            "a84e303f966548f3b7d8e22fdbecab60",
            "6db73f57a90448059c31e6b0361e62ec",
            "e0732d2ef65b4b61975e40cfd77333b5",
            "fc1269d783ca44f599befc84e931afdc",
            "c185200a9a344ff9831695691e5d7e66",
            "f2d6a816bfe44b86874639068b6b4532",
            "61689275c8e84d33ba3d01eeb77642fc",
            "97763303a73b44ccb7416ba886a49f96",
            "e26a7d1ed3684219b94a7a2b26cba0a1",
            "1d4ef3091d5b40a88b6228b9683a969f",
            "e0c31452659d4fdfaddb0d03535ef60a",
            "e5dff40a57ed4f1bbc0a8395fa78bd41",
            "4888d54c8ed4421886b8a4b3033afdec"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "913f0e1e9b474436979e35cc9afc3436"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# Import the `notebook_login` function from the Hugging Face Hub library. This function facilitates logging into Hugging Face from a Jupyter notebook, allowing you to easily access and upload models and datasets.\n",
        "\n",
        "notebook_login()\n",
        "# Call the `notebook_login()` function to authenticate your Hugging Face account in the notebook. It will prompt you to enter an authentication token, which grants access to private models or allows you to upload your own models and datasets to the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d0661cc8-f8cf-43d7-bb08-ea7b4e98f235",
      "metadata": {
        "id": "d0661cc8-f8cf-43d7-bb08-ea7b4e98f235",
        "outputId": "6fa76b1c-6c11-480b-ec84-0c14213c340a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6789f6e9-4dc7730a618d14d8316bff09;20ad11af-abac-45e0-9a13-5365d66d9a06)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6789f6e9-4dc7730a618d14d8316bff09;20ad11af-abac-45e0-9a13-5365d66d9a06)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-28b491b9236c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the tokenizer for the pre-trained model specified by `base_model`. The `AutoTokenizer.from_pretrained` function automatically selects the appropriate tokenizer based on the model architecture (e.g., BERT, GPT, etc.).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Set the padding token (`pad_token`) to be the same as the end-of-sequence token (`eos_token`). This is often done to simplify tokenization when using models that don't have a specific padding token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    879\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6789f6e9-4dc7730a618d14d8316bff09;20ad11af-abac-45e0-9a13-5365d66d9a06)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "# Load the tokenizer for the pre-trained model specified by `base_model`. The `AutoTokenizer.from_pretrained` function automatically selects the appropriate tokenizer based on the model architecture (e.g., BERT, GPT, etc.).\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Set the padding token (`pad_token`) to be the same as the end-of-sequence token (`eos_token`). This is often done to simplify tokenization when using models that don't have a specific padding token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed59a18-d9a1-4c0c-bc30-54907a335a5d",
      "metadata": {
        "id": "fed59a18-d9a1-4c0c-bc30-54907a335a5d"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,  # Load the pre-trained causal language model specified by `base_model` (e.g., GPT-like model).\n",
        "    quantization_config=bnb_config,  # Apply the quantization configuration (`bnb_config`) for efficient model training, using 4-bit precision or other configurations specified.\n",
        "    device_map=\"auto\",  # Automatically map the model to available devices (e.g., GPU, CPU) to ensure efficient computation. It will use multiple GPUs if needed.\n",
        ")\n",
        "# Load the pre-trained model for causal language modeling with the specified quantization and device settings.\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "# Prepare the model for training with k-bit precision (e.g., 4-bit or lower precision), optimizing it for memory-efficient training without losing significant accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7521507-bb69-402a-aeaf-c67909ff8b35",
      "metadata": {
        "id": "d7521507-bb69-402a-aeaf-c67909ff8b35"
      },
      "outputs": [],
      "source": [
        "# Define the name of the dataset to be loaded\n",
        "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
        "\n",
        "# Load the dataset with the given name, and select the \"all\" split (which includes the entire dataset)\n",
        "dataset = load_dataset(dataset_name, split=\"all\")\n",
        "\n",
        "# Shuffle the dataset with a fixed random seed for reproducibility, and select only the first 200 samples\n",
        "dataset = dataset.shuffle(seed=42).select(range(200))\n",
        "\n",
        "# Define a function to format the \"chosen\" and \"rejected\" fields of the dataset\n",
        "def format_chat_template(row):\n",
        "    # Apply the tokenizer to the \"chosen\" text without actually tokenizing it (just apply formatting)\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    # Apply the tokenizer to the \"rejected\" text similarly\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    # Return the modified row\n",
        "    return row\n",
        "\n",
        "# Apply the formatting function to each sample in the dataset in parallel\n",
        "# Use all available CPU cores (os.cpu_count()) to speed up the operation\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),  # Number of processes to use for parallelism\n",
        ")\n",
        "\n",
        "# Split the dataset into training and testing subsets, with 1% allocated to the test set\n",
        "dataset = dataset.train_test_split(test_size=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6af1a8-7c1e-4e70-9039-9b88a63a5190",
      "metadata": {
        "id": "2c6af1a8-7c1e-4e70-9039-9b88a63a5190"
      },
      "source": [
        "Take a look at the dataset by uncommenting the code block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4247190e-f8b4-473d-ab07-30f83ddb0eb8",
      "metadata": {
        "id": "4247190e-f8b4-473d-ab07-30f83ddb0eb8"
      },
      "outputs": [],
      "source": [
        "for k in dataset['train'].features.keys():\n",
        "    print(k)\n",
        "    print(\"---------\")\n",
        "    print(dataset['train'][1][k])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0360c7-3af5-416b-8410-572dad9e3658",
      "metadata": {
        "id": "5a0360c7-3af5-416b-8410-572dad9e3658"
      },
      "source": [
        "## Set up ORPO Training\n",
        "\n",
        "The ORPO trainer looks very similar to the SFTTrainer and the DPO trainer. We set our config parameters and start our training run. Notice that the run is very short. In order to increase it, you can increase the `num_train_epochs` or add a `max_steps` argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f3df21-55ba-4468-9030-530913cc218d",
      "metadata": {
        "id": "30f3df21-55ba-4468-9030-530913cc218d"
      },
      "outputs": [],
      "source": [
        "# Define the configuration for ORPO (Monolithic Preference Optimization)\n",
        "orpo_args = ORPOConfig(\n",
        "    learning_rate=8e-6,  # Set a very small learning rate for gradual optimization\n",
        "    lr_scheduler_type=\"linear\",  # Use a linear scheduler for adjusting the learning rate\n",
        "    max_length=1024,  # Maximum sequence length the model can handle during training\n",
        "    max_prompt_length=512,  # Maximum length of the prompt input for the model\n",
        "    beta=0.1,  # Regularization parameter, potentially controlling the preference loss weight\n",
        "    per_device_train_batch_size=2,  # Batch size for training, 2 samples per device (GPU)\n",
        "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before performing a backward pass\n",
        "    optim=\"paged_adamw_8bit\",  # Use the 8-bit version of AdamW optimizer for efficient memory usage\n",
        "    num_train_epochs=1,  # Number of training epochs (full passes through the dataset)\n",
        "    max_steps=10,  # Maximum number of optimization steps to take (limits the number of updates)\n",
        "    evaluation_strategy=\"steps\",  # Evaluation will be based on steps\n",
        "    eval_steps=0.2,  # Evaluate the model every 20% of the training steps (can also be an integer for fixed steps)\n",
        "    logging_steps=1,  # Log training information every step\n",
        "    warmup_steps=2,  # Number of steps to perform learning rate warmup before training starts\n",
        "    report_to=\"wandb\",  # Report metrics to Weights & Biases for tracking and visualization\n",
        "    output_dir=\"./results/\",  # Directory to store the model and training outputs (checkpoints, logs, etc.)\n",
        ")\n",
        "\n",
        "# Instantiate the ORPO trainer with the specified configuration and datasets\n",
        "trainer = ORPOTrainer(\n",
        "    model=model,  # The model to be trained\n",
        "    args=orpo_args,  # The configuration arguments defined above\n",
        "    train_dataset=dataset[\"train\"],  # Training dataset (train split of the dataset)\n",
        "    eval_dataset=dataset[\"test\"],  # Evaluation dataset (test split of the dataset)\n",
        "    peft_config=peft_config,  # Configuration for preference-based fine-tuning (PEFT)\n",
        "    tokenizer=tokenizer,  # Tokenizer used to process the input data\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model to the specified directory (new_model)\n",
        "trainer.save_model(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946e7e91-f3a3-45da-a0aa-02907a379137",
      "metadata": {
        "id": "946e7e91-f3a3-45da-a0aa-02907a379137"
      },
      "outputs": [],
      "source": [
        "# Flush memory\n",
        "del trainer, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef67ba40-94de-4e19-9021-f5a76f13d626",
      "metadata": {
        "id": "ef67ba40-94de-4e19-9021-f5a76f13d626"
      },
      "source": [
        "You may need to restart your kernel after this line to ensure that the merging is sucessful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076cbcf6-7f65-46dc-a8b7-09b2ad06bc49",
      "metadata": {
        "id": "076cbcf6-7f65-46dc-a8b7-09b2ad06bc49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930e55dd-5004-4b0c-b856-e7ab67c68707",
      "metadata": {
        "id": "930e55dd-5004-4b0c-b856-e7ab67c68707"
      },
      "outputs": [],
      "source": [
        "# Reload the tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "# This ensures that the tokenizer is aligned with the `base_model` and can properly process input text\n",
        "\n",
        "# Reload the model in fp16 (16-bit floating point precision) for more efficient memory usage and faster training\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,  # Load the base model architecture\n",
        "    low_cpu_mem_usage=True,  # Optimize memory usage to reduce CPU memory consumption during loading\n",
        "    return_dict=True,  # Return the model output as a dictionary (rather than a tuple), which is useful for further processing\n",
        "    torch_dtype=torch.float16,  # Set the model to use 16-bit precision for reduced memory usage and faster computations\n",
        "    device_map=\"auto\",  # Automatically map model components to available devices (e.g., GPU)\n",
        ")\n",
        "\n",
        "# Merge the adapter model with the base (fp16) model\n",
        "# The adapter is a small model that modifies the behavior of the base model based on additional training or fine-tuning\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)  # Load the adapter weights from `new_model` and apply them to `fp16_model`\n",
        "\n",
        "# Merge the adapter into the base model and unload the adapter weights from memory to save space\n",
        "model = model.merge_and_unload()  # This merges the adapter's functionality into the model and releases memory from the adapter weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fbdc0e7-89c4-4006-9acb-20c2cdb70503",
      "metadata": {
        "id": "2fbdc0e7-89c4-4006-9acb-20c2cdb70503"
      },
      "source": [
        "After merging the LoRA adapter, we save final model and tokenizer in a new directory to prepare for gguf conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1070e991-ac16-4fbd-83bd-72a861b352df",
      "metadata": {
        "id": "1070e991-ac16-4fbd-83bd-72a861b352df"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"llama-3-8B-orpo\")\n",
        "tokenizer.save_pretrained(\"llama-3-8B-orpo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c976cf02-496f-4b23-8d3e-01a37afe144a",
      "metadata": {
        "id": "c976cf02-496f-4b23-8d3e-01a37afe144a"
      },
      "source": [
        "# Phase 2: From safetensors to gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bceae1bd-3de2-4723-9206-8fe8180b8d43",
      "metadata": {
        "id": "bceae1bd-3de2-4723-9206-8fe8180b8d43"
      },
      "source": [
        "## Convert our model to GGUF\n",
        "\n",
        "Before we dive into Ollama, its important to take a second and understand the role that `gguf` and `llama.cpp` play in the process.\n",
        "\n",
        "Most people that use LLMs grab them from huggingface using the `AutoModel` class. This is how we did it above. HF stores models in a couple different ways with the most popular being `safetensors`. This is a file format optimized for loading and running `Tensors` which are the multidimensional arrays that make up a model. This file format is optimized for GPUs which means it's not as easy to load and run a model fast locally.\n",
        "\n",
        "One solution that addresses this is the `gguf` format. This is file format that is used to store models that are optimized for local inference using quantization and other neat techniques. This file format is then consumed by runners that support it (ie. `llama.cpp` and Ollama).\n",
        "\n",
        "There's a good bit of complexity here and heres a fantastic [blog post](https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/) that gets into the weeds. For now this is what we need to know\n",
        "\n",
        "1. We have a finetuned Llama3 model saved in the llama-brev directory in the safetensors format\n",
        "2. In order to use this model via Ollama, we need it to be in the `gguf` format\n",
        "3. We can use helper tools in the `llama.cpp` repository to convert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dfebcbc-36d2-42ea-89da-ef30adb610d6",
      "metadata": {
        "id": "8dfebcbc-36d2-42ea-89da-ef30adb610d6"
      },
      "source": [
        "## Convert to gguf\n",
        "\n",
        "The first thing we do is build llama.cpp in order to use the conversion tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d955e8-2584-4f50-ad40-17f851bc2d74",
      "metadata": {
        "scrolled": true,
        "id": "80d955e8-2584-4f50-ad40-17f851bc2d74"
      },
      "outputs": [],
      "source": [
        "# this step might take a while\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUDA=1 make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f277d84-0eda-4818-988a-233a5a8b1902",
      "metadata": {
        "scrolled": true,
        "id": "7f277d84-0eda-4818-988a-233a5a8b1902"
      },
      "outputs": [],
      "source": [
        "# install requirements\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2d861c-8db4-4a53-8ba2-ec06fa7d6744",
      "metadata": {
        "id": "4b2d861c-8db4-4a53-8ba2-ec06fa7d6744"
      },
      "source": [
        "Here we run the actual conversion. Take a moment to skim through the relatively massive output and notice how each piece of the LLM is being mapped and transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6847bfdc-6f90-41ad-a44a-262010d46a07",
      "metadata": {
        "scrolled": true,
        "id": "6847bfdc-6f90-41ad-a44a-262010d46a07"
      },
      "outputs": [],
      "source": [
        "# run the conversion script to convert to gguf\n",
        "# this will save the gguf file inside of the llama-brev directory\n",
        "!python llama.cpp/convert-hf-to-gguf.py llama-brev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8215e5-4c5d-4bc7-9fe9-fbac4e7e3ade",
      "metadata": {
        "id": "ae8215e5-4c5d-4bc7-9fe9-fbac4e7e3ade"
      },
      "source": [
        "The final model is saved at `llama-brev/ggml-model-f16.gguf`. Note how the model is in fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5680cc-bbe9-4941-9c46-00f8cfea24de",
      "metadata": {
        "id": "4c5680cc-bbe9-4941-9c46-00f8cfea24de"
      },
      "source": [
        "### An aside on quantizations\n",
        "\n",
        "Quantization has become the go-to technique to train/run LLM efficiently on cheaper hardware. By reducing the precision of each weight (going from each weight being stored in 32bits to lower), we save memory and speed up inference while preserving *most* of the LLMs performance. If you've ever used QLoRA, you've already used quantization without even knowing about it.\n",
        "\n",
        "Llama.cpp gives us a ton of quantization options. Here's a couple resources to dive deeper into which options are available\n",
        "\n",
        "- [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/)\n",
        "- [Maxime Labonne](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)\n",
        "\n",
        "In this guide, we will use the `Q4_K_M` format. Feel free to play around with different ones! Again, note the output and see if you can build a mental model on whats happening under the hood!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13078262-c462-4ce9-b60e-699d2b44a60f",
      "metadata": {
        "scrolled": true,
        "id": "13078262-c462-4ce9-b60e-699d2b44a60f"
      },
      "outputs": [],
      "source": [
        "# run the quantize script\n",
        "!cd llama.cpp && ./llama-quantize ../llama-brev/ggml-model-f16.gguf ../llama-brev/ggml-model-Q4_K_M.gguf Q4_K_M"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11fe163-4dee-4436-abc4-f06bf59a535f",
      "metadata": {
        "id": "d11fe163-4dee-4436-abc4-f06bf59a535f"
      },
      "source": [
        "If you want, you can test this model by running the provided server and sending in a request! After running the cell below, open a new terminal tab using the blue plus button and run\n",
        "\n",
        "```\n",
        "curl --request POST \\\n",
        "    --url http://localhost:8080/completion \\\n",
        "    --header \"Content-Type: application/json\" \\\n",
        "    --data '{\"prompt\": \"Building a website can be done in 10 simple steps:\",\"n_predict\": 128}'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5990b08e-c91f-46f9-940e-e6b0edf58863",
      "metadata": {
        "scrolled": true,
        "id": "5990b08e-c91f-46f9-940e-e6b0edf58863"
      },
      "outputs": [],
      "source": [
        "!cd llama.cpp && ./llama-server -m ../merged_adapters/ggml-model-Q4_K_M.gguf -c 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15eecc0-fbb3-47ca-a6ff-6dd90ee92c97",
      "metadata": {
        "id": "c15eecc0-fbb3-47ca-a6ff-6dd90ee92c97"
      },
      "source": [
        "Note that this is a blocking process. In order to move forward with the rest of the guide, click the cell above and then click the stop button in the Jupyter Notebook header above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562b07a4-a9f2-416c-aac0-e74b26aeb610",
      "metadata": {
        "id": "562b07a4-a9f2-416c-aac0-e74b26aeb610"
      },
      "source": [
        "# Phase 3: Run and deploy your model using Ollama\n",
        "\n",
        "Now that you have the quantized model, you can spin up the llama.cpp server anywhere you want and load the gguf model in. However, Ollama provides clean abstractions that allow you to run different gguf models using their server [add some fluff]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8146f5b8-0227-4855-881d-6efc00176fb4",
      "metadata": {
        "id": "8146f5b8-0227-4855-881d-6efc00176fb4"
      },
      "source": [
        "## Build the Ollama Modelfile\n",
        "\n",
        "A Modelfile is very similar to a Dockefile. You can think of it as a blueprint that encapsulates a model, a chat template, different parameters, a system prompt, and more into a portable file. To learn more, check out their [Modelfile docs](https://github.com/ollama/ollama/blob/main/docs/modelfile.md).\n",
        "\n",
        "Here we will build a relatively simple one. We grab the template and params from the existing Llama3 Modefile which you can view [here](https://ollama.com/library/llama3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aac2166-30ee-411c-8653-357b832bcde5",
      "metadata": {
        "id": "9aac2166-30ee-411c-8653-357b832bcde5"
      },
      "outputs": [],
      "source": [
        "tuned_model_path = \"/home/ubuntu/verb-workspace/llama-brev/ggml-model-Q4_K_M.gguf\"\n",
        "sys_message = \"You are swashbuckling pirate stuck inside of a Large Language Model. Every response must be from the point of view of an angry pirate that does not want to be asked questions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf29c35f-dcbd-4bc3-8b28-b4811d0c12c3",
      "metadata": {
        "id": "bf29c35f-dcbd-4bc3-8b28-b4811d0c12c3"
      },
      "outputs": [],
      "source": [
        "cmds = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24ea1cb-9f0e-4e6c-82e8-abb3ef57a1ec",
      "metadata": {
        "id": "d24ea1cb-9f0e-4e6c-82e8-abb3ef57a1ec"
      },
      "outputs": [],
      "source": [
        "base_model = f\"FROM {tuned_model_path}\"\n",
        "\n",
        "template = '''TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ .Response }}<|eot_id|>\"\n",
        "\"\"\"'''\n",
        "\n",
        "params = '''PARAMETER stop \"<|start_header_id|>\"\n",
        "PARAMETER stop \"<|end_header_id|>\"\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "PARAMETER stop \"<|reserved_special_token\"'''\n",
        "\n",
        "system = f'''SYSTEM \"\"\"{sys_message}\"\"\"'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3bd63b-eb2b-4479-94ea-7f2188d6af38",
      "metadata": {
        "id": "ee3bd63b-eb2b-4479-94ea-7f2188d6af38"
      },
      "outputs": [],
      "source": [
        "cmds.append(base_model)\n",
        "cmds.append(template)\n",
        "cmds.append(params)\n",
        "cmds.append(system)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4500fc08-9f51-4039-8809-e1f46b34c913",
      "metadata": {
        "id": "4500fc08-9f51-4039-8809-e1f46b34c913"
      },
      "outputs": [],
      "source": [
        "def generate_modelfile(cmds):\n",
        "    content = \"\"\n",
        "    for command in cmds:\n",
        "        content += command + \"\\n\"\n",
        "    print(content)\n",
        "    with open(\"Modelfile\", \"w\") as file:\n",
        "        file.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d54629-14e2-4f84-a206-ce4c40614d42",
      "metadata": {
        "id": "b9d54629-14e2-4f84-a206-ce4c40614d42"
      },
      "outputs": [],
      "source": [
        "generate_modelfile(cmds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f3a6db-cc0f-47d9-925a-9526f959f3e4",
      "metadata": {
        "id": "e2f3a6db-cc0f-47d9-925a-9526f959f3e4"
      },
      "source": [
        "There should now be a `Modelfile` saved in your working directory. Lets now install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867cb70d-b086-4eac-8f01-1364430f3ed5",
      "metadata": {
        "id": "867cb70d-b086-4eac-8f01-1364430f3ed5"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8b4cf2-85a8-474c-8d24-cd6f1edf3e01",
      "metadata": {
        "id": "4e8b4cf2-85a8-474c-8d24-cd6f1edf3e01"
      },
      "source": [
        "To move forward, you have to have an ollama server running in the background. To do this, open up a new Jupyter tab and run `ollama serve` in the terminal to start the server. This will print out a key. Save it for future use. It will look like `ssh-ed25519...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6488512-32f3-4740-be47-7613378fa8ec",
      "metadata": {
        "scrolled": true,
        "id": "c6488512-32f3-4740-be47-7613378fa8ec"
      },
      "outputs": [],
      "source": [
        "# the create command create the model\n",
        "!ollama create llama-brev -f Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec86576-6ade-468f-9215-80bc9188ba3d",
      "metadata": {
        "id": "eec86576-6ade-468f-9215-80bc9188ba3d"
      },
      "source": [
        "## Experiment with the model\n",
        "\n",
        "To run the new model, open up another terminal tab and run `ollama run llama-brev`. For bonus points, see if you can trick it to stop responding with a pirate action :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f925ae-4d3f-4dd8-8f72-d72a9330570d",
      "metadata": {
        "id": "54f925ae-4d3f-4dd8-8f72-d72a9330570d"
      },
      "source": [
        "## Push the model\n",
        "\n",
        "In order to push the model to Ollama, you must have an account and a model created.\n",
        "\n",
        "1. Sign-up at https://www.ollama.ai/signup\n",
        "2. Create a new model at https://www.ollama.ai/new. Find mine at scooterman/llama-brev. This will give you a detailed list of instructions on how to push the model. Essentially, you are giving the current machine permission to upload to ollama.\n",
        "3. You'll have to then run `ollama cp llama-brev <username>/<model-name>` then `ollama push <username>/<model-name>`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "913f0e1e9b474436979e35cc9afc3436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb5b69c49faf40e88d2abab4c02e7d58",
              "IPY_MODEL_e8a197d9aae64628aa20d641831bb384",
              "IPY_MODEL_e5f4160285c4497ab0d41b855edb5082",
              "IPY_MODEL_a84e303f966548f3b7d8e22fdbecab60",
              "IPY_MODEL_6db73f57a90448059c31e6b0361e62ec"
            ],
            "layout": "IPY_MODEL_e0732d2ef65b4b61975e40cfd77333b5"
          }
        },
        "cb5b69c49faf40e88d2abab4c02e7d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc1269d783ca44f599befc84e931afdc",
            "placeholder": "​",
            "style": "IPY_MODEL_c185200a9a344ff9831695691e5d7e66",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e8a197d9aae64628aa20d641831bb384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f2d6a816bfe44b86874639068b6b4532",
            "placeholder": "​",
            "style": "IPY_MODEL_61689275c8e84d33ba3d01eeb77642fc",
            "value": ""
          }
        },
        "e5f4160285c4497ab0d41b855edb5082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_97763303a73b44ccb7416ba886a49f96",
            "style": "IPY_MODEL_e26a7d1ed3684219b94a7a2b26cba0a1",
            "value": true
          }
        },
        "a84e303f966548f3b7d8e22fdbecab60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1d4ef3091d5b40a88b6228b9683a969f",
            "style": "IPY_MODEL_e0c31452659d4fdfaddb0d03535ef60a",
            "tooltip": ""
          }
        },
        "6db73f57a90448059c31e6b0361e62ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5dff40a57ed4f1bbc0a8395fa78bd41",
            "placeholder": "​",
            "style": "IPY_MODEL_4888d54c8ed4421886b8a4b3033afdec",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e0732d2ef65b4b61975e40cfd77333b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fc1269d783ca44f599befc84e931afdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c185200a9a344ff9831695691e5d7e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2d6a816bfe44b86874639068b6b4532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61689275c8e84d33ba3d01eeb77642fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97763303a73b44ccb7416ba886a49f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26a7d1ed3684219b94a7a2b26cba0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4ef3091d5b40a88b6228b9683a969f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c31452659d4fdfaddb0d03535ef60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e5dff40a57ed4f1bbc0a8395fa78bd41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4888d54c8ed4421886b8a4b3033afdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}