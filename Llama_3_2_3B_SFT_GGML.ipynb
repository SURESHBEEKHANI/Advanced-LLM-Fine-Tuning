{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Llama_3_2_3B_SFT_GGML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede",
      "metadata": {
        "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede"
      },
      "source": [
        "### **Fine-Tuning Meta-Llama-3.2-3B Used unsloth for CPU and GPU Inference - GGML**\n",
        "\n",
        "On September 25, 2024, Meta introduced Llama 3.2, a collection of multilingual large language models (LLMs) in 1B and 3B sizes. These models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. Notably, the Llama 3.2 1B and 3B models support a context length of 128K tokens, making them suitable for extensive text processing tasks.\n",
        "HUGGING FACE\n",
        "\n",
        "To access the Llama 3.2-1B model, you can download it from [Hugging Face](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct) The approval process is typically swift, often taking about 20 minutes.\n",
        "HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a7bfcf-0552-433f-8afb-145e661ab34a",
      "metadata": {
        "id": "01a7bfcf-0552-433f-8afb-145e661ab34a"
      },
      "source": [
        "### Table of Contents\n",
        "1. Install dependancies\n",
        "2. Download model\n",
        "3. Fintuning flow\n",
        "4. convert GGML formate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a",
      "metadata": {
        "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a"
      },
      "source": [
        "## Step 1: Install All the Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bace79c2-2f05-4c2b-bbea-db212e93839d",
      "metadata": {
        "id": "bace79c2-2f05-4c2b-bbea-db212e93839d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import necessary libraries Load model and tokenizer"
      ],
      "metadata": {
        "id": "lgU5P1iGA3F1"
      },
      "id": "lgU5P1iGA3F1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be21a74-407f-49a9-83e9-771cf90d29b1",
      "metadata": {
        "id": "2be21a74-407f-49a9-83e9-771cf90d29b1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configuration settings\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # Use token if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "6kJJwkhahEY8"
      },
      "id": "6kJJwkhahEY8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b422e214-977a-4c45-9d66-b6a698c546eb",
      "metadata": {
        "id": "b422e214-977a-4c45-9d66-b6a698c546eb"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I'm great thanks!<|eot_id|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
      ],
      "metadata": {
        "id": "pd46xmlwO0GA"
      },
      "id": "pd46xmlwO0GA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the dataset**"
      ],
      "metadata": {
        "id": "2N_KLFXFPiOe"
      },
      "id": "2N_KLFXFPiOe"
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Assuming tokenizer is already defined, and chat_template is set to \"llama-3.1\"\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "\n",
        "# Function to format the prompts based on conversation examples\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load the dataset and slice to get the first 500 records\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "dataset_500 = dataset.select(range(500))  # Select only the first 500 records\n",
        "\n",
        "# Apply formatting function to the 500-record subset\n",
        "formatted_dataset = dataset_500.map(formatting_prompts_func, batched=True)\n"
      ],
      "metadata": {
        "id": "vF8DbtFHPS3y"
      },
      "id": "vF8DbtFHPS3y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}