{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Llama_3_2_3B_SFT_GGML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede",
      "metadata": {
        "id": "e0dd4a63-5f87-43e9-a67f-ca9e00b48ede"
      },
      "source": [
        "### **Fine-Tuning Meta-Llama-3.2-3B Used unsloth for CPU and GPU Inference - GGML**\n",
        "\n",
        "On September 25, 2024, Meta introduced Llama 3.2, a collection of multilingual large language models (LLMs) in 1B and 3B sizes. These models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. Notably, the Llama 3.2 1B and 3B models support a context length of 128K tokens, making them suitable for extensive text processing tasks.\n",
        "HUGGING FACE\n",
        "\n",
        "To access the Llama 3.2-1B model, you can download it from [Hugging Face](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct) The approval process is typically swift, often taking about 20 minutes.\n",
        "HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a7bfcf-0552-433f-8afb-145e661ab34a",
      "metadata": {
        "id": "01a7bfcf-0552-433f-8afb-145e661ab34a"
      },
      "source": [
        "### Table of Contents\n",
        "1. Install dependancies\n",
        "2. Download model\n",
        "3. Fintuning flow\n",
        "4. convert GGML formate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a",
      "metadata": {
        "id": "b2fab017-7a3d-4322-8ec7-bc20eabe7e9a"
      },
      "source": [
        "## Step 1: Install All the Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bace79c2-2f05-4c2b-bbea-db212e93839d",
      "metadata": {
        "id": "bace79c2-2f05-4c2b-bbea-db212e93839d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import necessary libraries Load model and tokenizer"
      ],
      "metadata": {
        "id": "lgU5P1iGA3F1"
      },
      "id": "lgU5P1iGA3F1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be21a74-407f-49a9-83e9-771cf90d29b1",
      "metadata": {
        "id": "2be21a74-407f-49a9-83e9-771cf90d29b1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configuration settings\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # Use token if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "6kJJwkhahEY8"
      },
      "id": "6kJJwkhahEY8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b422e214-977a-4c45-9d66-b6a698c546eb",
      "metadata": {
        "id": "b422e214-977a-4c45-9d66-b6a698c546eb"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a03b3f-5398-455e-880c-f503cba51821",
      "metadata": {
        "id": "c3a03b3f-5398-455e-880c-f503cba51821"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5081c400-f9c0-403b-b145-0e75e5a9982e",
      "metadata": {
        "id": "5081c400-f9c0-403b-b145-0e75e5a9982e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77919e07-7495-4a50-82c7-99459ba815c2",
      "metadata": {
        "id": "77919e07-7495-4a50-82c7-99459ba815c2"
      },
      "source": [
        "## 3. Set our Training Arguments\n",
        "\n",
        "A lot of tutorials simply paste a list of arguments leaving it up to the reader to figure out what each argument does. Below I've added comments which explain what each argument does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614e0a54-2740-438d-a72c-f3c0215c6558",
      "metadata": {
        "id": "614e0a54-2740-438d-a72c-f3c0215c6558"
      },
      "outputs": [],
      "source": [
        "# Output directory where the results and checkpoint are stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs - how many times does the model see the whole dataset\n",
        "num_train_epochs = 1 #Increase this for a larger finetune\n",
        "\n",
        "# Enable fp16/bf16 training. This is the type of each weight. Since we are on an A100\n",
        "# we can set bf16 to true because it can handle that type of computation\n",
        "bf16 = True\n",
        "\n",
        "# Batch size is the number of training examples used to train a single forward and backward pass.\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Gradients are accumulated over multiple mini-batches before updating the model weights.\n",
        "# This allows for effectively training with a larger batch size on hardware with limited memory\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# memory optimization technique that reduces RAM usage during training by intermittently storing\n",
        "# intermediate activations instead of retaining them throughout the entire forward pass, trading\n",
        "# computational time for lower memory consumption.\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 5\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 100\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba492aa-ad13-46d4-be7b-923240b6a133",
      "metadata": {
        "id": "0ba492aa-ad13-46d4-be7b-923240b6a133"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bec1455-c08d-4671-bb3c-48cf07ec86a8",
      "metadata": {
        "id": "7bec1455-c08d-4671-bb3c-48cf07ec86a8"
      },
      "source": [
        "## Run our training job using WandB for logging\n",
        "\n",
        "Weights and Biases is industry standard for monitoring and evaluating your training job. I highly suggest setting up an account to monitor this run and use it for future ML jobs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1250d948-e191-45a4-bd57-1fd9226666a0",
      "metadata": {
        "scrolled": true,
        "id": "1250d948-e191-45a4-bd57-1fd9226666a0"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57e00a2-a90a-4e43-a79d-2fe98dda58f6",
      "metadata": {
        "id": "f57e00a2-a90a-4e43-a79d-2fe98dda58f6"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93b0c17-77af-4328-889a-d418d0814101",
      "metadata": {
        "id": "c93b0c17-77af-4328-889a-d418d0814101"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6ad908-f10e-4605-bff3-efdfbe722a5c",
      "metadata": {
        "id": "3c6ad908-f10e-4605-bff3-efdfbe722a5c"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef576ca-fa8d-4045-8a75-3e74116d4abf",
      "metadata": {
        "id": "1ef576ca-fa8d-4045-8a75-3e74116d4abf"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}