{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Llama 3.2 Vision\n",
        "**fine-tune a multimodal model by Meta AI on the Amazon product dataset using the Unsloth framework**"
      ],
      "metadata": {
        "id": "443Pt2LEIwmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Install and upgrade UnSloth library for optimized model training\n",
        "\n",
        "- Use `%%capture` to suppress installation output in Jupyter/Colab environments.\n",
        "- Install the `unsloth` package from PyPI for initial setup.\n",
        "- Uninstall the existing `unsloth` package to ensure a clean installation.\n",
        "- Upgrade to the latest version of `unsloth` directly from the GitHub repository."
      ],
      "metadata": {
        "id": "MB-5a2vvJg9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bDTd1MSsVnY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# The `%%capture` magic in Jupyter/Colab captures output, suppressing it from being displayed.\n",
        "# Install the `unsloth` package from PyPI\n",
        "!pip install unsloth\n",
        "# Uninstall `unsloth` to ensure a clean installation, then install the latest version from GitHub\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the model\n",
        "\n",
        "\n",
        "Load the `unsloth/Llama-3.2-11B-Vision-Instruct` model using FastVisionModel.\n",
        "\n",
        "Enable 4-bit quantization to reduce memory usage.\n",
        "Utilize UnSloth's gradient checkpointing for efficient training and inference.\n"
      ],
      "metadata": {
        "id": "ZZGiemxYJE4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "id": "DnQHDnjrGEe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Add LoRA fine-tuning configuration for Llama 3.2 Vision model\n",
        "\n",
        "- Apply LoRA (Low-Rank Adaptation) to fine-tune the vision and language components of the model.\n",
        "- Enable fine-tuning for vision layers, language layers, attention modules, and MLP modules.\n",
        "- Configure LoRA parameters such as rank (`r`), alpha (`lora_alpha`), and dropout (`lora_dropout`).\n"
      ],
      "metadata": {
        "id": "b7q0LSXRJaPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 16,\n",
        "    lora_dropout               = 0,\n",
        "    bias                       = \"none\",\n",
        "    random_state               = 3407,\n",
        "    use_rslora                 = False,\n",
        "    loftq_config               = None,\n",
        ")"
      ],
      "metadata": {
        "id": "Ug0jCsRIGKB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.  Load a subset of the Amazon Product Descriptions dataset for Vision-Language tasks\n",
        "\n",
        "- Load the `philschmid/amazon-product-descriptions-vlm` dataset using the Hugging Face `datasets` library.\n",
        "- Select a subset of the training data (first 500 samples) for faster experimentation and prototyping.\n"
      ],
      "metadata": {
        "id": "qL6EyEBSJo8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\",\n",
        "                       split=\"train[0:500]\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "phKh9rQ-GPCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[45][\"description\"]"
      ],
      "metadata": {
        "id": "K_1fqY1BGZKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.  Convert Amazon product descriptions dataset into conversation format for Vision-Language Models\n",
        "\n",
        "- Define a system instruction for generating product descriptions based on images.\n",
        "- Implement a `convert_to_conversation` function to transform dataset samples into a conversation-like structure suitable for Vision-Language Models (VLMs).\n",
        "- Apply the transformation to the dataset to create a new `converted_dataset`.\n"
      ],
      "metadata": {
        "id": "lM6Sc6UkJtUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "You are an expert Amazon worker who is good at writing product descriptions.\n",
        "Write the product description accurately by looking at the image.\n",
        "\"\"\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "# Apply the conversion to each sample in the dataset\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ],
      "metadata": {
        "id": "23ep8xCbGbjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Enable inference on Llama 3.2 Vision model for generating product descriptions\n",
        "\n",
        "- Prepare the model for inference using `FastVisionModel.for_inference`.\n",
        "- Generate a product description by processing an image and instruction using the Vision-Language Model.\n",
        "- Use a streaming approach to display generated text in real-time.\n"
      ],
      "metadata": {
        "id": "TmDvSWdYKCZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[45][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "IWbxAUh2J6vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Set up SFTTrainer for fine-tuning Llama 3.2 Vision model on product descriptions\n",
        "\n",
        "- Enable the model for training using `FastVisionModel.for_training`.\n",
        "- Configure the `SFTTrainer` from the `trl` library for supervised fine-tuning (SFT) with UnSloth optimizations.\n",
        "- Use `UnslothVisionDataCollator` to handle multi-modal data efficiently during training.\n"
      ],
      "metadata": {
        "id": "PVyHtFP2KJPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model)  # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # For Weights and Biases\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "GNFYowCcIZml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Training the model\n",
        "Start the training process by running the trainer.train() code."
      ],
      "metadata": {
        "id": "GwnUDTMtKlzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "1vSgM_IzKlhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Saving the model and tokenizer"
      ],
      "metadata": {
        "id": "SU8f_mBHK9gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "rknoJrP3LSXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Model inference after fine-tuning\n",
        "\n",
        " Enable inference for Llama 3.2 Vision model to generate product descriptions\n",
        "\n",
        "- Prepare the model for inference using `FastVisionModel.for_inference`.\n",
        "- Generate a product description by processing an image and instruction with the Vision-Language Model.\n",
        "- Stream generated text in real-time using `TextStreamer`.\n"
      ],
      "metadata": {
        "id": "GuZ_exM4Kvvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model=\"lora_model\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[40][\"image\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ogqH85sWKvix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then use push_to_hub to save the model on the Hugging Face hub  GGUF format."
      ],
      "metadata": {
        "id": "dAktErqcT74Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/llama_3.2_vision_amazon_product_description\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"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\",  # Provide the Hugging Face token for authentication. Obtain a token at https://huggingface.co/settings/tokens\n",
        ")"
      ],
      "metadata": {
        "id": "zUdUQcXHT-P-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}