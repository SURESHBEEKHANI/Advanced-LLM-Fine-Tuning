{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9faf4b7db9e048feb65f65a48f5d475d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49c2455fe28b49faa46a798af0289078",
              "IPY_MODEL_22ca7c8a2ce54bcb8c9126a9194dc8e0",
              "IPY_MODEL_5b1da6f88e554e6f8a9b8d2659aef3c7"
            ],
            "layout": "IPY_MODEL_ca7b0e6f80a142efa836f395e834108a"
          }
        },
        "49c2455fe28b49faa46a798af0289078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f7f3e9bbf884837bc8bab708910f834",
            "placeholder": "​",
            "style": "IPY_MODEL_f798d93cf75d44eb8abff2d5979f9184",
            "value": "model.safetensors: 100%"
          }
        },
        "22ca7c8a2ce54bcb8c9126a9194dc8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31fb13632ee04df69f67dd9a9b822b76",
            "max": 6130708044,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_225e6a1bd80e49f38af3f2d74544fa95",
            "value": 6130707460
          }
        },
        "5b1da6f88e554e6f8a9b8d2659aef3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45313b03df3148bb8c05a63c3163b89d",
            "placeholder": "​",
            "style": "IPY_MODEL_cc544da273244d0ca2fb4a390ccb8bdb",
            "value": " 6.13G/6.13G [00:51&lt;00:00, 553MB/s]"
          }
        },
        "ca7b0e6f80a142efa836f395e834108a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f7f3e9bbf884837bc8bab708910f834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f798d93cf75d44eb8abff2d5979f9184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31fb13632ee04df69f67dd9a9b822b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225e6a1bd80e49f38af3f2d74544fa95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45313b03df3148bb8c05a63c3163b89d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc544da273244d0ca2fb4a390ccb8bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91776357f2064cbcb12e24c3a2706c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69ffc6d0e5de4dd4b595d67fa9a3c5cb",
              "IPY_MODEL_7bb691a3d0794361a9664f8b26bba0de",
              "IPY_MODEL_f70e096d4aa54e8b80b603836935854a"
            ],
            "layout": "IPY_MODEL_0e2053677dcb4ab1bb7373d934e658e2"
          }
        },
        "69ffc6d0e5de4dd4b595d67fa9a3c5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77c37217459e45529fc925ceafb47b71",
            "placeholder": "​",
            "style": "IPY_MODEL_1d55441e4ad441ad85e9bf39cc0044d7",
            "value": "generation_config.json: 100%"
          }
        },
        "7bb691a3d0794361a9664f8b26bba0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_425cba2a157e4a8c9d4916358a4d21a4",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70199fdb537c4eaa807e8a79e6e66954",
            "value": 190
          }
        },
        "f70e096d4aa54e8b80b603836935854a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d0f4b091fe4a5383002b7f9b5edcc3",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0436f71f1d4281bd181cf5d0f87ae7",
            "value": " 190/190 [00:00&lt;00:00, 12.2kB/s]"
          }
        },
        "0e2053677dcb4ab1bb7373d934e658e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c37217459e45529fc925ceafb47b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d55441e4ad441ad85e9bf39cc0044d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "425cba2a157e4a8c9d4916358a4d21a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70199fdb537c4eaa807e8a79e6e66954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1d0f4b091fe4a5383002b7f9b5edcc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0436f71f1d4281bd181cf5d0f87ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8e7ded3cf144e719d0d0455f3766113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f2bf8a9dbf04e17865500c9be32eb5e",
              "IPY_MODEL_76af731f23054eb4853efedfa98dbdaf",
              "IPY_MODEL_93b91adc114a4faabf9589dbbf8f8eb7"
            ],
            "layout": "IPY_MODEL_0d5c665501d2463b9ee8fabea35c2e50"
          }
        },
        "2f2bf8a9dbf04e17865500c9be32eb5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf3e239f2704a17938a7b3be03268c0",
            "placeholder": "​",
            "style": "IPY_MODEL_b84484ed4f7a46589d7f236fd8c5673d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "76af731f23054eb4853efedfa98dbdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2c3b4691f543739966673763e4aad3",
            "max": 46405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9f6633add647a68c7bca309fd3ecc5",
            "value": 46405
          }
        },
        "93b91adc114a4faabf9589dbbf8f8eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3277a328ad19437f9f600ebf0c592d86",
            "placeholder": "​",
            "style": "IPY_MODEL_9a7b00a545d048d88a0d7f5b48bb3bc8",
            "value": " 46.4k/46.4k [00:00&lt;00:00, 2.04MB/s]"
          }
        },
        "0d5c665501d2463b9ee8fabea35c2e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf3e239f2704a17938a7b3be03268c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84484ed4f7a46589d7f236fd8c5673d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d2c3b4691f543739966673763e4aad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9f6633add647a68c7bca309fd3ecc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3277a328ad19437f9f600ebf0c592d86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7b00a545d048d88a0d7f5b48bb3bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35d364504f014a1b86198911de101f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1cac8f291934270bdd9fb5f2f6ae823",
              "IPY_MODEL_dd88e70220e4472095bdbc0d2df2d838",
              "IPY_MODEL_9b493bda14aa4e83909debce7cb981e3"
            ],
            "layout": "IPY_MODEL_8e09e818129c451e95277335058a23c2"
          }
        },
        "c1cac8f291934270bdd9fb5f2f6ae823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90cf5baa69644399af8ef91c22787526",
            "placeholder": "​",
            "style": "IPY_MODEL_c7dd8060a79e432eaa54c67b8f6d0fd0",
            "value": "tokenizer.model: 100%"
          }
        },
        "dd88e70220e4472095bdbc0d2df2d838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b01911751ac44efa974d1e58a1310c7",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_057ee840267a416ca11ebea1b2a70ac6",
            "value": 4241003
          }
        },
        "9b493bda14aa4e83909debce7cb981e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf198d1151aa4344a1c3cbf501611302",
            "placeholder": "​",
            "style": "IPY_MODEL_90ab3668ee54480eaf4bb4a00857b7b5",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 15.8MB/s]"
          }
        },
        "8e09e818129c451e95277335058a23c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cf5baa69644399af8ef91c22787526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7dd8060a79e432eaa54c67b8f6d0fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b01911751ac44efa974d1e58a1310c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057ee840267a416ca11ebea1b2a70ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf198d1151aa4344a1c3cbf501611302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ab3668ee54480eaf4bb4a00857b7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00a8c09371b4291afd061e6002452f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_296b6bbb0fc84cdbb51708940f38733b",
              "IPY_MODEL_30466a53f15e4b5ab1f9e3bfe746675a",
              "IPY_MODEL_5190daedaddc4f72afccc7f2be66f61c"
            ],
            "layout": "IPY_MODEL_d7845ef5ce6f483c8576714fb161a3e9"
          }
        },
        "296b6bbb0fc84cdbb51708940f38733b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99a380a6c2624c88ad996700b5a8f737",
            "placeholder": "​",
            "style": "IPY_MODEL_c984b222cbe04c5dab47c4f833a6c0c1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "30466a53f15e4b5ab1f9e3bfe746675a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a2775a1fd8c44b4b854b314792ca8c5",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c50abd5dc68542f1b458b4135eb2873d",
            "value": 636
          }
        },
        "5190daedaddc4f72afccc7f2be66f61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbd2d4a4ed794d6ca151c910c933d185",
            "placeholder": "​",
            "style": "IPY_MODEL_6e18e612b39640228468d62facbf7000",
            "value": " 636/636 [00:00&lt;00:00, 53.7kB/s]"
          }
        },
        "d7845ef5ce6f483c8576714fb161a3e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a380a6c2624c88ad996700b5a8f737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c984b222cbe04c5dab47c4f833a6c0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a2775a1fd8c44b4b854b314792ca8c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50abd5dc68542f1b458b4135eb2873d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbd2d4a4ed794d6ca151c910c933d185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e18e612b39640228468d62facbf7000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12d2647df86495a8f3ce78163f01aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1f44d065512415390f0bd64c9d9df35",
              "IPY_MODEL_ba524f0d3ddc46f88775e8aef17117ef",
              "IPY_MODEL_44bd1bcde6ce447e8df08be1bea2a6cb"
            ],
            "layout": "IPY_MODEL_5bbb3e5d21bc4f3f841064597e886f60"
          }
        },
        "d1f44d065512415390f0bd64c9d9df35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dfd6f965074462cac2cd746d4c8f934",
            "placeholder": "​",
            "style": "IPY_MODEL_7adb2fef3f75421bbda08cba2756c859",
            "value": "tokenizer.json: 100%"
          }
        },
        "ba524f0d3ddc46f88775e8aef17117ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_879eeb8b05fd4275a6991969d95ee7c2",
            "max": 17525357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d428830bbc2408f8f9966cb57f2f3c7",
            "value": 17525357
          }
        },
        "44bd1bcde6ce447e8df08be1bea2a6cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff713f7ad034b84841f6a5f69b671f9",
            "placeholder": "​",
            "style": "IPY_MODEL_52522b25b753498598558502dc47f317",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 42.8MB/s]"
          }
        },
        "5bbb3e5d21bc4f3f841064597e886f60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfd6f965074462cac2cd746d4c8f934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7adb2fef3f75421bbda08cba2756c859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "879eeb8b05fd4275a6991969d95ee7c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d428830bbc2408f8f9966cb57f2f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ff713f7ad034b84841f6a5f69b671f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52522b25b753498598558502dc47f317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e4d0307868e481a83050099897a0c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10ec912ccea043059e5b3f6b170d9435",
              "IPY_MODEL_48a41963fa504801863a69c230bc0f2d",
              "IPY_MODEL_b8973cf30e31402197fafebe0410a7af"
            ],
            "layout": "IPY_MODEL_802f13bc8a004cd699261595659259c4"
          }
        },
        "10ec912ccea043059e5b3f6b170d9435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b56bedf0994bc2b8c2e8d8fca97c86",
            "placeholder": "​",
            "style": "IPY_MODEL_2ac9461d74234ab8858dcd04e2901cab",
            "value": "Map: 100%"
          }
        },
        "48a41963fa504801863a69c230bc0f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b85249ba494be1be912aac7c009c67",
            "max": 11989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cbcb9daa4944810b79dddefe640a0f1",
            "value": 11989
          }
        },
        "b8973cf30e31402197fafebe0410a7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec6521ddac4546a2a6d61bf96f9397e3",
            "placeholder": "​",
            "style": "IPY_MODEL_911f871ac500436f82273a8818ab6c15",
            "value": " 11989/11989 [00:02&lt;00:00, 3825.42 examples/s]"
          }
        },
        "802f13bc8a004cd699261595659259c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b56bedf0994bc2b8c2e8d8fca97c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac9461d74234ab8858dcd04e2901cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6b85249ba494be1be912aac7c009c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cbcb9daa4944810b79dddefe640a0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec6521ddac4546a2a6d61bf96f9397e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "911f871ac500436f82273a8818ab6c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "311fc17bf747439c9893fb713272214d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4167dbc6bed340a1ae725ef5fd9560f0",
              "IPY_MODEL_70f7f26e86224841ae154610c037f82c",
              "IPY_MODEL_90c95ee6ebf54b3c980255e43bc5cea5"
            ],
            "layout": "IPY_MODEL_50e43346da7040d9b823fe6b10e141dd"
          }
        },
        "4167dbc6bed340a1ae725ef5fd9560f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0720e95e1364bcbab7c177d626bd2c7",
            "placeholder": "​",
            "style": "IPY_MODEL_c0481fc10e4f4cf7ac9aaa7988913ade",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "70f7f26e86224841ae154610c037f82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1048a552dd6e4b0587d2dc6e257ed791",
            "max": 11989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cefa7c5086034c5490490ad14044916f",
            "value": 11989
          }
        },
        "90c95ee6ebf54b3c980255e43bc5cea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_984f3ac5c5a548f6a5572378ee224096",
            "placeholder": "​",
            "style": "IPY_MODEL_febcc14b85ea4139a2f91701f9776efb",
            "value": " 11989/11989 [00:15&lt;00:00, 1627.43 examples/s]"
          }
        },
        "50e43346da7040d9b823fe6b10e141dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0720e95e1364bcbab7c177d626bd2c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0481fc10e4f4cf7ac9aaa7988913ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1048a552dd6e4b0587d2dc6e257ed791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cefa7c5086034c5490490ad14044916f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "984f3ac5c5a548f6a5572378ee224096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "febcc14b85ea4139a2f91701f9776efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Finetune_Gemma_NRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install needed packages**\n"
      ],
      "metadata": {
        "id": "yWJhJKWnSui3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WjLPjwhSj2k"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading a Pre-trained Language Model with Custom Configuration for Efficient Memory Usage**"
      ],
      "metadata": {
        "id": "fGylRbdfJilw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel  # FastLanguageModel provides easy loading of pre-trained models.\n",
        "import torch  # PyTorch library for tensor computations and deep learning\n",
        "\n",
        "# Configuration settings for the model\n",
        "max_seq_length = 2048  # Define the maximum sequence length (2048 tokens in this case).\n",
        "# The model can handle larger sequence lengths, and RoPE (Rotary Positional Embedding) scaling will be applied internally.\n",
        "\n",
        "dtype = None  # The data type of the model's parameters. None means auto-detection.\n",
        "# If you're using a Tesla T4 or V100, you might use Float16 for better performance.\n",
        "# For Ampere+ (like A100, V100), Bfloat16 is usually a better option.\n",
        "\n",
        "load_in_4bit = True  # If set to True, 4-bit quantization is applied to the model weights, reducing memory usage.\n",
        "# This is useful for low-memory environments or when working with large models. Set to False to use full precision.\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-9b\",  # Specify the name of the pre-trained model to load (in this case, \"gemma-2-9b\").\n",
        "    max_seq_length = max_seq_length,  # Pass the defined maximum sequence length.\n",
        "    dtype = dtype,  # Pass the dtype configuration (None for auto-detection).\n",
        "    load_in_4bit = load_in_4bit,  # Pass the flag for using 4-bit quantization.\n",
        "    # token = \"hf_...\",  # Uncomment and provide a token if using gated models like meta-llama (for example, Llama-2-7b-hf).\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "9faf4b7db9e048feb65f65a48f5d475d",
            "49c2455fe28b49faa46a798af0289078",
            "22ca7c8a2ce54bcb8c9126a9194dc8e0",
            "5b1da6f88e554e6f8a9b8d2659aef3c7",
            "ca7b0e6f80a142efa836f395e834108a",
            "9f7f3e9bbf884837bc8bab708910f834",
            "f798d93cf75d44eb8abff2d5979f9184",
            "31fb13632ee04df69f67dd9a9b822b76",
            "225e6a1bd80e49f38af3f2d74544fa95",
            "45313b03df3148bb8c05a63c3163b89d",
            "cc544da273244d0ca2fb4a390ccb8bdb",
            "91776357f2064cbcb12e24c3a2706c52",
            "69ffc6d0e5de4dd4b595d67fa9a3c5cb",
            "7bb691a3d0794361a9664f8b26bba0de",
            "f70e096d4aa54e8b80b603836935854a",
            "0e2053677dcb4ab1bb7373d934e658e2",
            "77c37217459e45529fc925ceafb47b71",
            "1d55441e4ad441ad85e9bf39cc0044d7",
            "425cba2a157e4a8c9d4916358a4d21a4",
            "70199fdb537c4eaa807e8a79e6e66954",
            "d1d0f4b091fe4a5383002b7f9b5edcc3",
            "0d0436f71f1d4281bd181cf5d0f87ae7",
            "f8e7ded3cf144e719d0d0455f3766113",
            "2f2bf8a9dbf04e17865500c9be32eb5e",
            "76af731f23054eb4853efedfa98dbdaf",
            "93b91adc114a4faabf9589dbbf8f8eb7",
            "0d5c665501d2463b9ee8fabea35c2e50",
            "4bf3e239f2704a17938a7b3be03268c0",
            "b84484ed4f7a46589d7f236fd8c5673d",
            "4d2c3b4691f543739966673763e4aad3",
            "de9f6633add647a68c7bca309fd3ecc5",
            "3277a328ad19437f9f600ebf0c592d86",
            "9a7b00a545d048d88a0d7f5b48bb3bc8",
            "35d364504f014a1b86198911de101f25",
            "c1cac8f291934270bdd9fb5f2f6ae823",
            "dd88e70220e4472095bdbc0d2df2d838",
            "9b493bda14aa4e83909debce7cb981e3",
            "8e09e818129c451e95277335058a23c2",
            "90cf5baa69644399af8ef91c22787526",
            "c7dd8060a79e432eaa54c67b8f6d0fd0",
            "0b01911751ac44efa974d1e58a1310c7",
            "057ee840267a416ca11ebea1b2a70ac6",
            "bf198d1151aa4344a1c3cbf501611302",
            "90ab3668ee54480eaf4bb4a00857b7b5",
            "c00a8c09371b4291afd061e6002452f4",
            "296b6bbb0fc84cdbb51708940f38733b",
            "30466a53f15e4b5ab1f9e3bfe746675a",
            "5190daedaddc4f72afccc7f2be66f61c",
            "d7845ef5ce6f483c8576714fb161a3e9",
            "99a380a6c2624c88ad996700b5a8f737",
            "c984b222cbe04c5dab47c4f833a6c0c1",
            "8a2775a1fd8c44b4b854b314792ca8c5",
            "c50abd5dc68542f1b458b4135eb2873d",
            "bbd2d4a4ed794d6ca151c910c933d185",
            "6e18e612b39640228468d62facbf7000",
            "d12d2647df86495a8f3ce78163f01aad",
            "d1f44d065512415390f0bd64c9d9df35",
            "ba524f0d3ddc46f88775e8aef17117ef",
            "44bd1bcde6ce447e8df08be1bea2a6cb",
            "5bbb3e5d21bc4f3f841064597e886f60",
            "6dfd6f965074462cac2cd746d4c8f934",
            "7adb2fef3f75421bbda08cba2756c859",
            "879eeb8b05fd4275a6991969d95ee7c2",
            "5d428830bbc2408f8f9966cb57f2f3c7",
            "6ff713f7ad034b84841f6a5f69b671f9",
            "52522b25b753498598558502dc47f317"
          ]
        },
        "id": "H2EJLqgyJyGf",
        "outputId": "1da7c6f4-b828-4c5e-c1bd-a9aeecff3adb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Gemma2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9faf4b7db9e048feb65f65a48f5d475d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91776357f2064cbcb12e24c3a2706c52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8e7ded3cf144e719d0d0455f3766113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35d364504f014a1b86198911de101f25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c00a8c09371b4291afd061e6002452f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d12d2647df86495a8f3ce78163f01aad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **We now add LoRA adapters so we only need to update 1 to 10% of all parameters!**"
      ],
      "metadata": {
        "id": "N_dGT6z9LAf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Configure the model with PEFT (Parameter-Efficient Fine-Tuning) settings using LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base model to be fine-tuned using PEFT techniques\n",
        "\n",
        "    # Low-Rank Adaptation (LoRA) rank\n",
        "    r=16,  # Defines the rank of the low-rank matrices. Common choices: 8, 16, 32, 64, 128.\n",
        "    # Larger values increase expressiveness but require more memory.\n",
        "\n",
        "    # Modules to target for LoRA fine-tuning\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    # Only these specified modules will be fine-tuned to reduce memory and computational overhead.\n",
        "\n",
        "    # LoRA-specific hyperparameters\n",
        "    lora_alpha=16,  # Scaling factor for LoRA weights. Balances new and pre-trained weights.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA. Setting to 0 often gives optimized performance.\n",
        "\n",
        "    # Bias handling in fine-tuning\n",
        "    bias=\"none\",  # Specifies bias tuning. \"none\" is optimized for performance. Alternatives: \"all\", \"lora_only\".\n",
        "\n",
        "    # Optimizations for VRAM and context length\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use gradient checkpointing to save memory during training.\n",
        "    # The \"unsloth\" setting reduces VRAM usage by ~30%, allowing larger batch sizes or longer contexts.\n",
        "\n",
        "    # Random seed for reproducibility\n",
        "    random_state=3407,  # Ensures the results are reproducible across runs.\n",
        "\n",
        "    # Advanced fine-tuning features\n",
        "    use_rslora=False,  # Enables Rank-Stabilized LoRA (rSLoRA) if set to True. Useful for stability in high ranks.\n",
        "    loftq_config=None,  # Configures LoftQ (Low Overhead Fine-Tuning Quantization), if used. Set to None for default.\n",
        ")"
      ],
      "metadata": {
        "id": "UI7dWgo6E8GO",
        "outputId": "d1a7a5f6-fa31-4e90-f2a1-3e32d442360b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.7 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We are using the **Named Entity Recognition (NER)** dataset from [SURESHBEEKHANI](https://huggingface.co/datasets/SURESHBEEKHANI/Named_entity_recognition). This dataset is ideal for training on named entity recognition tasks, where the goal is to identify entities such as person names, locations, and organizations within the text.\n",
        "\n",
        "You can replace the dataset loading section with your own data preparation steps, depending on your specific use case.\n",
        "\n",
        "**[NOTE]** To train only on the output (ignoring any extra context like the user's input), please refer to [TRL's documentation](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** It’s important to add the **EOS_TOKEN** to the tokenized output. Without it, your model could generate an infinite sequence! This marks the end of the generated text and helps control the length of the output.\n",
        "\n",
        "For training on conversational datasets, you may want to use the `llama-3` template. We have prepared a conversational notebook that you can find [here](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "If your goal is to work with text completions (such as for creative writing), consider using this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing).\n"
      ],
      "metadata": {
        "id": "tb0_jF9WAAja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template for Alpaca-based task instruction\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "# The alpaca_prompt template includes placeholders for instruction, input, and output. These will be filled dynamically during processing.\n",
        "\n",
        "# EOS_TOKEN (End of Sequence Token) to signal the end of a generated sequence. It's necessary to prevent infinite generation.\n",
        "EOS_TOKEN = tokenizer.eos_token  # Retrieve the EOS token from the tokenizer.\n",
        "\n",
        "# Define a function to format the dataset examples into the Alpaca prompt format\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instructions\"]  # Extract instructions from the dataset.\n",
        "    inputs = examples[\"input\"]  # Extract input data from the dataset.\n",
        "    outputs = examples[\"Output\"]  # Extract the expected output from the dataset.\n",
        "\n",
        "    texts = []  # Initialize a list to store the formatted prompt texts.\n",
        "\n",
        "    # Loop through each instruction, input, and output in parallel.\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Format the alpaca_prompt with the current instruction, input, and output.\n",
        "        # EOS_TOKEN is added to signal the end of the text generation.\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)  # Append the formatted text to the list.\n",
        "\n",
        "    # Return the formatted texts in a dictionary with the key \"text\".\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Load a dataset from Hugging Face's datasets library\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"SURESHBEEKHANI/Named_entity_recognition\", split=\"train\")\n",
        "# This loads the \"train\" split of the \"Named_entity_recognition\" dataset by the user \"SURESHBEEKHANI\".\n",
        "\n",
        "# Apply the formatting function to the dataset using the `map` method.\n",
        "# This will apply the `formatting_prompts_func` to each example in the dataset in a batched manner.\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "AcLqLN7n_kmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5e4d0307868e481a83050099897a0c25",
            "10ec912ccea043059e5b3f6b170d9435",
            "48a41963fa504801863a69c230bc0f2d",
            "b8973cf30e31402197fafebe0410a7af",
            "802f13bc8a004cd699261595659259c4",
            "08b56bedf0994bc2b8c2e8d8fca97c86",
            "2ac9461d74234ab8858dcd04e2901cab",
            "f6b85249ba494be1be912aac7c009c67",
            "6cbcb9daa4944810b79dddefe640a0f1",
            "ec6521ddac4546a2a6d61bf96f9397e3",
            "911f871ac500436f82273a8818ab6c15"
          ]
        },
        "outputId": "d8be2ade-6994-444c-8900-f2b7587369bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11989 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e4d0307868e481a83050099897a0c25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the Model\n",
        "Next, let's train the model using Hugging Face TRL's `SFTTrainer`! For more details, check out the [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n",
        "\n",
        "To speed up the process, we perform 100 steps in this example, but you can modify the `num_train_epochs` parameter to `1` for a full training run and set `max_steps=None` for training based on the number of epochs instead.\n",
        "\n",
        "Additionally, we also support using TRL's `DPOTrainer` for other fine-tuning strategies, which you can explore if needed!\n"
      ],
      "metadata": {
        "id": "7RbeMMMUBL0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from trl import SFTTrainer  # SFTTrainer is used for training models in a parameter-efficient way.\n",
        "from transformers import TrainingArguments  # TrainingArguments contains configuration options for model training.\n",
        "from unsloth import is_bfloat16_supported  # Function to check if the hardware supports bfloat16 precision.\n",
        "\n",
        "# Initialize the SFTTrainer with the necessary arguments\n",
        "trainer = SFTTrainer(\n",
        "    model = model,  # The pre-trained model to be fine-tuned.\n",
        "    tokenizer = tokenizer,  # The tokenizer used to process input data for the model.\n",
        "    train_dataset = dataset,  # The dataset to train on, assumed to be preprocessed and formatted.\n",
        "    dataset_text_field = \"text\",  # Specifies which field in the dataset contains the input text. Here, it's the \"text\" field.\n",
        "    max_seq_length = max_seq_length,  # The maximum length of input sequences for the model. Ensures sequences are not longer than this value.\n",
        "    dataset_num_proc = 2,  # Number of CPU processes to use for data loading. 2 can speed up the dataset loading process.\n",
        "    packing = False,  # If set to True, sequences will be packed into a single tensor (can make training faster for short sequences).\n",
        "\n",
        "    # Define the training configuration through TrainingArguments\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,  # Batch size per device during training.\n",
        "        gradient_accumulation_steps = 4,  # Number of steps to accumulate gradients before performing an update (helps with memory efficiency).\n",
        "        warmup_steps = 5,  # Number of steps to perform learning rate warmup before training starts.\n",
        "        max_steps = 50,  # The total number of training steps. Once reached, training will stop.\n",
        "        learning_rate = 2e-4,  # Learning rate for the optimizer.\n",
        "\n",
        "        # fp16 (16-bit floating point) and bf16 (bfloat16) are precision modes used to speed up training and reduce memory usage.\n",
        "        fp16 = not is_bfloat16_supported(),  # Use fp16 if bfloat16 is not supported by the hardware.\n",
        "        bf16 = is_bfloat16_supported(),  # Use bf16 if supported by the hardware (typically for Ampere GPUs or newer).\n",
        "\n",
        "        logging_steps = 1,  # Log training progress every step. Setting this to a higher value reduces logging frequency.\n",
        "        optim = \"adamw_8bit\",  # Specifies the optimizer used during training (AdamW with 8-bit precision for memory efficiency).\n",
        "        weight_decay = 0.01,  # Regularization parameter to prevent overfitting by penalizing large weights.\n",
        "        lr_scheduler_type = \"linear\",  # Learning rate scheduler type. \"linear\" gradually decays the learning rate during training.\n",
        "        seed = 3407,  # Random seed for reproducibility of results.\n",
        "        output_dir = \"outputs\",  # Directory to save model checkpoints and logs during training.\n",
        "        report_to = \"none\",  # Specifies where to report metrics (e.g., \"none\" means no reporting, or use \"wandb\" for Weights & Biases).\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "jjXierjMAVvk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "311fc17bf747439c9893fb713272214d",
            "4167dbc6bed340a1ae725ef5fd9560f0",
            "70f7f26e86224841ae154610c037f82c",
            "90c95ee6ebf54b3c980255e43bc5cea5",
            "50e43346da7040d9b823fe6b10e141dd",
            "c0720e95e1364bcbab7c177d626bd2c7",
            "c0481fc10e4f4cf7ac9aaa7988913ade",
            "1048a552dd6e4b0587d2dc6e257ed791",
            "cefa7c5086034c5490490ad14044916f",
            "984f3ac5c5a548f6a5572378ee224096",
            "febcc14b85ea4139a2f91701f9776efb"
          ]
        },
        "outputId": "9105bcc9-665e-4a54-85e3-4256881cfb3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/11989 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "311fc17bf747439c9893fb713272214d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "PMNXjrsNB7qF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3ec0d7-4ae1-41fe-d2a9-afb8782a4659"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "6.879 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "189zoApYBqgI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efc4af44-c4f6-4621-bffa-bb03b1a69dda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 11,989 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 50\n",
            " \"-____-\"     Number of trainable parameters = 54,018,048\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 13:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.609300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.436900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.034900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.881100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.720700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.555400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.611000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.545700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.582900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.524500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.582000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.550800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.506800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.548300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.566500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.546900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.537900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.539300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.529600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.540500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.503100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.548800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.549200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.537800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.566100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.547400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.555400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.530200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.571800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "yq6rqXmsCBSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1493c211-89e4-4fbe-c031-b534d45d71de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "847.5897 seconds used for training.\n",
            "14.13 minutes used for training.\n",
            "Peak reserved memory = 13.617 GB.\n",
            "Peak reserved memory for training = 6.738 GB.\n",
            "Peak reserved memory % of max memory = 92.331 %.\n",
            "Peak reserved memory for training % of max memory = 45.688 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ],
      "metadata": {
        "id": "phRIcf7JCoNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "         alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"On the Republican side , Senator John McCain seems on the verge of clinching his party 's nomination \"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "# Generate the summary using the model.\n",
        "# 'max_new_tokens' controls how many tokens the model is allowed to generate.\n",
        "# 'use_cache' allows for faster generation by caching previous results.\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the generated tokens back into readable text.\n",
        "# This will give us the model's summary of the provided input text.\n",
        "tokenizer.batch_decode(outputs)  # Convert the output tokens to text."
      ],
      "metadata": {
        "id": "FUFi-IboCIse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bde858e-bc01-4f9f-ed1c-3f28eb3fc0f2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\\n\\n### input:\\nOn the Republican side , Senator John McCain seems on the verge of clinching his party \\'s nomination \\n\\n### Output:\\n[{\\'end\\': 10, \\'entity\\': \\'I-PER\\', \\'index\\': 2, \\'score\\': 0.99999994, \\'start\\': 7, \\'word\\': \\'John\\'}, {\\'end\\': 14, \\'entity\\': \\'I-PER\\',']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the prompt template for text summarization.\n",
        "alpaca_prompt = \"\"\"Your AI assistant for NER\", \"The entities are categorized into different types, such as PERSON, LOCATION, ORGANIZATION, etc.\", \"Please review the extracted entities for any potential errors or misclassifications.\", \"For improved accuracy, try using a more domain-specific NER mode.\n",
        "\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"  # The summary part is left empty for generation.\n",
        "\n",
        "# FastLanguageModel.for_inference(model) enables optimizations for faster inference.\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference by configuring the model for efficient use\n",
        "\n",
        "# Example of a text summarization task.\n",
        "# Here, you provide a longer piece of text as input, and the model will generate a concise summary.\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "         alpaca_prompt.format(  # Format the prompt with the input text and an empty placeholder for the summary.\n",
        "            \"\"\"On the Republican side , Senator John McCain seems on the verge of clinching his party 's nomination \"\"\"\n",
        "            ,  # Insert input text for summarization.\n",
        "            \"\"  # The summary section is empty for the model to fill in.\n",
        "        )\n",
        "    ], return_tensors=\"pt\"  # Convert input to PyTorch tensors.\n",
        ").to(\"cuda\")  # Move the input data to the GPU for faster processing.\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Initialize the TextStreamer to decode the generated tokens during streaming.\n",
        "# This facilitates immediate feedback on the model’s output.\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate the summary using the model, streaming token-by-token for faster results.\n",
        "# The model will produce a summary up to a maximum of 128 tokens.\n",
        "_ = model.generate(\n",
        "    **inputs,  # Provide the tokenized input text to the model.\n",
        "    streamer=text_streamer,  # Enable token-by-token streaming.\n",
        "    max_new_tokens=128  # Limit the number of tokens in the generated summary.\n",
        ")"
      ],
      "metadata": {
        "id": "71NTd17ECyQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters,"
      ],
      "metadata": {
        "id": "DJicn-DqDBpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "kjRsm4U0C8sM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1592b9b9-9c79-4682-b82c-10ca1323b6b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Push the trained model to the Hugging Face Model Hub using the GGUF format"
      ],
      "metadata": {
        "id": "7_okKJHSD5Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"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\",  # Provide the Hugging Face token for authentication. Obtain a token at https://huggingface.co/settings/tokens\n",
        ")\n"
      ],
      "metadata": {
        "id": "CQGcMr4wD6AS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c828842-4faf-4808-bb0e-5208229f0abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 6.1G\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 3.35 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 12/42 [00:01<00:02, 13.40it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|██████████| 42/42 [05:03<00:00,  7.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting gemma2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF into f16 GGUF format.\n",
            "The output location will be /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Finetune_Gemma_NRE_SFT_GGUF\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float16 --> F16, shape = {3584, 256000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.30.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.31.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.32.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.33.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.34.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.35.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.36.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.37.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.38.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.39.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.40.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.40.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.40.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.40.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.40.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.40.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.weight,              torch.float16 --> F16, shape = {3584, 4096}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.weight,              torch.float16 --> F16, shape = {3584, 2048}\n",
            "INFO:hf-to-gguf:blk.41.attn_output.weight,         torch.float16 --> F16, shape = {4096, 3584}\n",
            "INFO:hf-to-gguf:blk.41.ffn_gate.weight,            torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.41.ffn_up.weight,              torch.float16 --> F16, shape = {3584, 14336}\n",
            "INFO:hf-to-gguf:blk.41.ffn_down.weight,            torch.float16 --> F16, shape = {14336, 3584}\n",
            "INFO:hf-to-gguf:blk.41.attn_norm.weight,           torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.post_attention_norm.weight, torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.ffn_norm.weight,            torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.41.post_ffw_norm.weight,       torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 1\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf: n_tensors = 464, total_size = 18.5G\n",
            "Writing: 100%|██████████| 18.5G/18.5G [06:56<00:00, 44.4Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4534 (955a6c2d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf' to '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 9b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type  f16:  295 tensors\n",
            "[   1/ 464]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   2/ 464]                    token_embd.weight - [ 3584, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1750.00 MiB ->   717.77 MiB\n",
            "[   3/ 464]                  blk.0.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[   4/ 464]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   5/ 464]             blk.0.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[   6/ 464]                  blk.0.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[   7/ 464]                  blk.0.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[   8/ 464]                blk.0.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[   9/ 464]                blk.0.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  10/ 464]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  11/ 464]                  blk.0.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  12/ 464]     blk.0.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  13/ 464]           blk.0.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  14/ 464]                  blk.1.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  15/ 464]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  16/ 464]             blk.1.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  17/ 464]                  blk.1.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  18/ 464]                  blk.1.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  19/ 464]                blk.1.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  20/ 464]                blk.1.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  21/ 464]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  22/ 464]                  blk.1.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  23/ 464]     blk.1.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  24/ 464]           blk.1.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  25/ 464]                  blk.2.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  26/ 464]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 464]             blk.2.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  28/ 464]                  blk.2.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  29/ 464]                  blk.2.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  30/ 464]                blk.2.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  31/ 464]                blk.2.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  32/ 464]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 464]                  blk.2.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  34/ 464]     blk.2.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  35/ 464]           blk.2.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  36/ 464]                  blk.3.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  37/ 464]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  38/ 464]             blk.3.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  39/ 464]                  blk.3.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  40/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  41/ 464]                blk.3.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  42/ 464]                blk.3.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  43/ 464]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  44/ 464]                  blk.3.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  45/ 464]     blk.3.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  46/ 464]           blk.3.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  47/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  48/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  49/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  50/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  51/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  52/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  53/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  54/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  56/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  58/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  59/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  60/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  61/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  62/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  63/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  64/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  65/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  66/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  67/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  68/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  70/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  71/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  72/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  73/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  74/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  75/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  76/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  77/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  78/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  80/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  81/ 464]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[  85/ 464]                blk.7.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[  86/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  87/ 464]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  88/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  89/ 464]     blk.7.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  90/ 464]           blk.7.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 464]                  blk.8.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  92/ 464]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 464]             blk.8.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  94/ 464]                  blk.8.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[  95/ 464]                  blk.8.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[  96/ 464]                blk.8.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  97/ 464]                blk.8.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[  98/ 464]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 464]                  blk.8.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 100/ 464]     blk.8.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 101/ 464]           blk.8.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 102/ 464]                  blk.9.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 103/ 464]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 104/ 464]             blk.9.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 105/ 464]                  blk.9.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 106/ 464]                  blk.9.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 107/ 464]                blk.9.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 108/ 464]                blk.9.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 109/ 464]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 110/ 464]                  blk.9.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 111/ 464]     blk.9.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 112/ 464]           blk.9.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 113/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 114/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 116/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 117/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 118/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 119/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 120/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 121/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 122/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 124/ 464]                 blk.11.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 125/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 126/ 464]            blk.11.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 127/ 464]                 blk.11.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 128/ 464]                 blk.11.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 129/ 464]               blk.11.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 130/ 464]               blk.11.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 131/ 464]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 132/ 464]                 blk.11.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 133/ 464]    blk.11.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 134/ 464]          blk.11.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 464]                 blk.12.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 136/ 464]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 137/ 464]            blk.12.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 138/ 464]                 blk.12.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 139/ 464]                 blk.12.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 140/ 464]               blk.12.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 141/ 464]               blk.12.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 142/ 464]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 143/ 464]                 blk.12.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 144/ 464]    blk.12.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 145/ 464]          blk.12.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 146/ 464]                 blk.13.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 147/ 464]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 148/ 464]            blk.13.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 149/ 464]                 blk.13.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 150/ 464]                 blk.13.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 151/ 464]               blk.13.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 152/ 464]               blk.13.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 153/ 464]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 154/ 464]                 blk.13.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 155/ 464]    blk.13.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 156/ 464]          blk.13.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 157/ 464]                 blk.14.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 158/ 464]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 464]            blk.14.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 160/ 464]                 blk.14.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 161/ 464]                 blk.14.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 162/ 464]               blk.14.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 163/ 464]               blk.14.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 164/ 464]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 464]                 blk.14.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 166/ 464]    blk.14.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 167/ 464]          blk.14.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 168/ 464]                 blk.15.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 169/ 464]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 170/ 464]            blk.15.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 171/ 464]                 blk.15.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 172/ 464]                 blk.15.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 173/ 464]               blk.15.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 174/ 464]               blk.15.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 175/ 464]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 176/ 464]                 blk.15.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 177/ 464]    blk.15.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 178/ 464]          blk.15.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 179/ 464]                 blk.16.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 180/ 464]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 181/ 464]            blk.16.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 182/ 464]                 blk.16.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 183/ 464]                 blk.16.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 184/ 464]               blk.16.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 185/ 464]               blk.16.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 186/ 464]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 464]                 blk.16.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 188/ 464]    blk.16.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 464]          blk.16.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 190/ 464]                 blk.17.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 191/ 464]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 192/ 464]            blk.17.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 193/ 464]                 blk.17.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 194/ 464]                 blk.17.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 195/ 464]               blk.17.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 196/ 464]               blk.17.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 197/ 464]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 198/ 464]                 blk.17.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 199/ 464]    blk.17.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 200/ 464]          blk.17.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 464]                 blk.18.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 202/ 464]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 203/ 464]            blk.18.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 204/ 464]                 blk.18.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 205/ 464]                 blk.18.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 206/ 464]               blk.18.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 207/ 464]               blk.18.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 208/ 464]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 209/ 464]                 blk.18.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 210/ 464]    blk.18.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 464]          blk.18.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 212/ 464]                 blk.19.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 213/ 464]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 214/ 464]            blk.19.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 215/ 464]                 blk.19.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 216/ 464]                 blk.19.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 217/ 464]               blk.19.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 218/ 464]               blk.19.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 219/ 464]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 220/ 464]                 blk.19.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 221/ 464]    blk.19.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 222/ 464]          blk.19.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 464]                 blk.20.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 224/ 464]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 464]            blk.20.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 226/ 464]                 blk.20.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 227/ 464]                 blk.20.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 228/ 464]               blk.20.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 229/ 464]               blk.20.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 230/ 464]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 464]                 blk.20.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 232/ 464]    blk.20.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 233/ 464]          blk.20.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 234/ 464]                 blk.21.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 235/ 464]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 236/ 464]            blk.21.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 237/ 464]                 blk.21.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 238/ 464]                 blk.21.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 239/ 464]               blk.21.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 240/ 464]               blk.21.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 241/ 464]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 242/ 464]                 blk.21.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 243/ 464]    blk.21.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 244/ 464]          blk.21.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 245/ 464]                 blk.22.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 246/ 464]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 464]            blk.22.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 248/ 464]                 blk.22.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 249/ 464]                 blk.22.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 250/ 464]               blk.22.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 251/ 464]               blk.22.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 252/ 464]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 253/ 464]                 blk.22.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 254/ 464]    blk.22.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 464]          blk.22.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 256/ 464]                 blk.23.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 257/ 464]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 258/ 464]            blk.23.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 259/ 464]                 blk.23.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 260/ 464]                 blk.23.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 261/ 464]               blk.23.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 262/ 464]               blk.23.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 263/ 464]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 264/ 464]                 blk.23.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 265/ 464]    blk.23.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 266/ 464]          blk.23.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 464]                 blk.24.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 268/ 464]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 269/ 464]            blk.24.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 270/ 464]                 blk.24.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 271/ 464]                 blk.24.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 272/ 464]               blk.24.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 273/ 464]               blk.24.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 274/ 464]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 275/ 464]                 blk.24.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 276/ 464]    blk.24.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 277/ 464]          blk.24.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 278/ 464]                 blk.25.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 279/ 464]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 280/ 464]            blk.25.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 281/ 464]                 blk.25.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 282/ 464]                 blk.25.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 283/ 464]               blk.25.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 284/ 464]               blk.25.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 285/ 464]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 286/ 464]                 blk.25.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 287/ 464]    blk.25.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 288/ 464]          blk.25.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 289/ 464]                 blk.26.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 290/ 464]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 464]            blk.26.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 292/ 464]                 blk.26.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 293/ 464]                 blk.26.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 294/ 464]               blk.26.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 295/ 464]               blk.26.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 296/ 464]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 464]                 blk.26.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 298/ 464]    blk.26.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 299/ 464]          blk.26.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 300/ 464]                 blk.27.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 301/ 464]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 302/ 464]            blk.27.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 303/ 464]                 blk.27.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 304/ 464]                 blk.27.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 305/ 464]               blk.27.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 306/ 464]               blk.27.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 307/ 464]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 308/ 464]                 blk.27.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 309/ 464]    blk.27.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 310/ 464]          blk.27.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 311/ 464]                 blk.28.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 312/ 464]              blk.28.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 313/ 464]            blk.28.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 314/ 464]                 blk.28.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 315/ 464]                 blk.28.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 316/ 464]               blk.28.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 317/ 464]               blk.28.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 318/ 464]               blk.28.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 464]                 blk.28.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 320/ 464]    blk.28.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 464]          blk.28.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 322/ 464]                 blk.29.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 323/ 464]              blk.29.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 324/ 464]            blk.29.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 325/ 464]                 blk.29.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 326/ 464]                 blk.29.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 327/ 464]               blk.29.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 328/ 464]               blk.29.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 329/ 464]               blk.29.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 330/ 464]                 blk.29.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 331/ 464]    blk.29.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 332/ 464]          blk.29.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 464]                 blk.30.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 334/ 464]              blk.30.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 335/ 464]            blk.30.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 336/ 464]                 blk.30.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 337/ 464]                 blk.30.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 338/ 464]               blk.30.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 339/ 464]               blk.30.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 340/ 464]               blk.30.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 341/ 464]                 blk.30.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 342/ 464]    blk.30.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 343/ 464]          blk.30.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 344/ 464]                 blk.31.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 345/ 464]              blk.31.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 346/ 464]            blk.31.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 347/ 464]                 blk.31.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 348/ 464]                 blk.31.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 349/ 464]               blk.31.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 350/ 464]               blk.31.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 351/ 464]               blk.31.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 352/ 464]                 blk.31.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 353/ 464]    blk.31.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 354/ 464]          blk.31.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 355/ 464]                 blk.32.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 356/ 464]              blk.32.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 357/ 464]            blk.32.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 358/ 464]                 blk.32.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 359/ 464]                 blk.32.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 360/ 464]               blk.32.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 361/ 464]               blk.32.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 362/ 464]               blk.32.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 363/ 464]                 blk.32.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 364/ 464]    blk.32.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 365/ 464]          blk.32.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 366/ 464]                 blk.33.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 367/ 464]              blk.33.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 368/ 464]            blk.33.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 369/ 464]                 blk.33.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 370/ 464]                 blk.33.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 371/ 464]               blk.33.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 372/ 464]               blk.33.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 373/ 464]               blk.33.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 374/ 464]                 blk.33.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 375/ 464]    blk.33.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 376/ 464]          blk.33.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 377/ 464]                 blk.34.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 378/ 464]              blk.34.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 379/ 464]            blk.34.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 380/ 464]                 blk.34.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 381/ 464]                 blk.34.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 382/ 464]               blk.34.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 383/ 464]               blk.34.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 384/ 464]               blk.34.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 385/ 464]                 blk.34.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 386/ 464]    blk.34.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 387/ 464]          blk.34.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 388/ 464]                 blk.35.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 389/ 464]              blk.35.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 390/ 464]            blk.35.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 391/ 464]                 blk.35.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 392/ 464]                 blk.35.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 393/ 464]               blk.35.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 394/ 464]               blk.35.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 395/ 464]               blk.35.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 396/ 464]                 blk.35.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 397/ 464]    blk.35.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 398/ 464]          blk.35.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 399/ 464]                 blk.36.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 400/ 464]              blk.36.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 401/ 464]            blk.36.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 402/ 464]                 blk.36.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 403/ 464]                 blk.36.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 404/ 464]               blk.36.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 405/ 464]               blk.36.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 406/ 464]               blk.36.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 407/ 464]                 blk.36.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 408/ 464]    blk.36.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 409/ 464]          blk.36.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 410/ 464]                 blk.37.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 411/ 464]              blk.37.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 412/ 464]            blk.37.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 413/ 464]                 blk.37.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 414/ 464]                 blk.37.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 415/ 464]               blk.37.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 416/ 464]               blk.37.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 417/ 464]               blk.37.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 418/ 464]                 blk.37.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 419/ 464]    blk.37.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 420/ 464]          blk.37.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 421/ 464]                 blk.38.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 422/ 464]              blk.38.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 423/ 464]            blk.38.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 424/ 464]                 blk.38.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 425/ 464]                 blk.38.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 426/ 464]               blk.38.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 427/ 464]               blk.38.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 428/ 464]               blk.38.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 429/ 464]                 blk.38.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 430/ 464]    blk.38.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 431/ 464]          blk.38.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 432/ 464]                 blk.39.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 433/ 464]              blk.39.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 434/ 464]            blk.39.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 435/ 464]                 blk.39.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 436/ 464]                 blk.39.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 437/ 464]               blk.39.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 438/ 464]               blk.39.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 439/ 464]               blk.39.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 440/ 464]                 blk.39.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 441/ 464]    blk.39.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 442/ 464]          blk.39.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 443/ 464]                 blk.40.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 444/ 464]              blk.40.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 445/ 464]            blk.40.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 446/ 464]                 blk.40.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 447/ 464]                 blk.40.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 448/ 464]               blk.40.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 449/ 464]               blk.40.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 450/ 464]               blk.40.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 451/ 464]                 blk.40.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 452/ 464]    blk.40.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 453/ 464]          blk.40.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 454/ 464]                 blk.41.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q4_K .. size =    14.00 MiB ->     3.94 MiB\n",
            "[ 455/ 464]              blk.41.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 456/ 464]            blk.41.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 457/ 464]                 blk.41.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q4_K .. size =    28.00 MiB ->     7.88 MiB\n",
            "[ 458/ 464]                 blk.41.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\n",
            "[ 459/ 464]               blk.41.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\n",
            "[ 460/ 464]               blk.41.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 461/ 464]               blk.41.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 462/ 464]                 blk.41.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q4_K .. size =    98.00 MiB ->    27.56 MiB\n",
            "[ 463/ 464]    blk.41.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 464/ 464]          blk.41.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "llama_model_quantize_impl: model size  = 17628.31 MB\n",
            "llama_model_quantize_impl: quant size  =  5488.40 MB\n",
            "\n",
            "main: quantize time = 1193636.15 ms\n",
            "main:    total time = 1193636.15 ms\n",
            "Unsloth: Conversion completed! Output location: /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
            "main: build = 4534 (955a6c2d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf' to '/content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.Q8_0.gguf' as Q8_0 using 4 threads\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from /content/SURESHBEEKHANI/Finetune_Gemma_NRE_SFT_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 9b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 9B\n",
            "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 42\n",
            "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  169 tensors\n",
            "llama_model_loader: - type  f16:  295 tensors\n",
            "[   1/ 464]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   2/ 464]                    token_embd.weight - [ 3584, 256000,     1,     1], type =    f16, converting to q8_0 .. size =  1750.00 MiB ->   929.69 MiB\n",
            "[   3/ 464]                  blk.0.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[   4/ 464]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   5/ 464]             blk.0.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[   6/ 464]                  blk.0.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[   7/ 464]                  blk.0.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[   8/ 464]                blk.0.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[   9/ 464]                blk.0.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  10/ 464]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  11/ 464]                  blk.0.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  12/ 464]     blk.0.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  13/ 464]           blk.0.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  14/ 464]                  blk.1.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  15/ 464]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  16/ 464]             blk.1.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  17/ 464]                  blk.1.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  18/ 464]                  blk.1.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  19/ 464]                blk.1.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  20/ 464]                blk.1.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  21/ 464]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  22/ 464]                  blk.1.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  23/ 464]     blk.1.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  24/ 464]           blk.1.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  25/ 464]                  blk.2.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  26/ 464]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 464]             blk.2.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  28/ 464]                  blk.2.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  29/ 464]                  blk.2.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  30/ 464]                blk.2.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  31/ 464]                blk.2.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  32/ 464]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 464]                  blk.2.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  34/ 464]     blk.2.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  35/ 464]           blk.2.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  36/ 464]                  blk.3.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  37/ 464]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  38/ 464]             blk.3.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  39/ 464]                  blk.3.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  40/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  41/ 464]                blk.3.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  42/ 464]                blk.3.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  43/ 464]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  44/ 464]                  blk.3.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  45/ 464]     blk.3.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  46/ 464]           blk.3.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  47/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  48/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  49/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  50/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  51/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  52/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  53/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  54/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  56/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  58/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  59/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  60/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  61/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  62/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  63/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  64/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  65/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  66/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  67/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  68/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  70/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  71/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  72/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  73/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  74/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  75/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  76/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  77/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  78/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  80/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  81/ 464]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  85/ 464]                blk.7.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  86/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  87/ 464]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  88/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  89/ 464]     blk.7.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  90/ 464]           blk.7.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 464]                  blk.8.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  92/ 464]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 464]             blk.8.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  94/ 464]                  blk.8.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[  95/ 464]                  blk.8.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[  96/ 464]                blk.8.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  97/ 464]                blk.8.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[  98/ 464]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 464]                  blk.8.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 100/ 464]     blk.8.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 101/ 464]           blk.8.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 102/ 464]                  blk.9.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 103/ 464]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 104/ 464]             blk.9.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 105/ 464]                  blk.9.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 106/ 464]                  blk.9.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 107/ 464]                blk.9.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 108/ 464]                blk.9.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 109/ 464]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 110/ 464]                  blk.9.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 111/ 464]     blk.9.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 112/ 464]           blk.9.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 113/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 114/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 116/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 117/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 118/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 119/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 120/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 121/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 122/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 124/ 464]                 blk.11.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 125/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 126/ 464]            blk.11.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 127/ 464]                 blk.11.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 128/ 464]                 blk.11.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 129/ 464]               blk.11.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 130/ 464]               blk.11.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 131/ 464]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 132/ 464]                 blk.11.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 133/ 464]    blk.11.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 134/ 464]          blk.11.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 464]                 blk.12.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 136/ 464]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 137/ 464]            blk.12.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 138/ 464]                 blk.12.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 139/ 464]                 blk.12.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 140/ 464]               blk.12.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 141/ 464]               blk.12.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 142/ 464]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 143/ 464]                 blk.12.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 144/ 464]    blk.12.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 145/ 464]          blk.12.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 146/ 464]                 blk.13.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 147/ 464]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 148/ 464]            blk.13.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 149/ 464]                 blk.13.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 150/ 464]                 blk.13.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 151/ 464]               blk.13.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 152/ 464]               blk.13.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 153/ 464]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 154/ 464]                 blk.13.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 155/ 464]    blk.13.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 156/ 464]          blk.13.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 157/ 464]                 blk.14.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 158/ 464]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 464]            blk.14.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 160/ 464]                 blk.14.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 161/ 464]                 blk.14.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 162/ 464]               blk.14.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 163/ 464]               blk.14.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 164/ 464]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 464]                 blk.14.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 166/ 464]    blk.14.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 167/ 464]          blk.14.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 168/ 464]                 blk.15.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 169/ 464]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 170/ 464]            blk.15.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 171/ 464]                 blk.15.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 172/ 464]                 blk.15.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 173/ 464]               blk.15.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 174/ 464]               blk.15.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 175/ 464]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 176/ 464]                 blk.15.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 177/ 464]    blk.15.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 178/ 464]          blk.15.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 179/ 464]                 blk.16.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 180/ 464]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 181/ 464]            blk.16.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 182/ 464]                 blk.16.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 183/ 464]                 blk.16.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 184/ 464]               blk.16.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 185/ 464]               blk.16.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 186/ 464]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 464]                 blk.16.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 188/ 464]    blk.16.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 464]          blk.16.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 190/ 464]                 blk.17.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 191/ 464]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 192/ 464]            blk.17.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 193/ 464]                 blk.17.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 194/ 464]                 blk.17.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 195/ 464]               blk.17.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 196/ 464]               blk.17.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 197/ 464]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 198/ 464]                 blk.17.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 199/ 464]    blk.17.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 200/ 464]          blk.17.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 464]                 blk.18.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 202/ 464]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 203/ 464]            blk.18.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 204/ 464]                 blk.18.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 205/ 464]                 blk.18.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 206/ 464]               blk.18.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 207/ 464]               blk.18.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 208/ 464]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 209/ 464]                 blk.18.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 210/ 464]    blk.18.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 464]          blk.18.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 212/ 464]                 blk.19.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 213/ 464]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 214/ 464]            blk.19.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 215/ 464]                 blk.19.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 216/ 464]                 blk.19.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 217/ 464]               blk.19.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 218/ 464]               blk.19.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 219/ 464]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 220/ 464]                 blk.19.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 221/ 464]    blk.19.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 222/ 464]          blk.19.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 464]                 blk.20.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 224/ 464]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 464]            blk.20.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 226/ 464]                 blk.20.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 227/ 464]                 blk.20.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 228/ 464]               blk.20.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 229/ 464]               blk.20.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 230/ 464]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 464]                 blk.20.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 232/ 464]    blk.20.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 233/ 464]          blk.20.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 234/ 464]                 blk.21.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 235/ 464]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 236/ 464]            blk.21.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 237/ 464]                 blk.21.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 238/ 464]                 blk.21.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 239/ 464]               blk.21.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 240/ 464]               blk.21.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 241/ 464]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 242/ 464]                 blk.21.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 243/ 464]    blk.21.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 244/ 464]          blk.21.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 245/ 464]                 blk.22.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 246/ 464]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 464]            blk.22.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 248/ 464]                 blk.22.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 249/ 464]                 blk.22.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 250/ 464]               blk.22.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 251/ 464]               blk.22.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 252/ 464]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 253/ 464]                 blk.22.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 254/ 464]    blk.22.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 464]          blk.22.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 256/ 464]                 blk.23.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 257/ 464]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 258/ 464]            blk.23.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 259/ 464]                 blk.23.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 260/ 464]                 blk.23.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 261/ 464]               blk.23.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 262/ 464]               blk.23.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 263/ 464]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 264/ 464]                 blk.23.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 265/ 464]    blk.23.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 266/ 464]          blk.23.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 464]                 blk.24.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 268/ 464]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 269/ 464]            blk.24.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 270/ 464]                 blk.24.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 271/ 464]                 blk.24.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 272/ 464]               blk.24.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 273/ 464]               blk.24.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 274/ 464]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 275/ 464]                 blk.24.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 276/ 464]    blk.24.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 277/ 464]          blk.24.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 278/ 464]                 blk.25.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 279/ 464]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 280/ 464]            blk.25.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 281/ 464]                 blk.25.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 282/ 464]                 blk.25.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 283/ 464]               blk.25.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 284/ 464]               blk.25.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 285/ 464]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 286/ 464]                 blk.25.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 287/ 464]    blk.25.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 288/ 464]          blk.25.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 289/ 464]                 blk.26.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 290/ 464]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 464]            blk.26.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 292/ 464]                 blk.26.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 293/ 464]                 blk.26.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 294/ 464]               blk.26.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 295/ 464]               blk.26.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 296/ 464]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 464]                 blk.26.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 298/ 464]    blk.26.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 299/ 464]          blk.26.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 300/ 464]                 blk.27.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 301/ 464]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 302/ 464]            blk.27.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 303/ 464]                 blk.27.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 304/ 464]                 blk.27.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 305/ 464]               blk.27.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 306/ 464]               blk.27.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 307/ 464]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 308/ 464]                 blk.27.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 309/ 464]    blk.27.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 310/ 464]          blk.27.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 311/ 464]                 blk.28.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 312/ 464]              blk.28.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 313/ 464]            blk.28.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 314/ 464]                 blk.28.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 315/ 464]                 blk.28.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 316/ 464]               blk.28.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 317/ 464]               blk.28.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 318/ 464]               blk.28.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 464]                 blk.28.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 320/ 464]    blk.28.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 464]          blk.28.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 322/ 464]                 blk.29.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 323/ 464]              blk.29.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 324/ 464]            blk.29.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 325/ 464]                 blk.29.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 326/ 464]                 blk.29.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 327/ 464]               blk.29.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 328/ 464]               blk.29.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 329/ 464]               blk.29.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 330/ 464]                 blk.29.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 331/ 464]    blk.29.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 332/ 464]          blk.29.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 464]                 blk.30.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 334/ 464]              blk.30.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 335/ 464]            blk.30.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 336/ 464]                 blk.30.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 337/ 464]                 blk.30.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 338/ 464]               blk.30.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 339/ 464]               blk.30.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 340/ 464]               blk.30.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 341/ 464]                 blk.30.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 342/ 464]    blk.30.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 343/ 464]          blk.30.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 344/ 464]                 blk.31.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 345/ 464]              blk.31.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 346/ 464]            blk.31.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 347/ 464]                 blk.31.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 348/ 464]                 blk.31.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 349/ 464]               blk.31.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 350/ 464]               blk.31.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 351/ 464]               blk.31.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 352/ 464]                 blk.31.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 353/ 464]    blk.31.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 354/ 464]          blk.31.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 355/ 464]                 blk.32.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 356/ 464]              blk.32.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 357/ 464]            blk.32.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 358/ 464]                 blk.32.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 359/ 464]                 blk.32.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 360/ 464]               blk.32.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 361/ 464]               blk.32.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 362/ 464]               blk.32.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 363/ 464]                 blk.32.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 364/ 464]    blk.32.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 365/ 464]          blk.32.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 366/ 464]                 blk.33.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 367/ 464]              blk.33.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 368/ 464]            blk.33.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 369/ 464]                 blk.33.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 370/ 464]                 blk.33.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 371/ 464]               blk.33.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 372/ 464]               blk.33.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 373/ 464]               blk.33.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 374/ 464]                 blk.33.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 375/ 464]    blk.33.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 376/ 464]          blk.33.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 377/ 464]                 blk.34.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 378/ 464]              blk.34.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 379/ 464]            blk.34.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 380/ 464]                 blk.34.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 381/ 464]                 blk.34.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 382/ 464]               blk.34.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 383/ 464]               blk.34.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 384/ 464]               blk.34.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 385/ 464]                 blk.34.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 386/ 464]    blk.34.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 387/ 464]          blk.34.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 388/ 464]                 blk.35.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 389/ 464]              blk.35.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 390/ 464]            blk.35.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 391/ 464]                 blk.35.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 392/ 464]                 blk.35.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 393/ 464]               blk.35.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 394/ 464]               blk.35.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 395/ 464]               blk.35.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 396/ 464]                 blk.35.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 397/ 464]    blk.35.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 398/ 464]          blk.35.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 399/ 464]                 blk.36.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 400/ 464]              blk.36.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 401/ 464]            blk.36.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 402/ 464]                 blk.36.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 403/ 464]                 blk.36.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 404/ 464]               blk.36.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 405/ 464]               blk.36.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 406/ 464]               blk.36.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 407/ 464]                 blk.36.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 408/ 464]    blk.36.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 409/ 464]          blk.36.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 410/ 464]                 blk.37.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 411/ 464]              blk.37.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 412/ 464]            blk.37.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 413/ 464]                 blk.37.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 414/ 464]                 blk.37.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 415/ 464]               blk.37.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 416/ 464]               blk.37.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 417/ 464]               blk.37.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 418/ 464]                 blk.37.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 419/ 464]    blk.37.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 420/ 464]          blk.37.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 421/ 464]                 blk.38.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 422/ 464]              blk.38.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 423/ 464]            blk.38.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 424/ 464]                 blk.38.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 425/ 464]                 blk.38.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 426/ 464]               blk.38.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 427/ 464]               blk.38.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 428/ 464]               blk.38.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 429/ 464]                 blk.38.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 430/ 464]    blk.38.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 431/ 464]          blk.38.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 432/ 464]                 blk.39.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 433/ 464]              blk.39.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 434/ 464]            blk.39.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 435/ 464]                 blk.39.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 436/ 464]                 blk.39.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 437/ 464]               blk.39.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 438/ 464]               blk.39.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 439/ 464]               blk.39.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 440/ 464]                 blk.39.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 441/ 464]    blk.39.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 442/ 464]          blk.39.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 443/ 464]                 blk.40.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 444/ 464]              blk.40.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 445/ 464]            blk.40.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 446/ 464]                 blk.40.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 447/ 464]                 blk.40.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 448/ 464]               blk.40.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 449/ 464]               blk.40.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 450/ 464]               blk.40.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 451/ 464]                 blk.40.ffn_up.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 452/ 464]    blk.40.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 453/ 464]          blk.40.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 454/ 464]                 blk.41.attn_k.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 455/ 464]              blk.41.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 456/ 464]            blk.41.attn_output.weight - [ 4096,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 457/ 464]                 blk.41.attn_q.weight - [ 3584,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    28.00 MiB ->    14.88 MiB\n",
            "[ 458/ 464]                 blk.41.attn_v.weight - [ 3584,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    14.00 MiB ->     7.44 MiB\n",
            "[ 459/ 464]               blk.41.ffn_down.weight - [14336,  3584,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n",
            "[ 460/ 464]               blk.41.ffn_gate.weight - [ 3584, 14336,     1,     1], type =    f16, converting to q8_0 .. size =    98.00 MiB ->    52.06 MiB\n"
          ]
        }
      ]
    }
  ]
}