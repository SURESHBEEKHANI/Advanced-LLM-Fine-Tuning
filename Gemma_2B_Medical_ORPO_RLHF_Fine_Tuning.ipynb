{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Advanced-LLM-Fine-Tuning/blob/main/Gemma_2B_Medical_ORPO_RLHF_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and Upgrade Unsloth Library to Latest Nightly Version"
      ],
      "metadata": {
        "id": "X8HeU2eB-uQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install the `unsloth` library using pip\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall the current version of `unsloth` and install the latest nightly version\n",
        "# directly from the GitHub repository. This ensures you have the most up-to-date code.\n",
        "# Flags used:\n",
        "# - `--upgrade`: Ensures the latest version is installed.\n",
        "# - `--no-cache-dir`: Disables caching for a fresh installation.\n",
        "# - `--no-deps`: Skips installing dependencies (useful if dependencies are already installed).\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "8lSNa9FYvmA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Configure a Pretrained FastLanguageModel with Custom Settings\n"
      ],
      "metadata": {
        "id": "GCuyE0O6-82g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2351740d-24de-47a7-c748-58d5f66d64d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.1.7: Fast Gemma patching. Transformers: 4.48.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Import the FastLanguageModel module from the unsloth library\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Define the maximum sequence length for the model (can be customized)\n",
        "max_seq_length = 4096  # Choose any! RoPE Scaling is auto-supported internally.\n",
        "\n",
        "# Specify the data type for the model (None for auto-detection, or specify Float16/Bfloat16)\n",
        "dtype = None  # Float16 for Tesla T4, V100; Bfloat16 for Ampere+; None for auto-detection.\n",
        "\n",
        "# Enable or disable 4-bit quantization to reduce memory usage\n",
        "load_in_4bit = True  # Use 4-bit quantization. Set to False if not required.\n",
        "\n",
        "# Load the pretrained model and tokenizer with the specified parameters\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-2b-it-bnb-4bit\",  # Model name to be loaded\n",
        "    max_seq_length=max_seq_length,             # Maximum sequence length\n",
        "    dtype=dtype,                               # Data type\n",
        "    load_in_4bit=load_in_4bit                  # Enable 4-bit quantization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "_H6EWbe-t_bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "16b3d3b0-4fcb-4e44-f623-ff3d65067bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.7 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Formatting Medical Reasoning Dataset for ORPOTrainer\n"
      ],
      "metadata": {
        "id": "pkCQUItbC-lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GetrTh37qgDp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "659af940dbe44ba78214828fc4bb6697",
            "9a64dc73e9244f13b195573be3f63d4b",
            "627e360f252549d28ff40e3b83215678",
            "06b79a921f46416cb778509c69998750",
            "e1cd2963bde34da18dd3c5213b58f090",
            "4539935793084157b98a0c4a28bfc19f",
            "38b3f3714f994aa884e9b008e9b2c163",
            "ed09542d01604e14ac7b5bdb6247ffec",
            "d46cc82e81e84f98b0973d019f82f130",
            "f0e357099e1442b396f280c48e6566c7",
            "2dbfaaf5018a49a8a90e46aa3df4ebd9"
          ]
        },
        "outputId": "98b3645d-f385-41e5-a055-728b59f4a90b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659af940dbe44ba78214828fc4bb6697"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Formatting Medical Reasoning Dataset for ORPOTrainer\n",
        "# This script formats the dataset by creating prompts based on the given instruction, input, and responses.\n",
        "# The format follows the Alpaca prompt style, including \"instruction\", \"input\", and \"response\" sections.\n",
        "\n",
        "# Define the Alpaca-style prompt template for formatting the data\n",
        "alpaca_prompt = \"\"\"Below is a medical scenario with an input that describes a situation or a question related to healthcare. Write a response that appropriately completes the medical reasoning request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Define the End of Sequence (EOS) token\n",
        "EOS_TOKEN = \"<EOS>\"  # Placeholder for EOS token, ensure this matches your tokenizer's EOS token\n",
        "\n",
        "def format_prompt(sample):\n",
        "    # Extract the instruction, input, accepted response, and rejected response from the sample\n",
        "    instruction = sample[\"instruction\"]  # Instruction on how to approach the medical problem\n",
        "    input_data = sample[\"Input\"]\n",
        "    accepted = sample[\"accepted\"]  # The accepted (valid) reasoning response\n",
        "    rejected = sample[\"rejected\"]  # The rejected (invalid) reasoning response\n",
        "\n",
        "    # ORPOTrainer expects keys: prompt (formatted instruction and input), chosen (accepted response), and rejected (rejected response)\n",
        "    # Create a formatted prompt using the Alpaca template, leaving the response section empty\n",
        "    sample[\"Input\"] = alpaca_prompt.format(instruction, input_data, \"\")\n",
        "\n",
        "    # Add the accepted response, appending the EOS token at the end\n",
        "    sample[\"chosen\"] = accepted + EOS_TOKEN\n",
        "\n",
        "    # Add the rejected response, appending the EOS token at the end\n",
        "    sample[\"rejected\"] = rejected + EOS_TOKEN\n",
        "\n",
        "    return sample  # Return the formatted sample for further use\n",
        "\n",
        "# Placeholder statement, does nothing, but ensures syntactical correctness\n",
        "pass\n",
        "\n",
        "# Example of loading and processing the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"SURESHBEEKHANI/medical-reasoning-orpo\"\n",
        "dataset = load_dataset(dataset_name, split=\"all\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(4200))  # Limit to 1000 samples for a quick demo\n",
        "\n",
        "# Apply the `format_prompt` function to each sample in the dataset to format them correctly\n",
        "dataset = dataset.map(format_prompt)  # The `map` function applies `format_prompt` across all samples in the dataset\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "dataset = dataset.train_test_split(test_size=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out some examples to see how the dataset should look like"
      ],
      "metadata": {
        "id": "7PH-7Cudu8Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "U1738BWgZ2ij",
        "outputId": "04b169d2-838b-4af4-f7ca-252e2c003498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Input', 'accepted', 'rejected', 'instruction', 'chosen'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Input', 'accepted', 'rejected', 'instruction', 'chosen'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF63zQqNlNJC",
        "outputId": "6224b577-3f3d-4a7e-e61d-b5e6efb91001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INSTRUCTION: ==================================================\n",
            "('Below is a medical scenario with an input that describes a situation or a '\n",
            " 'question related to healthcare. Write a response that appropriately '\n",
            " 'completes the medical reasoning request.\\n'\n",
            " '\\n'\n",
            " '### Instruction:\\n'\n",
            " 'Given the following medical question or situation, provide the most suitable '\n",
            " 'reasoning or explanation.\\n'\n",
            " '\\n'\n",
            " '### Input:\\n'\n",
            " 'In a 26-year-old G2P1 patient with gestational diabetes treated with insulin '\n",
            " 'undergoing labor induction at 40 weeks gestation, with the following blood '\n",
            " 'work results: fasting glucose 92 mg/dL and HbA1c 7.8%, what should be '\n",
            " 'administered during labor to manage her glucose levels appropriately?\\n'\n",
            " '\\n'\n",
            " '### Response:\\n')\n",
            "ACCEPTED: ==================================================\n",
            "(\"Okay, so we have a 26-year-old woman who's currently pregnant with her \"\n",
            " \"second child, and she's 40 weeks along. She's been diagnosed with \"\n",
            " \"gestational diabetes and has been managing it with insulin. Now, we're \"\n",
            " \"getting ready to induce labor. Right, let's look at her current blood \"\n",
            " 'glucose situation. Her fasting glucose level is 92 mg/dL, which actually '\n",
            " \"isn't too bad. It's within a controlled range. But hang on, her HbA1c is \"\n",
            " '7.8%, which shows that her overall blood sugar control during the pregnancy '\n",
            " \"wasn't ideal. Hmm, that’s something we need to keep in mind.\\n\"\n",
            " '\\n'\n",
            " \"During labor, the body's energy requirements can really fluctuate, and \"\n",
            " 'managing blood glucose can become tricky. Usually, when you have gestational '\n",
            " \"diabetes and you’re in labor, it's important to monitor blood glucose \"\n",
            " 'frequently. This is mainly because we want to prevent both hypo- and '\n",
            " 'hyperglycemia, which can have side effects for both mom and baby.\\n'\n",
            " '\\n'\n",
            " 'Hmm, for insulin-treated patients, things can require a bit of adjustment. '\n",
            " 'In labor, many practitioners would either employ an insulin drip or a '\n",
            " 'modified subcutaneous regimen to help maintain glucose levels within a '\n",
            " 'specific range. Typically, that range is between 70 to 110 mg/dL. We '\n",
            " 'definitely want to keep the glucose neither too high nor too low.\\n'\n",
            " '\\n'\n",
            " \"Now, let's think about fluids. During labor, they often use IV fluids that \"\n",
            " 'contain glucose, like D5 1/2 normal saline. This is crucial because it '\n",
            " 'provides a steady supply of glucose to prevent hypoglycemia, especially when '\n",
            " \"we're administering insulin as well.\\n\"\n",
            " '\\n'\n",
            " 'Oh, but wait. Actually, now that I think about it, we should consider using '\n",
            " 'normal saline first to avoid swinging the glucose levels too high initially. '\n",
            " 'Maybe later, if her glucose levels drop too low, then we could consider '\n",
            " 'adding the glucose to the fluids.\\n'\n",
            " '\\n'\n",
            " 'Alright, since the goal is clear—we want to maintain her blood glucose '\n",
            " 'levels between that 70 to 110 mg/dL band during labor—I think an insulin '\n",
            " 'drip combined with normal saline makes for a good starting approach. This '\n",
            " 'allows us to follow her blood glucose closely and adjust as needed.\\n'\n",
            " '\\n'\n",
            " \"So, to be sure, when she's in labor, she'll need a consistent way to control \"\n",
            " 'her glucose, a constant check on her levels, and we should be prepared to '\n",
            " 'adjust her insulin drip as required. It sounds about right to give her '\n",
            " 'normal saline first and then carefully introduce glucose if necessary.\\n'\n",
            " '\\n'\n",
            " \"Oh yes, and it's common practice to use a continuous IV insulin infusion \"\n",
            " 'because it’s pretty efficient in keeping glucose levels stable. Combine this '\n",
            " 'with the ability to quickly adjust based on her frequent blood glucose '\n",
            " \"readings, and we've got a solid strategy here.\\n\"\n",
            " '\\n'\n",
            " 'In conclusion, it sounds like the best plan to manage her glucose during '\n",
            " 'labor is starting with an IV insulin drip and normal saline. We can make '\n",
            " 'changes to the fluids by adding glucose if her blood sugar levels indicate '\n",
            " 'it’s safe and needed. That should help maintain her glucose levels safely '\n",
            " 'during labor. Great! Finally, this feels like a balanced approach that '\n",
            " 'aligns with the general management guidelines for women with gestational '\n",
            " \"diabetes in labor. To manage this patient's glucose levels during labor, it \"\n",
            " 'is appropriate to use a continuous intravenous (IV) insulin infusion along '\n",
            " 'with normal saline initially. This approach will help maintain her blood '\n",
            " 'glucose levels within the targeted range of 70 to 110 mg/dL. Frequent blood '\n",
            " 'glucose monitoring should be conducted to allow adjustments to the insulin '\n",
            " 'infusion as needed. If blood glucose levels drop too low, glucose-containing '\n",
            " 'IV fluids, such as D5 1/2 normal saline, can be introduced to prevent '\n",
            " 'hypoglycemia. This strategy ensures careful control of glucose levels during '\n",
            " 'labor, aligning with best practices for managing gestational diabetes in '\n",
            " 'such scenarios.<EOS>')\n",
            "REJECTED: ==================================================\n",
            "('Based on the provided information, it appears that this patient is '\n",
            " 'experiencing gestational diabetes. To manage her glucose levels '\n",
            " 'appropriately during labor and delivery:\\n'\n",
            " '\\n'\n",
            " '1) Fasting blood sugar should be closely monitored to ensure she has not '\n",
            " 'developed hyperglycemia or hypoglycemia due a previous episode of GDM in '\n",
            " 'pregnancy (which may require additional interventions).\\n'\n",
            " '2a-b): Administer 30 mg/dL intravenous insulin infusion at an initial rate, '\n",
            " 'then adjust as needed based on her response. This will help maintain normal '\n",
            " 'glucose levels during labor and delivery while minimizing the risk for '\n",
            " 'hyperglycemia or hypoglycemia that could occur if she develops a new episode '\n",
            " 'of GDM in pregnancy (which may require additional interventions).\\n'\n",
            " '3) HbA1c level should be closely monitored to ensure it remains within an '\n",
            " 'acceptable range, typically 5.7% - <6%. This will help prevent long-term '\n",
            " 'complications associated with high blood glucose levels during labor and '\n",
            " 'delivery such as retinopathy or nephropathy in the newborn (which may '\n",
            " 'require additional interventions).\\n'\n",
            " '4) If her HbA1c level is not close to this target value by midlabor, '\n",
            " \"consider administering a higher dose of insulin infusion. However it's \"\n",
            " 'essential that she receives continuous monitoring throughout pregnancy for '\n",
            " 'any signs indicating an impending episode and adjust accordingly based on '\n",
            " 'lab results during labor (which may require additional interventions).\\n'\n",
            " '5) If her glucose levels are still high after delivery or if there is '\n",
            " 'evidence suggesting the need to initiate close surveillance, consider '\n",
            " 'administering a glucagon-like substance such as sodium thiosulfate '\n",
            " 'intravenously at 1 mg/kg dose. This will help rapidly lower blood sugar and '\n",
            " 'prevent hyperglycemia in cases where she has not yet developed an episode of '\n",
            " 'GDM during pregnancy (which may require additional interventions).<EOS>')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "\n",
        "# Print the instruction section\n",
        "print('INSTRUCTION: ' + '=' * 50)\n",
        "pprint.pprint(row[\"Input\"])\n",
        "\n",
        "# Print the accepted response section\n",
        "print('ACCEPTED: ' + '=' * 50)\n",
        "pprint.pprint(row[\"chosen\"])\n",
        "\n",
        "# Print the rejected response section\n",
        "print('REJECTED: ' + '=' * 50)\n",
        "pprint.pprint(row[\"rejected\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqyleKojqgDq"
      },
      "outputs": [],
      "source": [
        "# Enable reward modelling stats\n",
        "# Import the PatchDPOTrainer class from the unsloth module\n",
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "# Instantiate PatchDPOTrainer to enable reward modelling statistics\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `ORPOTrainer`! More docs here: [TRL ORPO docs](https://huggingface.co/docs/trl/main/en/orpo_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "J1ZlnJpkxIuV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "QtoqUw80QDV0",
        "outputId": "8eb4474a-4b68-4a73-8cac-5de9a5792b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 4,000 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 500\n",
            " \"-____-\"     Number of trainable parameters = 19,611,648\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GemmaFixedRotaryEmbedding' object has no attribute 'current_rope_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7fbc9794495d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Start training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/orpo_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_loss_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;31m# Make sure to move the loss to the device the original accumulating loss is at back in the `Trainer` class:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/orpo_trainer.py\u001b[0m in \u001b[0;36mget_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mforward_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenated_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         (\n\u001b[1;32m    816\u001b[0m             \u001b[0mpolicy_chosen_logps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/orpo_trainer.py\u001b[0m in \u001b[0;36mconcatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_router_logits\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0mconcatenated_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"concatenated_input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcatenated_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"concatenated_attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m ):\n\u001b[0;32m-> 1130\u001b[0;31m     return self.base_model(\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_no_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             outputs = self.model(\n\u001b[0m\u001b[1;32m    991\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;31m# unsloth's check for granite too has \"version >= 4.45.0 (rightly so)\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# so let granite always use the attention refactor implementation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, position_ids, seq_len)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# x: [bs, num_attention_heads, seq_len, head_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_rope_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_cos_sin_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GemmaFixedRotaryEmbedding' object has no attribute 'current_rope_size'"
          ]
        }
      ],
      "source": [
        "from trl import ORPOConfig, ORPOTrainer  # Import necessary classes for configuration and training\n",
        "from unsloth import is_bfloat16_supported  # Import function to check if bfloat16 is supported\n",
        "\n",
        "# Initialize the ORPOTrainer with model, datasets, tokenizer, and optimized configuration\n",
        "orpo_trainer = ORPOTrainer(\n",
        "    model=model,  # Predefined model to be trained\n",
        "    train_dataset=dataset[\"train\"],  # Training dataset\n",
        "    eval_dataset=dataset[\"test\"],  # Evaluation dataset\n",
        "    tokenizer=tokenizer,  # Tokenizer associated with the model\n",
        "    args=ORPOConfig(  # Configuration for the trainer\n",
        "        max_length=max_seq_length,  # Maximum sequence length for inputs\n",
        "        max_prompt_length=max_seq_length // 2,  # Maximum length for the prompt (half of max length)\n",
        "        max_completion_length=max_seq_length // 2,  # Maximum length for the completion (other half)\n",
        "        per_device_train_batch_size=8,  # Increase batch size for better gradient stability\n",
        "        gradient_accumulation_steps=8,  # Increase accumulation steps to effectively simulate a larger batch size\n",
        "        beta=0.2,  # Slightly higher beta for regularization (tune this as needed)\n",
        "        logging_steps=10,  # Log less frequently to reduce I/O overhead\n",
        "        optim=\"adamw_8bit\",  # Optimizer with 8-bit precision to save memory\n",
        "        learning_rate=5e-5,  # Lower learning rate for stable training\n",
        "        lr_scheduler_type=\"cosine\",  # Cosine decay for better learning rate adjustment\n",
        "        weight_decay=0.01,  # Add weight decay to prevent overfitting\n",
        "        max_steps=1000,  # Increase training steps for convergence\n",
        "        warmup_steps=100,  # Add a warmup phase to stabilize initial training\n",
        "        fp16=not is_bfloat16_supported(),  # Use FP16 precision if bfloat16 is not supported\n",
        "        bf16=is_bfloat16_supported(),  # Use bfloat16 precision if supported\n",
        "        output_dir=\"outputs\",  # Directory to save outputs such as checkpoints\n",
        "        report_to=\"wandb\",  # Report metrics to WandB for better tracking\n",
        "        save_steps=50,  # Save checkpoints regularly to avoid loss of progress\n",
        "        evaluation_strategy=\"steps\",  # Evaluate at each step interval\n",
        "        eval_steps=50,  # Evaluate model every 50 steps\n",
        "        save_total_limit=3,  # Keep only the last 3 checkpoints to save disk space\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWGFqAo5Q2me"
      },
      "outputs": [],
      "source": [
        "orpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ],
      "metadata": {
        "id": "FgEvCW76xblp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "# This is a placeholder for the prompt template, presumably copied from another section of the code.\n",
        "\n",
        "# Enable native 2x faster inference using FastLanguageModel\n",
        "# This allows the model to run inference faster by optimizing internal processes for speed.\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare input data by formatting the prompt with specific instructions and input/output placeholders\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        # Format the prompt with the given instruction, input, and an empty output for generation\n",
        "        alpaca_prompt.format(\n",
        "            \"Given the following medical question or situation, provide the most suitable reasoning or explanation\",  # Instruction text to guide the model\n",
        "            \"A Fabry-Perot interferometer is used to resolve the mode structure of a He-Ne laser operating at 6328 Å with a frequency separation between the modes of 150 MHz. Determine the required plate spacing for cases where the reflectance of the mirrors is (a) 0.9 and (b) 0.999.\",  # The input query/question\n",
        "            \"\",  # Empty output to leave space for the generated response\n",
        "        )\n",
        "    ],\n",
        "    # Return tokenized inputs in PyTorch tensor format\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")  # Move inputs to GPU for faster computation\n",
        "\n",
        "# Generate output based on the model's inference from the formatted input\n",
        "outputs = model.generate(\n",
        "    **inputs,              # Pass the tokenized inputs to the model\n",
        "    max_new_tokens = 200,  # Limit the output to a maximum of 200 tokens\n",
        "    use_cache = True       # Enable caching to speed up the generation process by reusing previous computations\n",
        ")\n",
        "\n",
        "# Decode the generated tokens back into text format for human-readable output\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "0DJPbbtGxcFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ],
      "metadata": {
        "id": "absTV8M1xzwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "# This is a placeholder for the prompt template, which is presumably defined elsewhere in the code.\n",
        "\n",
        "# Enable native 2x faster inference by optimizing the model for inference tasks\n",
        "# This method configures the model to use efficient inference settings, improving the processing speed.\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the input data for the model by formatting the prompt with a specific instruction and input\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        # Format the prompt by inserting the instruction, input question, and leave the output blank for generation\n",
        "        alpaca_prompt.format(\n",
        "            \"Given the following medical question or situation, provide the most suitable reasoning or explanation\",  # Instruction provided to the model\n",
        "            \"A Fabry-Perot interferometer is used to resolve the mode structure of a He-Ne laser operating at 6328 Å with a frequency separation between the modes of 150 MHz. Determine the required plate spacing for cases where the reflectance of the mirrors is (a) 0.9 and (b) 0.999.\",  # Input question or scenario\n",
        "            \"\",  # Blank output, as the model will generate the response\n",
        "        )\n",
        "    ],\n",
        "    return_tensors = \"pt\"  # Ensure the input is tokenized and returned as a PyTorch tensor\n",
        ").to(\"cuda\")  # Move the tensor to the GPU for faster processing\n",
        "\n",
        "# Import the TextStreamer class from the transformers library\n",
        "# The TextStreamer is used to stream the output during generation, allowing more efficient generation of long text outputs.\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Initialize the TextStreamer with the tokenizer to handle token-to-text conversion during generation\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Generate text from the model based on the provided inputs\n",
        "# Using the TextStreamer, this will stream the generation process, allowing tokens to be decoded and displayed progressively\n",
        "_ = model.generate(\n",
        "    **inputs,                 # Pass the tokenized input data to the model\n",
        "    streamer = text_streamer, # Use the TextStreamer to handle the output streaming\n",
        "    max_new_tokens = 128      # Limit the generation to a maximum of 128 new tokens\n",
        ")"
      ],
      "metadata": {
        "id": "xKzWRN1Px0Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "Y_3rdZXmx3Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "HJbRqLynx3a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference"
      ],
      "metadata": {
        "id": "mwIRb8DByBGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import FastLanguageModel from the unsloth library, which provides methods for fast inference with language models\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the pre-trained model and tokenizer using the specified configuration\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\",  # Specify the model name or path to the model you trained or want to use\n",
        "    max_seq_length = max_seq_length,  # Set the maximum sequence length for tokenized inputs\n",
        "    dtype = dtype,  # Define the data type for the model (e.g., float32, float16, etc.)\n",
        "    load_in_4bit = load_in_4bit,  # If True, load the model with reduced precision (4-bit) for more efficient memory usage\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference by configuring the model for optimized inference operations\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "# The alpaca_prompt should be defined elsewhere in the code or copied from previous sections.\n",
        "\n",
        "# Prepare the input data by formatting the prompt with a specific instruction and input for the model\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        # Format the prompt string by injecting the instruction, input scenario, and an empty output for the model to generate\n",
        "        alpaca_prompt.format(\n",
        "            \"Given the following medical question or situation, provide the most suitable reasoning or explanation\",  # Instruction text to guide the model\n",
        "            \"A Fabry-Perot interferometer is used to resolve the mode structure of a He-Ne laser operating at 6328 Å with a frequency separation between the modes of 150 MHz. Determine the required plate spacing for cases where the reflectance of the mirrors is (a) 0.9 and (b) 0.999.\",  # Input question or scientific scenario\n",
        "            \"\",  # Blank output, as the model will generate the response in place of this empty string\n",
        "        )\n",
        "    ],\n",
        "    return_tensors = \"pt\"  # Ensure the tokenized input is returned as a PyTorch tensor\n",
        ").to(\"cuda\")  # Move the tokenized input to the GPU for faster processing during inference\n",
        "\n",
        "# Generate output from the model based on the tokenized input\n",
        "outputs = model.generate(\n",
        "    **inputs,           # Pass the tokenized inputs to the model\n",
        "    max_new_tokens = 64,  # Set the maximum number of new tokens to be generated in the output\n",
        "    use_cache = True     # Use the model's caching mechanism to speed up subsequent generations by reusing computations\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens back into human-readable text format\n",
        "tokenizer.batch_decode(outputs)  # Decode the generated tokens into a string and return the result\n"
      ],
      "metadata": {
        "id": "UPHJs9wDyBbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Push the trained model to the Hugging Face Model Hub using the GGUF format"
      ],
      "metadata": {
        "id": "ssjW3ST2yI1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the trained model to the Hugging Face Model Hub using the GGUF format\n",
        "model.push_to_hub_gguf(\n",
        "    \"SURESHBEEKHANI/Gemma-2B-Medical-O1-SFT-ORPO-RLHF-Fine-Tuning\",  # Specify the model repository path on Hugging Face Hub. Replace \"hf\" with your Hugging Face username.\n",
        "    tokenizer,  # Pass the tokenizer associated with the model to ensure compatibility on the hub\n",
        "    quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],  # Specify the quantization methods to apply for optimized model storage (e.g., q4_k_m, q8_0, q5_k_m)\n",
        "    token=\"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\",  # Provide the Hugging Face token for authentication. Obtain a token at https://huggingface.co/settings/tokens\n",
        ")"
      ],
      "metadata": {
        "id": "FKVTNAwyyKFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "659af940dbe44ba78214828fc4bb6697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a64dc73e9244f13b195573be3f63d4b",
              "IPY_MODEL_627e360f252549d28ff40e3b83215678",
              "IPY_MODEL_06b79a921f46416cb778509c69998750"
            ],
            "layout": "IPY_MODEL_e1cd2963bde34da18dd3c5213b58f090"
          }
        },
        "9a64dc73e9244f13b195573be3f63d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4539935793084157b98a0c4a28bfc19f",
            "placeholder": "​",
            "style": "IPY_MODEL_38b3f3714f994aa884e9b008e9b2c163",
            "value": "Map: 100%"
          }
        },
        "627e360f252549d28ff40e3b83215678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed09542d01604e14ac7b5bdb6247ffec",
            "max": 4200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d46cc82e81e84f98b0973d019f82f130",
            "value": 4200
          }
        },
        "06b79a921f46416cb778509c69998750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0e357099e1442b396f280c48e6566c7",
            "placeholder": "​",
            "style": "IPY_MODEL_2dbfaaf5018a49a8a90e46aa3df4ebd9",
            "value": " 4200/4200 [00:02&lt;00:00, 1791.44 examples/s]"
          }
        },
        "e1cd2963bde34da18dd3c5213b58f090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4539935793084157b98a0c4a28bfc19f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b3f3714f994aa884e9b008e9b2c163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed09542d01604e14ac7b5bdb6247ffec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d46cc82e81e84f98b0973d019f82f130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0e357099e1442b396f280c48e6566c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbfaaf5018a49a8a90e46aa3df4ebd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}